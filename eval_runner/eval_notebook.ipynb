{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Running on host:  submit-1\n"
     ]
    }
   ],
   "source": [
    "# BOILER PLATE, MUST BE RUN ON SUBMIT NODE\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import socket\n",
    "print(\"Running on host: \", socket.gethostname())\n",
    "\n",
    "import sys \n",
    "lib_path = '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'\n",
    "if lib_path not in sys.path:\n",
    "    sys.path.append(lib_path)\n",
    "#=================================================\n",
    "from typing import Any \n",
    "import time\n",
    "\n",
    "import pandas as pd \n",
    "import json\n",
    "import os\n",
    "import wandb \n",
    "import launch_evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wait_seconds': 900, 'wandb_project': 'mm10.1_stage1', 'configs_path': '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite', 'max_jobs': 2, 'watch_path': '/fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_BigG_336px_R1'}\n",
      "Scanning for new checkpoints in /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_BigG_336px_R1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:launch_job:Eval output is at: /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1/evals/eval_vqa_checkpoint-1100/eval_vqa_eval_results.txt\n",
      "WARNING:launch_job:stdout is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1/evals/eval_vqa_checkpoint-1100/241021_15_58_36_081612/run_log.txt\n",
      "WARNING:launch_job:stderr is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1/evals/eval_vqa_checkpoint-1100/241021_15_58_36_081612/run_log.txt\n",
      "WARNING:launch_job:Config file written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1/evals/eval_vqa_checkpoint-1100/241021_15_58_36_081612/config.json\n",
      "WARNING:launch_job:script written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1/evals/eval_vqa_checkpoint-1100/241021_15_58_36_081612/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:launch_job:Eval output is at: /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1/evals/eval_mmmu_checkpoint-1200/eval_mmmu_eval_results.txt\n",
      "WARNING:launch_job:stdout is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1/evals/eval_mmmu_checkpoint-1200/241021_15_58_36_122656/run_log.txt\n",
      "WARNING:launch_job:stderr is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1/evals/eval_mmmu_checkpoint-1200/241021_15_58_36_122656/run_log.txt\n",
      "WARNING:launch_job:Config file written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1/evals/eval_mmmu_checkpoint-1200/241021_15_58_36_122656/config.json\n",
      "WARNING:launch_job:script written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1/evals/eval_mmmu_checkpoint-1200/241021_15_58_36_122656/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'wait_seconds': 900, 'wandb_project': 'mm10.1_stage1', 'configs_path': '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite', 'max_jobs': 2, 'watch_path': '/fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1'}\n",
      "Scanning for new checkpoints in /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1\n",
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     num_jobs \u001b[38;5;241m=\u001b[39m run_eval_sweep(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mjob_config)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(num_jobs)\n\u001b[0;32m---> 63\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwait_seconds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_eval_sweep(\n",
    "    configs_path: str,\n",
    "    watch_path: str,\n",
    "    max_jobs: int,\n",
    "    wait_seconds: int,\n",
    "    conda_env: str | None = None,\n",
    "    prefix_name: str | None = None,\n",
    "    start_step: int | None = None,\n",
    "    stop_step: int | None = None,\n",
    "    every_steps: int | None = None,\n",
    "    base_eval_output_dir: str = \"evals\",\n",
    "    overrides: list[list[tuple[str, Any]]] | None = None,\n",
    "    rerun_if_exists: bool = False,\n",
    "    wandb_project: str | None = None\n",
    "):\n",
    "    num_jobs = 0\n",
    "    \n",
    "    available_steps = launch_evals._get_available_checkpoints(watch_path)\n",
    "    \n",
    "    for step in available_steps:\n",
    "        if (start_step is not None and step < start_step) or (\n",
    "            every_steps is not None and step % every_steps != 0\n",
    "        ):\n",
    "            continue\n",
    "        \n",
    "        new_jobs = launch_evals.run_eval_steps(\n",
    "            configs_path=configs_path,\n",
    "            watch_path=watch_path,\n",
    "            max_num_jobs=max_jobs - num_jobs,\n",
    "            conda_env=conda_env,\n",
    "            prefix_name=prefix_name,\n",
    "            steps=[step],\n",
    "            base_eval_output_dir=base_eval_output_dir,\n",
    "            overrides=overrides,\n",
    "            rerun_if_exists=rerun_if_exists,\n",
    "        )\n",
    "        \n",
    "        num_jobs += new_jobs\n",
    "        if num_jobs == max_jobs:\n",
    "            # will not launch more job in this iteration\n",
    "            break\n",
    "\n",
    "        if stop_step is not None and step == stop_step:\n",
    "            break\n",
    "        \n",
    "    return num_jobs\n",
    "\n",
    "with open('eval_watcher_config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "global_config = config.copy()\n",
    "global_config.pop(\"watch_jobs\")\n",
    "\n",
    "for job in config['watch_jobs']:\n",
    "    job_config = global_config.copy()\n",
    "    job_config.update(job)\n",
    "    \n",
    "    print(job_config)\n",
    "    print(f\"Scanning for new checkpoints in {job_config['watch_path']}\")\n",
    "    num_jobs = run_eval_sweep(**job_config)\n",
    "    print(num_jobs)\n",
    "    \n",
    "time.sleep(global_config[\"wait_seconds\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of eval dataloader before sampling: 100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# Step 1: Define a custom dataset\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Let's assume ids are just numbers from 0 to 99\n",
    "        self.ids = list(range(100))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return the id and a dummy tensor data (for example purposes)\n",
    "        return self.ids[index], torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Step 2: Create the dataset and dataloader\n",
    "dataset = DummyDataset()\n",
    "eval_dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Step 3: Access the length of the dataset before sampling\n",
    "length_eval_dataloader_before_sampling = len(eval_dataloader.dataset.ids)\n",
    "print(\"Length of eval dataloader before sampling:\", length_eval_dataloader_before_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Subset' object has no attribute 'ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m subset_dataset \u001b[38;5;241m=\u001b[39m Subset(dataset, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43msubset_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mids\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'ids'"
     ]
    }
   ],
   "source": [
    "subset_dataset = Subset(dataset, indices=range(50))\n",
    "len(subset_dataset.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing run at /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH19_70B_BigG_336px_R1_test/evals/wandb_report_run.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/hpcaas/.mounts/fs-06bc3d6b93146dddd/user/tranx/experiments/eval_runner/wandb/run-20241016_191919-9qy4weo4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1/runs/9qy4weo4' target=\"_blank\">MH19_70B_BigG_336px_R1_test_evals</a></strong> to <a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1' target=\"_blank\">https://fairwandb.org/sg_share_ai/mm10.1_stage1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1/runs/9qy4weo4' target=\"_blank\">https://fairwandb.org/sg_share_ai/mm10.1_stage1/runs/9qy4weo4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>mmmu</td><td>▂▂▁▄▅▅▅█▅▄▅▄▆▆▅▅▅▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ai2d</td><td>nan</td></tr><tr><td>chartqa</td><td>nan</td></tr><tr><td>docvqa</td><td>nan</td></tr><tr><td>infographics</td><td>nan</td></tr><tr><td>infographics_w_ocr</td><td>nan</td></tr><tr><td>mathvista</td><td>nan</td></tr><tr><td>mmbench</td><td>nan</td></tr><tr><td>mmmu</td><td>56.78</td></tr><tr><td>vqa</td><td>nan</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MH19_70B_BigG_336px_R1_test_evals</strong> at: <a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1/runs/9qy4weo4' target=\"_blank\">https://fairwandb.org/sg_share_ai/mm10.1_stage1/runs/9qy4weo4</a><br/> View project at: <a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1' target=\"_blank\">https://fairwandb.org/sg_share_ai/mm10.1_stage1</a><br/>Synced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241016_191919-9qy4weo4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_wandb_eval_report(\n",
    "    report_df: pd.DataFrame,\n",
    "    wandb_project: str,\n",
    "    checkpoint_watch_path: str,\n",
    "    wandb_entity: str=\"sg_share_ai\",\n",
    "    eval_base_dir: str=\"evals\",\n",
    "    delete_if_exists: bool = True\n",
    "):\n",
    "    wandb.login()\n",
    "    \n",
    "    # look for existing run\n",
    "    prev_run_config = None\n",
    "    wandb_run_config_file = os.path.join(checkpoint_watch_path, eval_base_dir, \"wandb_report_run.json\")\n",
    "    if delete_if_exists and os.path.isfile(wandb_run_config_file):\n",
    "        print(f\"Deleting existing run at {wandb_run_config_file}\")\n",
    "        with open(wandb_run_config_file, 'r') as f:\n",
    "            prev_run_config = json.load(f)\n",
    "            try:\n",
    "                api = wandb.Api()\n",
    "                prev_run = api.run(f\"{prev_run_config['entity']}/{prev_run_config['project']}/{prev_run_config['id']}\")\n",
    "                prev_run.delete()\n",
    "            except Exception as e:\n",
    "                print(f\"Unable to delete previous run. {e}\")\n",
    "                \n",
    "    checkpoint_folder = os.path.basename(checkpoint_watch_path)\n",
    "    \n",
    "    run = wandb.init(\n",
    "        entity=wandb_entity,\n",
    "        project=wandb_project,\n",
    "        name=f\"{checkpoint_folder}_{eval_base_dir}\"\n",
    "    )\n",
    "    \n",
    "    for step, row in report_df.iterrows():\n",
    "        wandb.log(dict(row), step=step)\n",
    "    \n",
    "    # Saving run config to eval folder\n",
    "    run_config = {\n",
    "        \"entity\": run.entity,\n",
    "        \"id\": run.id,\n",
    "        \"project\": run.project,\n",
    "        \"name\": run.name\n",
    "    }\n",
    "    \n",
    "    with open(wandb_run_config_file, 'w') as f:\n",
    "        json.dump(run_config, f)\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "create_wandb_eval_report(\n",
    "    checkpoint_watch_path=\"/fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH19_70B_BigG_336px_R1_test\",\n",
    "    wandb_project=\"mm10.1_stage1\",\n",
    "    report_df=df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MM10.1 - Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing run at /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH19_70B_BigG_336px_R1_test/evals/wandb_report_run.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/hpcaas/.mounts/fs-06bc3d6b93146dddd/user/tranx/experiments/eval_runner/wandb/run-20241016_192214-xmszucy1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1/runs/xmszucy1' target=\"_blank\">MH19_70B_BigG_336px_R1_test_evals</a></strong> to <a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1' target=\"_blank\">https://fairwandb.org/sg_share_ai/mm10.1_stage1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1/runs/xmszucy1' target=\"_blank\">https://fairwandb.org/sg_share_ai/mm10.1_stage1/runs/xmszucy1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>mmmu</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ai2d</td><td>nan</td></tr><tr><td>chartqa</td><td>nan</td></tr><tr><td>docvqa</td><td>nan</td></tr><tr><td>infographics</td><td>nan</td></tr><tr><td>infographics_w_ocr</td><td>nan</td></tr><tr><td>mathvista</td><td>nan</td></tr><tr><td>mmbench</td><td>nan</td></tr><tr><td>mmmu</td><td>44.22</td></tr><tr><td>vqa</td><td>nan</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MH19_70B_BigG_336px_R1_test_evals</strong> at: <a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1/runs/xmszucy1' target=\"_blank\">https://fairwandb.org/sg_share_ai/mm10.1_stage1/runs/xmszucy1</a><br/> View project at: <a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1' target=\"_blank\">https://fairwandb.org/sg_share_ai/mm10.1_stage1</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241016_192214-xmszucy1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "watch_path=\"/fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH19_70B_BigG_336px_R1_test\"\n",
    "\n",
    "df = launch_evals.read_eval_results(\n",
    "    watch_path=watch_path,\n",
    "    read_format=\"report_mmai\"\n",
    ")\n",
    "\n",
    "create_wandb_eval_report(\n",
    "    checkpoint_watch_path=watch_path,\n",
    "    wandb_project=\"mm10.1_stage1\",\n",
    "    report_df=df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/hpcaas/.mounts/fs-06bc3d6b93146dddd/user/tranx/experiments/eval_runner/wandb/run-20241016_192245-t6rkxlh6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1/runs/t6rkxlh6' target=\"_blank\">MH22_70B_BigG_336px_R1_test_evals</a></strong> to <a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1' target=\"_blank\">https://fairwandb.org/sg_share_ai/mm10.1_stage1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1/runs/t6rkxlh6' target=\"_blank\">https://fairwandb.org/sg_share_ai/mm10.1_stage1/runs/t6rkxlh6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>mmmu</td><td>▂▂▁▄▅▅▅█▅▄▅▄▆▆▅▅▅▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ai2d</td><td>nan</td></tr><tr><td>chartqa</td><td>nan</td></tr><tr><td>docvqa</td><td>nan</td></tr><tr><td>infographics</td><td>nan</td></tr><tr><td>infographics_w_ocr</td><td>nan</td></tr><tr><td>mathvista</td><td>nan</td></tr><tr><td>mmbench</td><td>nan</td></tr><tr><td>mmmu</td><td>56.78</td></tr><tr><td>vqa</td><td>nan</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MH22_70B_BigG_336px_R1_test_evals</strong> at: <a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1/runs/t6rkxlh6' target=\"_blank\">https://fairwandb.org/sg_share_ai/mm10.1_stage1/runs/t6rkxlh6</a><br/> View project at: <a href='https://fairwandb.org/sg_share_ai/mm10.1_stage1' target=\"_blank\">https://fairwandb.org/sg_share_ai/mm10.1_stage1</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241016_192245-t6rkxlh6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "watch_path=\"/fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22_70B_BigG_336px_R1_test\"\n",
    "\n",
    "df = launch_evals.read_eval_results(\n",
    "    watch_path=watch_path,\n",
    "    read_format=\"report_mmai\"\n",
    ")\n",
    "\n",
    "create_wandb_eval_report(\n",
    "    checkpoint_watch_path=watch_path,\n",
    "    wandb_project=\"mm10.1_stage1\",\n",
    "    report_df=df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test py launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:launch_job:Eval output is at: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_chartqa_checkpoint-15000/eval_chartqa_eval_results.txt\n",
      "WARNING:launch_job:stdout is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_chartqa_checkpoint-15000/241014_18_09_04_052047/run_log.txt\n",
      "WARNING:launch_job:stderr is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_chartqa_checkpoint-15000/241014_18_09_04_052047/run_log.txt\n",
      "WARNING:launch_job:Config file written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_chartqa_checkpoint-15000/241014_18_09_04_052047/config.json\n",
      "WARNING:launch_job:script written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_chartqa_checkpoint-15000/241014_18_09_04_052047/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:launch_job:Eval output is at: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_vqa_checkpoint-15000/eval_vqa_eval_results.txt\n",
      "WARNING:launch_job:stdout is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_vqa_checkpoint-15000/241014_18_09_04_108199/run_log.txt\n",
      "WARNING:launch_job:stderr is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_vqa_checkpoint-15000/241014_18_09_04_108199/run_log.txt\n",
      "WARNING:launch_job:Config file written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_vqa_checkpoint-15000/241014_18_09_04_108199/config.json\n",
      "WARNING:launch_job:script written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_vqa_checkpoint-15000/241014_18_09_04_108199/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:launch_job:Eval output is at: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_textvqa_checkpoint-15000/eval_textvqa_eval_results.txt\n",
      "WARNING:launch_job:stdout is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_textvqa_checkpoint-15000/241014_18_09_04_151480/run_log.txt\n",
      "WARNING:launch_job:stderr is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_textvqa_checkpoint-15000/241014_18_09_04_151480/run_log.txt\n",
      "WARNING:launch_job:Config file written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_textvqa_checkpoint-15000/241014_18_09_04_151480/config.json\n",
      "WARNING:launch_job:script written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_textvqa_checkpoint-15000/241014_18_09_04_151480/launch_script.sh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:launch_job:Eval output is at: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_infographics_w_ocr_checkpoint-15000/eval_infographics_w_ocr_eval_results.txt\n",
      "WARNING:launch_job:stdout is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_infographics_w_ocr_checkpoint-15000/241014_18_09_04_195357/run_log.txt\n",
      "WARNING:launch_job:stderr is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_infographics_w_ocr_checkpoint-15000/241014_18_09_04_195357/run_log.txt\n",
      "WARNING:launch_job:Config file written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_infographics_w_ocr_checkpoint-15000/241014_18_09_04_195357/config.json\n",
      "WARNING:launch_job:script written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_infographics_w_ocr_checkpoint-15000/241014_18_09_04_195357/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:launch_job:Eval output is at: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_mmmu_checkpoint-15000/eval_mmmu_eval_results.txt\n",
      "WARNING:launch_job:stdout is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_mmmu_checkpoint-15000/241014_18_09_04_237528/run_log.txt\n",
      "WARNING:launch_job:stderr is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_mmmu_checkpoint-15000/241014_18_09_04_237528/run_log.txt\n",
      "WARNING:launch_job:Config file written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_mmmu_checkpoint-15000/241014_18_09_04_237528/config.json\n",
      "WARNING:launch_job:script written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_mmmu_checkpoint-15000/241014_18_09_04_237528/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:launch_job:Eval output is at: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_mmbench_checkpoint-15000/eval_mmbench_eval_results.txt\n",
      "WARNING:launch_job:stdout is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_mmbench_checkpoint-15000/241014_18_09_04_282284/run_log.txt\n",
      "WARNING:launch_job:stderr is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_mmbench_checkpoint-15000/241014_18_09_04_282284/run_log.txt\n",
      "WARNING:launch_job:Config file written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_mmbench_checkpoint-15000/241014_18_09_04_282284/config.json\n",
      "WARNING:launch_job:script written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_mmbench_checkpoint-15000/241014_18_09_04_282284/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:launch_job:Eval output is at: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_mathvista_checkpoint-15000/eval_mathvista_eval_results.txt\n",
      "WARNING:launch_job:stdout is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_mathvista_checkpoint-15000/241014_18_09_04_327771/run_log.txt\n",
      "WARNING:launch_job:stderr is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_mathvista_checkpoint-15000/241014_18_09_04_327771/run_log.txt\n",
      "WARNING:launch_job:Config file written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_mathvista_checkpoint-15000/241014_18_09_04_327771/config.json\n",
      "WARNING:launch_job:script written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_mathvista_checkpoint-15000/241014_18_09_04_327771/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:launch_job:Eval output is at: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_docvqa_checkpoint-15000/eval_docvqa_eval_results.txt\n",
      "WARNING:launch_job:stdout is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_docvqa_checkpoint-15000/241014_18_09_04_370504/run_log.txt\n",
      "WARNING:launch_job:stderr is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_docvqa_checkpoint-15000/241014_18_09_04_370504/run_log.txt\n",
      "WARNING:launch_job:Config file written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_docvqa_checkpoint-15000/241014_18_09_04_370504/config.json\n",
      "WARNING:launch_job:script written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_docvqa_checkpoint-15000/241014_18_09_04_370504/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:launch_job:Eval output is at: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_ai2d_checkpoint-15000/eval_ai2d_eval_results.txt\n",
      "WARNING:launch_job:stdout is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_ai2d_checkpoint-15000/241014_18_09_04_410421/run_log.txt\n",
      "WARNING:launch_job:stderr is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_ai2d_checkpoint-15000/241014_18_09_04_410421/run_log.txt\n",
      "WARNING:launch_job:Config file written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_ai2d_checkpoint-15000/241014_18_09_04_410421/config.json\n",
      "WARNING:launch_job:script written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_ai2d_checkpoint-15000/241014_18_09_04_410421/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:launch_job:Eval output is at: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_infographics_checkpoint-15000/eval_infographics_eval_results.txt\n",
      "WARNING:launch_job:stdout is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_infographics_checkpoint-15000/241014_18_09_04_451796/run_log.txt\n",
      "WARNING:launch_job:stderr is redirected to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_infographics_checkpoint-15000/241014_18_09_04_451796/run_log.txt\n",
      "WARNING:launch_job:Config file written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_infographics_checkpoint-15000/241014_18_09_04_451796/config.json\n",
      "WARNING:launch_job:script written to: /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py_fix/eval_infographics_checkpoint-15000/241014_18_09_04_451796/launch_script.sh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "launch_evals.run_eval_steps(\n",
    "    configs_path=\"/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10/stage2/eval_overwrite_fix\",\n",
    "    watch_path=\"/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128\",\n",
    "    steps=[15000],\n",
    "    base_eval_output_dir=\"evals_py_fix\",\n",
    "    max_num_jobs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>73.77</td>\n",
       "      <td>48.8</td>\n",
       "      <td>78.01</td>\n",
       "      <td>50.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.08</td>\n",
       "      <td>64.92</td>\n",
       "      <td>59.13</td>\n",
       "      <td>73.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mmmu  docvqa  mathvista   ai2d  chartqa  vqa  textvqa  \\\n",
       "15000  NaN   73.77       48.8  78.01    50.69  NaN    72.08   \n",
       "\n",
       "       infographics_w_ocr  infographics  mmbench  \n",
       "15000               64.92         59.13    73.19  "
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "launch_evals.read_eval_results(\n",
    "    watch_path=\"/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128\",\n",
    "    base_eval_output_dir=\"evals_py_fix\",\n",
    "    read_format=\"report_mmai\",\n",
    "    steps=[15000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>56.44</td>\n",
       "      <td>72.8</td>\n",
       "      <td>48.8</td>\n",
       "      <td>78.01</td>\n",
       "      <td>52.22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.08</td>\n",
       "      <td>64.92</td>\n",
       "      <td>59.13</td>\n",
       "      <td>72.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mmmu  docvqa  mathvista   ai2d  chartqa  vqa  textvqa  \\\n",
       "15000  56.44    72.8       48.8  78.01    52.22  NaN    72.08   \n",
       "\n",
       "       infographics_w_ocr  infographics  mmbench  \n",
       "15000               64.92         59.13    72.24  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "launch_evals.read_eval_results(\n",
    "    watch_path=\"/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128\",\n",
    "    base_eval_output_dir=\"evals_py\",\n",
    "    read_format=\"report_mmai\",\n",
    "    steps=[15000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_mmmu.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_chartqa.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_infographics_w_ocr.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_docvqa.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_ai2d.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_textvqa.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_mmbench.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_infographics.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_mathvista.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_vqa.json']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(\"/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_*.json\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/*\"\n",
    "os.path.isdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_mmmu.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_chartqa.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_infographics_w_ocr.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_docvqa.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_ai2d.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_textvqa.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_mmbench.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_infographics.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_mathvista.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_vqa.json']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(\"/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/*\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_mmmu.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(\"/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_mmmu.json\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_mmmu.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_chartqa.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_infographics_w_ocr.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_docvqa.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_ai2d.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_textvqa.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_mmbench.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_infographics.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_mathvista.json',\n",
       " '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/eval_vqa.json']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob \n",
    "glob(\"/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10.1/stage1/eval_overwrite/*\", recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval args: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'eval_vqa_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_vqa', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'media_only_regularizer_weight': None, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'max_eval_samples': 25000, 'train_file': '/fsx_0/dataset01/vqa/VQAv2_train_processed.jsonl', 'validation_file': '/fsx_0/dataset01/vqa/VQAv2_val_multians_20231118.json', 'max_length': 30, 'max_new_tokens': 32, 'min_new_tokens': 1, 'max_seq_len': 100, 'batch_size_generation': 16, 'instr_prompt': 'vqa', 'task_type': 'instruction_tune', 'generation_task': 'InstructionGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}eval args: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'eval_vqa_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_vqa', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'media_only_regularizer_weight': None, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'max_eval_samples': 25000, 'train_file': '/fsx_0/dataset01/vqa/VQAv2_train_processed.jsonl', 'validation_file': '/fsx_0/dataset01/vqa/VQAv2_val_multians_20231118.json', 'max_length': 30, 'max_new_tokens': 32, 'min_new_tokens': 1, 'max_seq_len': 100, 'batch_size_generation': 16, 'instr_prompt': 'vqa', 'task_type': 'instruction_tune', 'generation_task': 'InstructionGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}eval args: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'eval_vqa_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_vqa', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'media_only_regularizer_weight': None, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'max_eval_samples': 25000, 'train_file': '/fsx_0/dataset01/vqa/VQAv2_train_processed.jsonl', 'validation_file': '/fsx_0/dataset01/vqa/VQAv2_val_multians_20231118.json', 'max_length': 30, 'max_new_tokens': 32, 'min_new_tokens': 1, 'max_seq_len': 100, 'batch_size_generation': 16, 'instr_prompt': 'vqa', 'task_type': 'instruction_tune', 'generation_task': 'InstructionGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}\n",
      "Exclude key 'debug' from eval_args due to it being in the excluded listExclude key 'fsdp' from eval_args due to it being in the excluded listeval args: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'eval_vqa_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_vqa', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'media_only_regularizer_weight': None, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'max_eval_samples': 25000, 'train_file': '/fsx_0/dataset01/vqa/VQAv2_train_processed.jsonl', 'validation_file': '/fsx_0/dataset01/vqa/VQAv2_val_multians_20231118.json', 'max_length': 30, 'max_new_tokens': 32, 'min_new_tokens': 1, 'max_seq_len': 100, 'batch_size_generation': 16, 'instr_prompt': 'vqa', 'task_type': 'instruction_tune', 'generation_task': 'InstructionGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}eval args: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'eval_vqa_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_vqa', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'media_only_regularizer_weight': None, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'max_eval_samples': 25000, 'train_file': '/fsx_0/dataset01/vqa/VQAv2_train_processed.jsonl', 'validation_file': '/fsx_0/dataset01/vqa/VQAv2_val_multians_20231118.json', 'max_length': 30, 'max_new_tokens': 32, 'min_new_tokens': 1, 'max_seq_len': 100, 'batch_size_generation': 16, 'instr_prompt': 'vqa', 'task_type': 'instruction_tune', 'generation_task': 'InstructionGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}\n",
      "Exclude key 'fsdp' from eval_args due to it being in the excluded listExclude key 'fsdp_min_num_params' from eval_args due to it being in the excluded listeval args: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'eval_vqa_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_vqa', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP':Exclude key 'fsdp_config' from eval_args due to it being in the excluded list\n",
      "eval args: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'eval_vqa_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': Ne, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_vqa', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': \n",
      "eval args after sanitization: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'gradient_accumulation_steps': 2, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpointreat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': Exclude key 'debug' from eval_args due to it being in the excluded list\n",
      "s/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'past_index': -1, 'run_name': 'eval_vqa_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_vqa', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'clips_per_video': 1, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggSetting perception to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt\n",
      "eval args after sanitization: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'gradient_accumulation_steps': 2, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpointeval args after sanitization: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'gradient_accumulation_steps': 2, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'past_index': -1, 'run_name': 'eval_vqa_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_vqa', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'clips_per_video': 1, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggExclude key 'debug' from eval_args due to it being in the excluded list\n",
      "one, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_vqa', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP':ingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'max_eval_samples': 25000, 'train_file': '/fsx_0/dataset01/vqa/VQAv2_train_processed.jsonl', 'validation_file': '/fsx_0/dataset01/vqa/VQAv2_val_multians_20231118.json', 'max_length': 30, 'max_new_tokens': 32, 'min_new_tokens': 1, 'max_seq_len': 100, 'batch_size_generation': 16, 'instr_prompt': 'vqa', 'task_type': 'instruction_tune', 'generation_task': 'InstructionGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}s/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'past_index': -1, 'run_name': 'eval_vqa_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_vqa', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'clips_per_video': 1, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'max_eval_samples': 25000, 'train_file': '/fsx_0/dataset01/vqa/VQAv2_train_processed.jsonl', 'validation_file': '/fsx_0/dataset01/vqa/VQAv2_val_multians_20231118.json', 'max_length': 30, 'max_new_tokens': 32, 'min_new_tokens': 1, 'max_seq_len': 100, 'batch_size_generation': 16, 'instr_prompt': 'vqa', 'task_type': 'instruction_tune', 'generation_task': 'InstructionGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'media_only_regularizer_weight': None, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'max_eval_samples': 25000, 'train_file': '/fsx_0/dataset01/vqa/VQAv2_train_processed.jsonl', 'validation_file': '/fsx_0/dataset01/vqa/VQAv2_val_multians_20231118.json', 'max_length': 30, 'max_new_tokens': 32, 'min_new_tokens': 1, 'max_seq_len': 100, 'batch_size_generation': 16, 'instr_prompt': 'vqa', 'task_type': 'instruction_tune', 'generation_task': 'InstructionGenerationTask', 'eval_only': True, 'stopping_token_ids': ''} True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'media_only_regularizer_weight': None, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'max_eval_samples': 25000, 'train_file': '/fsx_0/dataset01/vqa/VQAv2_train_processed.jsonl', 'validation_file': '/fsx_0/dataset01/vqa/VQAv2_val_multians_20231118.json', 'max_length': 30, 'max_new_tokens': 32, 'min_new_tokens': 1, 'max_seq_len': 100, 'batch_size_generation': 16, 'instr_prompt': 'vqa', 'task_type': 'instruction_tune', 'generation_task': 'InstructionGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}eval args after sanitization: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'gradient_accumulation_steps': 2, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'past_index': -1, 'run_name': 'eval_vqa_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_vqa', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'clips_per_video': 1, 'tingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'max_eval_samples': 25000, 'train_file': '/fsx_0/dataset01/vqa/VQAv2_train_processed.jsonl', 'validation_file': '/fsx_0/dataset01/vqa/VQAv2_val_multians_20231118.json', 'max_length': 30, 'max_new_tokens': 32, 'min_new_tokens': 1, 'max_seq_len': 100, 'batch_size_generation': 16, 'instr_prompt': 'vqa', 'task_type': 'instruction_tune', 'generation_task': 'InstructionGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}\n",
      "eval args after sanitization: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'gradient_accumulation_steps': 2, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'past_index': -1, 'run_name': 'eval_vqa_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_vqa', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'clips_per_video': 1, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': \n",
      "reat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'max_eval_samples': 25000, 'train_file': '/fsx_0/dataset01/vqa/VQAv2_train_processed.jsonl', 'validation_file': '/fsx_0/dataset01/vqa/VQAv2_val_multians_20231118.json', 'max_length': 30, 'max_new_tokens': 32, 'min_new_tokens': 1, 'max_seq_len': 100, 'batch_size_generation': 16, 'instr_prompt': 'vqa', 'task_type': 'instruction_tune', 'generation_task': 'InstructionGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}\n",
      "eval args after sanitization: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'gradient_accumulation_steps': 2, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'past_index': -1, 'run_name': 'eval_vqa_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_vqa', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'clips_per_video': 1, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggs/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'past_index': -1, 'run_name': 'eval_vqa_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_vqa', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'clips_per_video': 1, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'max_eval_samples': 25000, 'train_file': '/fsx_0/dataset01/vqa/VQAv2_train_processed.jsonl', 'validation_file': '/fsx_0/dataset01/vqa/VQAv2_val_multians_20231118.json', 'max_length': 30, 'max_new_tokens': 32, 'min_new_tokens': 1, 'max_seq_len': 100, 'batch_size_generation': 16, 'instr_prompt': 'vqa', 'task_type': 'instruction_tune', 'generation_task': 'InstructionGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}ingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'max_eval_samples': 25000, 'train_file': '/fsx_0/dataset01/vqa/VQAv2_train_processed.jsonl', 'validation_file': '/fsx_0/dataset01/vqa/VQAv2_val_multians_20231118.json', 'max_length': 30, 'max_new_tokens': 32, 'min_new_tokens': 1, 'max_seq_len': 100, 'batch_size_generation': 16, 'instr_prompt': 'vqa', 'task_type': 'instruction_tune', 'generation_task': 'InstructionGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}\n"
     ]
    }
   ],
   "source": [
    "!cat /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_vqa_checkpoint-18800/*/run_log.txt | grep \"lm_mh_tokenizer_version\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval args: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 8, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'eval_ai2d_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'media_only_regularizer_weight': None, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'validation_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'max_length': 200, 'max_seq_len': 10, 'batch_size_generation': 8, 'instr_prompt': 'ai2d', 'task_type': 'instruction_tune', 'generation_task': 'AI2DGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}\n",
      "eval args: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 8, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'eval_ai2d_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'media_only_regularizer_weight': None, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'validation_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'max_length': 200, 'max_seq_len': 10, 'batch_size_generation': 8, 'instr_prompt': 'ai2d', 'task_type': 'instruction_tune', 'generation_task': 'AI2DGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}Exclude key 'debug' from eval_args due to it being in the excluded list\n",
      "eval args: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 8, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'eval_ai2d_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'media_only_regularizer_weight': None, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'validation_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'max_length': 200, 'max_seq_len': 10, 'batch_size_generation': 8, 'instr_prompt': 'ai2d', 'task_type': 'instruction_tune', 'generation_task': 'AI2DGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}\n",
      "Exclude key 'fsdp_min_num_params' from eval_args due to it being in the excluded listeval args: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 8, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'eval_ai2d_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'media_only_regularizer_weight': None, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'validation_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'max_length': 200, 'max_seq_len': 10, 'batch_size_generation': 8, 'instr_prompt': 'ai2d', 'task_type': 'instruction_tune', 'generation_task': 'AI2DGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}\n",
      "Exclude key 'debug' from eval_args due to it being in the excluded listeval args: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 8, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'eval_ai2d_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'media_only_regularizer_weight': None, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'validation_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'max_length': 200, 'max_seq_len': 10, 'batch_size_generation': 8, 'instr_prompt': 'ai2d', 'task_type': 'instruction_tune', 'generation_task': 'AI2DGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}\n",
      "Exclude key 'fsdp_min_num_params' from eval_args due to it being in the excluded listExclude key 'fsdp_config' from eval_args due to it being in the excluded listeval args: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 8, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'eval_ai2d_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'media_only_regularizer_weight': None, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'validation_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'max_length': 200, 'max_seq_len': 10, 'batch_size_generation': 8, 'instr_prompt': 'ai2d', 'task_type': 'instruction_tune', 'generation_task': 'AI2DGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}\n",
      "eval args after sanitization: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'gradient_accumulation_steps': 2, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 8, 'past_index': -1, 'run_name': 'eval_ai2d_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'clips_per_video': 1, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', Exclude key 'debug' from eval_args due to it being in the excluded list\n",
      "eval args: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 8, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'eval_ai2d_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules'og_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 8, 'past_index': -1, 'run_name': 'eval_ai2d_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'clips_per_video': 1, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'validation_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'max_length': 200, 'max_seq_len': 10, 'batch_size_generation': 8, 'instr_prompt': 'ai2d', 'task_type': 'instruction_tune', 'generation_task': 'AI2DGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}: None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': TSetting perception to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt\n",
      "eval args after sanitization: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'gradient_accumulation_steps': 2, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'leval args after sanitization: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'gradient_accumulation_steps': 2, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 8, 'past_index': -1, 'run_name': 'eval_ai2d_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'clips_per_video': 1, 'tre: None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_project_name': None, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': T'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'validation_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'max_length': 200, 'max_seq_len': 10, 'batch_size_generation': 8, 'instr_prompt': 'ai2d', 'task_type': 'instruction_tune', 'generation_task': 'AI2DGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}Setting perception to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt\n",
      "rue, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'media_only_regularizer_weight': None, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'validation_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'max_length': 200, 'max_seq_len': 10, 'batch_size_generation': 8, 'instr_prompt': 'ai2d', 'task_type': 'instruction_tune', 'generation_task': 'AI2DGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}og_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 8, 'past_index': -1, 'run_name': 'eval_ai2d_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'clips_per_video': 1, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', at_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1,rue, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'media_only_regularizer_weight': None, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'validation_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'max_length': 200, 'max_seq_len': 10, 'batch_size_generation': 8, 'instr_prompt': 'ai2d', 'task_type': 'instruction_tune', 'generation_task': 'AI2DGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}Setting perception to /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt\n",
      "'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'validation_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'max_length': 200, 'max_seq_len': 10, 'batch_size_generation': 8, 'instr_prompt': 'ai2d', 'task_type': 'instruction_tune', 'generation_task': 'AI2DGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}eval args after sanitization: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'gradient_accumulation_steps': 2, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 8, 'past_index': -1, 'run_name': 'eval_ai2d_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'clips_per_video': 1, 'tre\n",
      "eval args after sanitization: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'gradient_accumulation_steps': 2, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 8, 'past_index': -1, 'run_name': 'eval_ai2d_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'clips_per_video': 1, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1,at_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1,Exclude key 'debug' from eval_args due to it being in the excluded list\n",
      "eval args after sanitization: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'gradient_accumulation_steps': 2, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 8, 'past_index': -1, 'run_name': 'eval_ai2d_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'clips_per_video': 1, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'validation_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'max_length': 200, 'max_seq_len': 10, 'batch_size_generation': 8, 'instr_prompt': 'ai2d', 'task_type': 'instruction_tune', 'generation_task': 'AI2DGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}\n",
      "eval args after sanitization: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 48, 'gradient_accumulation_steps': 2, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2024, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 8, 'past_index': -1, 'run_name': 'eval_ai2d_18800', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-11500/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'perception_tokenizer_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/tensorboard', 'eval_ckpt': 18800, 'eval_type': 'eval_ai2d', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'wandb_watch': 'false', 'use_mixed_precision_model_fsdp1': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'TikTokv5ChatFormat', 'clips_per_video': 1, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'tiktoken_v5', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': False, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception.pt', 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/checkpoint-18800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': True, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'enable_modality_norm_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'perceiver_temporal_position_encoding': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 0, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'perceiver_video_delta': False, 'use_llama3_audio_frontend': True, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'validation_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl', 'max_length': 200, 'max_seq_len': 10, 'batch_size_generation': 8, 'instr_prompt': 'ai2d', 'task_type': 'instruction_tune', 'generation_task': 'AI2DGenerationTask', 'eval_only': True, 'stopping_token_ids': ''}\n",
      "instruction_model_type: TikTokv5ChatFormat\n",
      "instruction_model_type: TikTokv5ChatFormat\n",
      "instruction_model_type: TikTokv5ChatFormat\n",
      "instruction_model_type: TikTokv5ChatFormat\n",
      "instruction_model_type: TikTokv5ChatFormat\n",
      "instruction_model_type: TikTokv5ChatFormat\n",
      "instruction_model_type: TikTokv5ChatFormat\n",
      "instruction_model_type: TikTokv5ChatFormat\n"
     ]
    }
   ],
   "source": [
    "!cat /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_docvqa_unfreeze_lr1_n128/evals_py/eval_ai2d_checkpoint-18800/*/run_log.txt | grep \"instruction_model_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TikTokv5ChatFormat'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp32a_lr1_n128/checkpoint-8000\"\n",
    "with open(f\"{checkpoint_path}/training_args.bin\", \"rb\") as fd:\n",
    "    training_args = torch.load(fd, weights_only=False)\n",
    "    \n",
    "model_args = training_args.model_args.to_dict()\n",
    "data_args = training_args.data_args\n",
    "training_args = training_args.to_dict()\n",
    "data_args.instruction_model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiktoken_v5'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args['lm_mh_tokenizer_version']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
