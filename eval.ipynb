{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on host:  submit-1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# BOILER PLATE, MUST BE RUN ON SUBMIT NODE\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import socket\n",
    "print(\"Running on host: \", socket.gethostname())\n",
    "\n",
    "import sys \n",
    "lib_path = '/fsx_0/user/tranx/experiments'\n",
    "if lib_path not in sys.path:\n",
    "    sys.path.append(lib_path)\n",
    "#=================================================\n",
    "    \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from lib import eval_helper\n",
    "from lib import utils\n",
    "from lib import slurm\n",
    "from lib.slurm import run_sbatch_job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.112"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*4*160*8*(10200 - 5800)/(8*1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9675"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"eval_plan\": \"Llama31_336px\",\n",
    "    \"json_config\": \"/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json\",\n",
    "    \"checkpoint_dir\": \"/fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch\",\n",
    "    \"benchmark_name\": \"vqa\",\n",
    "    \"checkpoint_id\": \"2000\"\n",
    "}\n",
    "\n",
    "job = run_job(\n",
    "    sbatch_base_script=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    sbatch_overwrite={\n",
    "        \"job-name\": \"eval_\" + params[\"benchmark_name\"]\n",
    "    },\n",
    "    positional_env_vars=list(params.values())\n",
    ")\n",
    "\n",
    "job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 2000\n",
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 3000\n",
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 4000\n",
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 5000\n",
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 6000\n",
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 7000\n",
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 8000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 2000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 3000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 4000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 5000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 6000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 7000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 8000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 2000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 3000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 4000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 5000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 6000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 7000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 8000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 2000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 3000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 4000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 5000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 6000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 7000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 8000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 2000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 3000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 4000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 5000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 6000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 7000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 8000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 2000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 3000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 4000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 5000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 6000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 7000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 8000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 2000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 3000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 4000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 5000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 6000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 7000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 8000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 2000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 3000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 4000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 5000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 6000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 7000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 8000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 2000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 3000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 4000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 5000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 6000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 7000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 8000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 2000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 3000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 4000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 5000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 6000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 7000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 8000\n"
     ]
    }
   ],
   "source": [
    "# Llama31\n",
    "EVAL_PLAN = \"Llama31_336px\"\n",
    "BASE_SCRIPT = \"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\"\n",
    "CONFIG_DIR_LLAMA31 = \"/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31\"\n",
    "CHECKPOINT_DIR_LLAMA31 = \"/fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch\"\n",
    "\n",
    "all_benchmarks = \"mmmu docvqa mathvista ai2d chartqa vqa textvqa infographics_w_ocr infographics mmbench\"\n",
    "checkpoint_start = 2000\n",
    "checkpoint_end = 8000\n",
    "checkpoint_int = 1000\n",
    "\n",
    "job_dict = {}\n",
    "for benchmark in all_benchmarks.split():\n",
    "    job_dict[benchmark] = {}\n",
    "\n",
    "    for chk in range(checkpoint_start, checkpoint_end + checkpoint_int, checkpoint_int):\n",
    "        params = {\n",
    "            \"eval_plan\": EVAL_PLAN,\n",
    "            \"json_config\": f\"{CONFIG_DIR_LLAMA31}/eval_{benchmark}.json\",\n",
    "            \"checkpoint_dir\": CHECKPOINT_DIR_LLAMA31,\n",
    "            \"benchmark_name\": benchmark,\n",
    "            \"checkpoint_id\": str(chk)\n",
    "        }\n",
    "\n",
    "        assert os.path.exists(params[\"json_config\"])\n",
    "        assert os.path.exists(f\"{params['checkpoint_dir']}/checkpoint-{chk}\")\n",
    "\n",
    "        job_id = run_sbatch_job(\n",
    "            sbatch_base_script=BASE_SCRIPT,\n",
    "            sbatch_overwrite={\n",
    "                \"job-name\": f\"eval_{benchmark}\"\n",
    "            },\n",
    "            positional_env_vars=list(params.values())\n",
    "        )\n",
    "\n",
    "        job_dict[benchmark][chk] = int(job_id)\n",
    "\n",
    "with open(f'job_dict_{EVAL_PLAN}.json', 'w') as f:\n",
    "    json.dump(job_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m checkpoints \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m8500\u001b[39m, \u001b[38;5;241m9000\u001b[39m, \u001b[38;5;241m9500\u001b[39m]\n\u001b[1;32m      5\u001b[0m job_dict_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_dict_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEVAL_PLAN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(job_dict_json)\n\u001b[1;32m      8\u001b[0m BASE_SCRIPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m CONFIG_DIR_LLAMA31 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Llama31\n",
    "EVAL_PLAN = \"Llama31_336px_8500_9500\"\n",
    "checkpoints = [8500, 9000, 9500]\n",
    "\n",
    "job_dict_json = f'job_dict_{EVAL_PLAN}.json'\n",
    "assert not os.path.exists(job_dict_json)\n",
    "\n",
    "BASE_SCRIPT = \"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\"\n",
    "CONFIG_DIR_LLAMA31 = \"/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31\"\n",
    "CHECKPOINT_DIR_LLAMA31 = \"/fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch\"\n",
    "\n",
    "all_benchmarks = \"mmmu docvqa mathvista ai2d chartqa vqa textvqa infographics_w_ocr infographics mmbench\"\n",
    "\n",
    "job_dict = {}\n",
    "for benchmark in all_benchmarks.split():\n",
    "    job_dict[benchmark] = {}\n",
    "\n",
    "    for chk in checkpoints:\n",
    "        params = {\n",
    "            \"eval_plan\": EVAL_PLAN,\n",
    "            \"json_config\": f\"{CONFIG_DIR_LLAMA31}/eval_{benchmark}.json\",\n",
    "            \"checkpoint_dir\": CHECKPOINT_DIR_LLAMA31,\n",
    "            \"benchmark_name\": benchmark,\n",
    "            \"checkpoint_id\": str(chk)\n",
    "        }\n",
    "\n",
    "        assert os.path.exists(params[\"json_config\"])\n",
    "        assert os.path.exists(f\"{params['checkpoint_dir']}/checkpoint-{chk}\")\n",
    "\n",
    "        job_id = run_job(\n",
    "            sbatch_base_script=BASE_SCRIPT,\n",
    "            sbatch_overwrite={\n",
    "                \"job-name\": f\"eval_{benchmark}\"\n",
    "            },\n",
    "            positional_env_vars=list(params.values())\n",
    "        )\n",
    "\n",
    "        job_dict[benchmark][chk] = int(job_id)\n",
    "\n",
    "with open(job_dict_json, 'w') as f:\n",
    "    json.dump(job_dict, f, indent=4)\n",
    "\n",
    "print(f\"Saved job_dict to {job_dict_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mmmu': {2000: 9759,\n",
       "  3000: 9760,\n",
       "  4000: 9761,\n",
       "  5000: 9762,\n",
       "  6000: 9763,\n",
       "  7000: 9764,\n",
       "  8000: 9765},\n",
       " 'docvqa': {2000: 9766,\n",
       "  3000: 9767,\n",
       "  4000: 9768,\n",
       "  5000: 9769,\n",
       "  6000: 9770,\n",
       "  7000: 9771,\n",
       "  8000: 9772},\n",
       " 'mathvista': {2000: 9773,\n",
       "  3000: 9774,\n",
       "  4000: 9775,\n",
       "  5000: 9776,\n",
       "  6000: 9777,\n",
       "  7000: 9778,\n",
       "  8000: 9779},\n",
       " 'ai2d': {2000: 9780,\n",
       "  3000: 9781,\n",
       "  4000: 9782,\n",
       "  5000: 9783,\n",
       "  6000: 9784,\n",
       "  7000: 9785,\n",
       "  8000: 9786},\n",
       " 'chartqa': {2000: 9787,\n",
       "  3000: 9788,\n",
       "  4000: 9789,\n",
       "  5000: 9790,\n",
       "  6000: 9791,\n",
       "  7000: 9792,\n",
       "  8000: 9793},\n",
       " 'vqa': {2000: 9794,\n",
       "  3000: 9795,\n",
       "  4000: 9796,\n",
       "  5000: 9797,\n",
       "  6000: 9798,\n",
       "  7000: 9799,\n",
       "  8000: 9800},\n",
       " 'textvqa': {2000: 9801,\n",
       "  3000: 9802,\n",
       "  4000: 9803,\n",
       "  5000: 9804,\n",
       "  6000: 9805,\n",
       "  7000: 9806,\n",
       "  8000: 9807},\n",
       " 'infographics_w_ocr': {2000: 9808,\n",
       "  3000: 9809,\n",
       "  4000: 9810,\n",
       "  5000: 9811,\n",
       "  6000: 9812,\n",
       "  7000: 9813,\n",
       "  8000: 9814},\n",
       " 'infographics': {2000: 9815,\n",
       "  3000: 9816,\n",
       "  4000: 9817,\n",
       "  5000: 9818,\n",
       "  6000: 9819,\n",
       "  7000: 9820,\n",
       "  8000: 9821},\n",
       " 'mmbench': {2000: 9822,\n",
       "  3000: 9823,\n",
       "  4000: 9824,\n",
       "  5000: 9825,\n",
       "  6000: 9826,\n",
       "  7000: 9827,\n",
       "  8000: 9828}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama31 - 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 10000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 10000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 10000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 10000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 10000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 10000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 10000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 10000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 10000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 10000\n",
      "job_dict saved to job_dict_Llama31_10k.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mmmu': {10000: 10008},\n",
       " 'docvqa': {10000: 10009},\n",
       " 'mathvista': {10000: 10010},\n",
       " 'ai2d': {10000: 10011},\n",
       " 'chartqa': {10000: 10012},\n",
       " 'vqa': {10000: 10013},\n",
       " 'textvqa': {10000: 10014},\n",
       " 'infographics_w_ocr': {10000: 10015},\n",
       " 'infographics': {10000: 10016},\n",
       " 'mmbench': {10000: 10017}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_dict_lm31_10k = eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    eval_plan=\"Llama31_10k\",\n",
    "    eval_config_dir=\"/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31\",\n",
    "    checkpoint_dir=\"/fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch\",\n",
    "    checkpoints=[10000],\n",
    "    # benchmarks=[\"mathvista\"]\n",
    ")\n",
    "\n",
    "job_dict_lm31_10k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama31 - fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/fb_llama3.1 mmmu 8000\n",
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/fb_llama3.1 mmmu 10000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 docvqa 8000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 docvqa 10000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/fb_llama3.1 mathvista 8000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/fb_llama3.1 mathvista 10000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/fb_llama3.1 ai2d 8000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/fb_llama3.1 ai2d 10000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 chartqa 8000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 chartqa 10000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 vqa 8000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 vqa 10000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 textvqa 8000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 textvqa 10000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/fb_llama3.1 infographics_w_ocr 8000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/fb_llama3.1 infographics_w_ocr 10000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/fb_llama3.1 infographics 8000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/fb_llama3.1 infographics 10000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/fb_llama3.1 mmbench 8000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/fb_llama3.1 mmbench 10000\n",
      "job_dict saved to job_dict_FBL_Llama31.json\n"
     ]
    }
   ],
   "source": [
    "job_dict_fb_lm31 = eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    eval_plan=\"FBL_Llama31\",\n",
    "    eval_config_dir=\"/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31\",\n",
    "    checkpoint_dir=\"/fsx_0/checkpoints/tranx/fb_llama3.1\",\n",
    "    checkpoints=[8000, 10000],\n",
    "    # benchmarks=[\"mathvista\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MH19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10500, 11500, 12500, 13500, 14500, 15500, 16500, 17500, 18500]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoints = [10500 + i*1000 for i in range(9)]\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found existing job_dict: eval/logs/job_dict_MH19_10500_18500.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m job_dict_mh19 \u001b[38;5;241m=\u001b[39m \u001b[43meval_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_eval_plan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_base_sbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_plan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMH19_10500_18500\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_config_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/fsx_0/checkpoints/tranx/MM9-Pretrain-70B/MH19_336px_128nodes_bz32_resume\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10500\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# benchmarks=[\"mathvista\"]\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-06bc3d6b93146dddd/user/tranx/experiments/eval_helper.py:41\u001b[0m, in \u001b[0;36mrun_eval_plan\u001b[0;34m(eval_base_sbatch, eval_plan, eval_config_dir, checkpoint_dir, checkpoints, benchmarks, rerun_if_exists)\u001b[0m\n\u001b[1;32m     39\u001b[0m job_dict_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval/logs/job_dict_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_plan\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(job_dict_json) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rerun_if_exists:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound existing job_dict: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_dict_json\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m benchmarks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     benchmarks \u001b[38;5;241m=\u001b[39m ALL_BENCHMARKS\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found existing job_dict: eval/logs/job_dict_MH19_10500_18500.json"
     ]
    }
   ],
   "source": [
    "job_dict_mh19 = eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    eval_plan=\"MH19_10500_18500\",\n",
    "    eval_config_dir=\"/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval\",\n",
    "    checkpoint_dir=\"/fsx_0/checkpoints/tranx/MM9-Pretrain-70B/MH19_336px_128nodes_bz32_resume\",\n",
    "    checkpoints=[10500 + i*1000 for i in range(9)],\n",
    "    # benchmarks=[\"mathvista\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got result for mmmu - 10500\n",
      "Got result for mmmu - 11500\n",
      "Got result for mmmu - 12500\n",
      "Got result for mmmu - 13500\n",
      "Got result for mmmu - 14500\n",
      "Got result for mmmu - 15500\n",
      "Got result for mmmu - 16500\n",
      "Got result for mmmu - 17500\n",
      "Got result for mmmu - 18500\n",
      "Got result for docvqa - 10500\n",
      "Got result for docvqa - 11500\n",
      "Got result for docvqa - 12500\n",
      "Got result for docvqa - 13500\n",
      "Got result for docvqa - 14500\n",
      "Got result for docvqa - 15500\n",
      "Got result for docvqa - 16500\n",
      "Got result for docvqa - 17500\n",
      "Got result for docvqa - 18500\n",
      "Got result for mathvista - 10500\n",
      "Got result for mathvista - 11500\n",
      "Got result for mathvista - 12500\n",
      "Got result for mathvista - 13500\n",
      "Got result for mathvista - 14500\n",
      "Got result for mathvista - 15500\n",
      "Got result for mathvista - 16500\n",
      "Got result for mathvista - 17500\n",
      "Got result for mathvista - 18500\n",
      "Got result for ai2d - 10500\n",
      "Got result for ai2d - 11500\n",
      "Got result for ai2d - 12500\n",
      "Got result for ai2d - 13500\n",
      "Got result for ai2d - 14500\n",
      "Got result for ai2d - 15500\n",
      "Got result for ai2d - 16500\n",
      "Got result for ai2d - 17500\n",
      "Got result for ai2d - 18500\n",
      "Got result for chartqa - 10500\n",
      "Got result for chartqa - 11500\n",
      "Got result for chartqa - 12500\n",
      "Got result for chartqa - 13500\n",
      "Got result for chartqa - 14500\n",
      "Got result for chartqa - 15500\n",
      "Got result for chartqa - 16500\n",
      "Got result for chartqa - 17500\n",
      "Got result for chartqa - 18500\n",
      "Got result for vqa - 10500\n",
      "Got result for vqa - 11500\n",
      "Got result for vqa - 12500\n",
      "Got result for vqa - 13500\n",
      "Got result for vqa - 14500\n",
      "Got result for vqa - 15500\n",
      "Got result for vqa - 16500\n",
      "Got result for vqa - 17500\n",
      "Got result for vqa - 18500\n",
      "Got result for textvqa - 10500\n",
      "Got result for textvqa - 11500\n",
      "Got result for textvqa - 12500\n",
      "Got result for textvqa - 13500\n",
      "Got result for textvqa - 14500\n",
      "Got result for textvqa - 15500\n",
      "Got result for textvqa - 16500\n",
      "Got result for textvqa - 17500\n",
      "Got result for textvqa - 18500\n",
      "Got result for infographics_w_ocr - 10500\n",
      "Got result for infographics_w_ocr - 11500\n",
      "Got result for infographics_w_ocr - 12500\n",
      "Got result for infographics_w_ocr - 13500\n",
      "Got result for infographics_w_ocr - 14500\n",
      "Got result for infographics_w_ocr - 15500\n",
      "Got result for infographics_w_ocr - 16500\n",
      "Got result for infographics_w_ocr - 17500\n",
      "Got result for infographics_w_ocr - 18500\n",
      "Got result for infographics - 10500\n",
      "Got result for infographics - 11500\n",
      "Got result for infographics - 12500\n",
      "Got result for infographics - 13500\n",
      "Got result for infographics - 14500\n",
      "Got result for infographics - 15500\n",
      "Got result for infographics - 16500\n",
      "Got result for infographics - 17500\n",
      "Got result for infographics - 18500\n",
      "Got result for mmbench - 10500\n",
      "Got result for mmbench - 11500\n",
      "Got result for mmbench - 12500\n",
      "Got result for mmbench - 13500\n",
      "Got result for mmbench - 14500\n",
      "Got result for mmbench - 15500\n",
      "Got result for mmbench - 16500\n",
      "Got result for mmbench - 17500\n",
      "Got result for mmbench - 18500\n"
     ]
    }
   ],
   "source": [
    "df = eval_helper.get_eval_scores(job_dict_mh19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mmmu/accuracy                                    0.5878\n",
       "mmmu/mllm_eval_accuracy                          0.5889\n",
       "docvqa/anls_total_score                          0.6201\n",
       "docvqa/mllm_evaluation_anls_score                0.6216\n",
       "mathvista/accuracy                                0.377\n",
       "ai2d/accuracy                                    0.7704\n",
       "chartqa/accuracy                                 0.4914\n",
       "vqa/accuracy                                     0.7035\n",
       "vqa/mllm_evaluation_accuracy                     0.7256\n",
       "textvqa/accuracy                                 66.898\n",
       "textvqa/mllm_eval_accuracy                       71.578\n",
       "infographics_w_ocr/anls_total_score              0.6256\n",
       "infographics_w_ocr/mllm_evaluation_anls_score    0.5712\n",
       "infographics/anls_total_score                    0.4798\n",
       "infographics/mllm_evaluation_anls_score          0.4177\n",
       "mmbench/overall                                  0.7422\n",
       "Name: 17500, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[17500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m job_dict_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_dict_Llama31_336px_8500_9500.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(job_dict_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m     job_dict \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m eval_helper\u001b[38;5;241m.\u001b[39mget_eval_scores(job_dict)\n\u001b[1;32m      8\u001b[0m df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# job_dict_file = 'job_dict_Llama31_336px.json'\n",
    "job_dict_file = 'job_dict_Llama31_336px_8500_9500.json'\n",
    "\n",
    "with open(job_dict_file, 'r') as f:\n",
    "    job_dict = json.load(f)\n",
    "\n",
    "df = eval_helper.get_eval_scores(job_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got result for mmmu - 8000\n",
      "Got result for mmmu - 10000\n",
      "Got result for docvqa - 8000\n",
      "Got result for docvqa - 10000\n",
      "Got result for mathvista - 8000\n",
      "Got result for mathvista - 10000\n",
      "Got result for ai2d - 8000\n",
      "Got result for ai2d - 10000\n",
      "Got result for chartqa - 8000\n",
      "Got result for chartqa - 10000\n",
      "Got result for vqa - 8000\n",
      "Got result for vqa - 10000\n",
      "Got result for textvqa - 8000\n",
      "Got result for textvqa - 10000\n",
      "Got result for infographics_w_ocr - 8000\n",
      "Got result for infographics_w_ocr - 10000\n",
      "Got result for infographics - 8000\n",
      "Got result for infographics - 10000\n",
      "Got result for mmbench - 8000\n",
      "Got result for mmbench - 10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu/accuracy</th>\n",
       "      <th>mmmu/mllm_eval_accuracy</th>\n",
       "      <th>docvqa/anls_total_score</th>\n",
       "      <th>docvqa/mllm_evaluation_anls_score</th>\n",
       "      <th>mathvista/accuracy</th>\n",
       "      <th>ai2d/accuracy</th>\n",
       "      <th>chartqa/accuracy</th>\n",
       "      <th>vqa/accuracy</th>\n",
       "      <th>vqa/mllm_evaluation_accuracy</th>\n",
       "      <th>textvqa/accuracy</th>\n",
       "      <th>textvqa/mllm_eval_accuracy</th>\n",
       "      <th>infographics_w_ocr/anls_total_score</th>\n",
       "      <th>infographics_w_ocr/mllm_evaluation_anls_score</th>\n",
       "      <th>infographics/anls_total_score</th>\n",
       "      <th>infographics/mllm_evaluation_anls_score</th>\n",
       "      <th>mmbench/overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>0.5067</td>\n",
       "      <td>0.5156</td>\n",
       "      <td>0.5386</td>\n",
       "      <td>0.5411</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.7121</td>\n",
       "      <td>0.4746</td>\n",
       "      <td>0.6831</td>\n",
       "      <td>0.7137</td>\n",
       "      <td>59.028</td>\n",
       "      <td>65.38</td>\n",
       "      <td>0.5902</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.4384</td>\n",
       "      <td>0.3746</td>\n",
       "      <td>0.6635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.4744</td>\n",
       "      <td>0.4822</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5432</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4578</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.7224</td>\n",
       "      <td>59.414</td>\n",
       "      <td>65.708</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.5354</td>\n",
       "      <td>0.4472</td>\n",
       "      <td>0.3785</td>\n",
       "      <td>0.6772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mmmu/accuracy mmmu/mllm_eval_accuracy docvqa/anls_total_score  \\\n",
       "8000         0.5067                  0.5156                  0.5386   \n",
       "10000        0.4744                  0.4822                    0.54   \n",
       "\n",
       "      docvqa/mllm_evaluation_anls_score mathvista/accuracy ai2d/accuracy  \\\n",
       "8000                             0.5411              0.344        0.7121   \n",
       "10000                            0.5432              0.348        0.6953   \n",
       "\n",
       "      chartqa/accuracy vqa/accuracy vqa/mllm_evaluation_accuracy  \\\n",
       "8000            0.4746       0.6831                       0.7137   \n",
       "10000           0.4578       0.6931                       0.7224   \n",
       "\n",
       "      textvqa/accuracy textvqa/mllm_eval_accuracy  \\\n",
       "8000            59.028                      65.38   \n",
       "10000           59.414                     65.708   \n",
       "\n",
       "      infographics_w_ocr/anls_total_score  \\\n",
       "8000                               0.5902   \n",
       "10000                               0.592   \n",
       "\n",
       "      infographics_w_ocr/mllm_evaluation_anls_score  \\\n",
       "8000                                          0.536   \n",
       "10000                                        0.5354   \n",
       "\n",
       "      infographics/anls_total_score infographics/mllm_evaluation_anls_score  \\\n",
       "8000                         0.4384                                  0.3746   \n",
       "10000                        0.4472                                  0.3785   \n",
       "\n",
       "      mmbench/overall  \n",
       "8000           0.6635  \n",
       "10000          0.6772  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_dict_file = 'job_dict_FBL_Llama31.json'\n",
    "with open(job_dict_file, 'r') as f:\n",
    "    job_dict = json.load(f)\n",
    "\n",
    "df2 = eval_helper.get_eval_scores(job_dict)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_SBATCH = \"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\"\n",
    "# ALIGNER_CODE_DIR=\"/fsx_0/user/tranx/eval_adel\"\n",
    "# EVAL_CONFIG_DIR=f\"/fsx_0/user/tranx/eval_adel/llm_mm_aligner/experiments/aws_adel/eval\"\n",
    "\n",
    "ALIGNER_CODE_DIR=\"/fsx_0/user/tranx/adel_prod\"\n",
    "EVAL_CONFIG_DIR=f\"/fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval\"\n",
    "\n",
    "# EVAL_CONFIG_DIR=f\"/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval\"\n",
    "\n",
    "# STAGE2_OUTPUT_DIR=\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New checkpoints: []\n"
     ]
    }
   ],
   "source": [
    "eval_helper.run_eval_sweep(\n",
    "    output_dir=f\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp\",\n",
    "    eval_sbatch=EVAL_SBATCH,\n",
    "    eval_config_dir=EVAL_CONFIG_DIR,\n",
    "    aligner_parent_dir=ALIGNER_CODE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2600, 2800, 3000, 3200, 3400, 3600, 3800, 4000, 4200, 4400, 4600, 4800]\n",
      "Got result for mmmu - 200: [('mmmu/accounting', 0.4), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.23333333333333334), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.9333333333333333), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.26666666666666666), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.43333333333333335), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.3333333333333333), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.5333333333333333), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5444444444444444), ('mmmu/mllm_eval_accuracy', 0.5555555555555556)]\n",
      "Got result for docvqa - 200: [('docvqa/anls_total_score', 0.648974586635455), ('docvqa/mllm_evaluation_anls_score', 0.6481197547746367), ('docvqa/mmllm_fixed_anls_score', 0.6838029270024617)]\n",
      "Got result for mathvista - 200: [('mathvista/accuracy', 0.391)]\n",
      "Got result for ai2d - 200: [('ai2d/accuracy', 0.7937176165803109)]\n",
      "Got result for chartqa - 200: [('chartqa/accuracy', 0.49371597896623964)]\n",
      "Got result for vqa - 200: [('vqa/accuracy', 0.7255479999999764), ('vqa/recall', 0.7492599999999743), ('vqa/bleu', 0.020937373861670494), ('vqa/mllm_evaluation_accuracy', 0.7450439999999751)]\n",
      "Got result for textvqa - 200: [('textvqa/accuracy', 68.12600000000033), ('textvqa/mllm_eval_accuracy', 73.26600000000035)]\n",
      "Got result for infographics_w_ocr - 200: [('infographics_w_ocr/anls_total_score', 0.6409916230210374), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5854567964169172), ('infographics_w_ocr/answer_type_multi_span_score', 0.5338974316374145), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6330233359166454), ('infographics_w_ocr/answer_type_question_span_score', 0.7330070539063752), ('infographics_w_ocr/answer_type_single_span_score', 0.6500951211141338), ('infographics_w_ocr/evidence_type_figure_score', 0.6175575898411356), ('infographics_w_ocr/evidence_type_map_score', 0.5759932724041634), ('infographics_w_ocr/evidence_type_table_list_score', 0.6430477870563691), ('infographics_w_ocr/evidence_type_text_score', 0.6880197894970036), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5443827499957463), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6244129158512719), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5369375807695697), ('infographics_w_ocr/reasoning_type_counting_score', 0.6284965034965034)]\n",
      "Got result for infographics - 200: [('infographics/anls_total_score', 0.5379493460605843), ('infographics/mllm_evaluation_anls_score', 0.47600775196257794), ('infographics/answer_type_multi_span_score', 0.36748425796360557), ('infographics/answer_type_non_extractive_score', 0.5428836383628429), ('infographics/answer_type_question_span_score', 0.7130050505050506), ('infographics/answer_type_single_span_score', 0.5466798770418363), ('infographics/evidence_type_figure_score', 0.5290105259537133), ('infographics/evidence_type_map_score', 0.4711856346711625), ('infographics/evidence_type_table_list_score', 0.4989027652359333), ('infographics/evidence_type_text_score', 0.5788382727884572), ('infographics/evidence_type_visual_layout_score', 0.4924818133769208), ('infographics/reasoning_type_arithmetic_score', 0.49103191329218726), ('infographics/reasoning_type_comparison_score', 0.446195245471771), ('infographics/reasoning_type_counting_score', 0.5816433566433565)]\n",
      "Got result for mmbench - 200: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.575), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.9166666666666666), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.4444444444444444), ('mmbench/ocr', 0.8461538461538461), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.25), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7380514196021993)]\n",
      "200\n",
      "Got result for mmmu - 400: [('mmmu/accounting', 0.6), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.43333333333333335), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.6), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5655555555555556), ('mmmu/mllm_eval_accuracy', 0.57)]\n",
      "Got result for docvqa - 400: [('docvqa/anls_total_score', 0.6534120102355975), ('docvqa/mllm_evaluation_anls_score', 0.6513522658103867), ('docvqa/mmllm_fixed_anls_score', 0.6844828999693582)]\n",
      "Got result for mathvista - 400: [('mathvista/accuracy', 0.399)]\n",
      "Got result for ai2d - 400: [('ai2d/accuracy', 0.7895077720207254)]\n",
      "Got result for chartqa - 400: [('chartqa/accuracy', 0.49791647438025527)]\n",
      "Got result for vqa - 400: [('vqa/accuracy', 0.7258479999999763), ('vqa/recall', 0.7517559999999741), ('vqa/bleu', 0.0), ('vqa/mllm_evaluation_accuracy', 0.7470839999999748)]\n",
      "Got result for textvqa - 400: [('textvqa/accuracy', 68.66400000000034), ('textvqa/mllm_eval_accuracy', 73.45800000000037)]\n",
      "Got result for infographics_w_ocr - 400: [('infographics_w_ocr/anls_total_score', 0.6333954609425488), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5798452893172775), ('infographics_w_ocr/answer_type_multi_span_score', 0.505847540654381), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6253293722552313), ('infographics_w_ocr/answer_type_question_span_score', 0.7112121821115034), ('infographics_w_ocr/answer_type_single_span_score', 0.6460722512370143), ('infographics_w_ocr/evidence_type_figure_score', 0.6117414086165958), ('infographics_w_ocr/evidence_type_map_score', 0.5532590759075908), ('infographics_w_ocr/evidence_type_table_list_score', 0.6334879754404233), ('infographics_w_ocr/evidence_type_text_score', 0.6731628556221999), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5600575002516679), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.609271037181996), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5240481182359088), ('infographics_w_ocr/reasoning_type_counting_score', 0.6273310023310023)]\n",
      "Got result for infographics - 400: [('infographics/anls_total_score', 0.5391218184507696), ('infographics/mllm_evaluation_anls_score', 0.47887276038929505), ('infographics/answer_type_multi_span_score', 0.38638430186891565), ('infographics/answer_type_non_extractive_score', 0.5447543866621627), ('infographics/answer_type_question_span_score', 0.6992306304806305), ('infographics/answer_type_single_span_score', 0.546695121686073), ('infographics/evidence_type_figure_score', 0.5284556051924668), ('infographics/evidence_type_map_score', 0.4789636273660133), ('infographics/evidence_type_table_list_score', 0.5067757725482563), ('infographics/evidence_type_text_score', 0.5869814975451305), ('infographics/evidence_type_visual_layout_score', 0.4864893326090637), ('infographics/reasoning_type_arithmetic_score', 0.5110702368407846), ('infographics/reasoning_type_comparison_score', 0.4765089285749754), ('infographics/reasoning_type_counting_score', 0.5652680652680652)]\n",
      "Got result for mmbench - 400: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7352971577832469)]\n",
      "400\n",
      "Got result for mmmu - 600: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.5333333333333333), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.6333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.5666666666666667), ('mmmu/geography', 0.5), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.43333333333333335), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.4), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.5333333333333333), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5433333333333333), ('mmmu/mllm_eval_accuracy', 0.5566666666666666)]\n",
      "Got result for docvqa - 600: [('docvqa/anls_total_score', 0.6575888089569821), ('docvqa/mllm_evaluation_anls_score', 0.657276969036617), ('docvqa/mmllm_fixed_anls_score', 0.6929927830043519)]\n",
      "Got result for mathvista - 600: [('mathvista/accuracy', 0.379)]\n",
      "Got result for ai2d - 600: [('ai2d/accuracy', 0.7946891191709845)]\n",
      "Got result for chartqa - 600: [('chartqa/accuracy', 0.49772606244006784)]\n",
      "Got result for vqa - 600: [('vqa/accuracy', 0.7261719999999774), ('vqa/recall', 0.7539559999999752), ('vqa/bleu', 0.03368200361728668), ('vqa/mllm_evaluation_accuracy', 0.7489759999999759)]\n",
      "Got result for textvqa - 600: [('textvqa/accuracy', 68.3380000000003), ('textvqa/mllm_eval_accuracy', 72.94600000000037)]\n",
      "Got result for infographics_w_ocr - 600: [('infographics_w_ocr/anls_total_score', 0.645120098458749), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5922845273478874), ('infographics_w_ocr/answer_type_multi_span_score', 0.5371055480168933), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6376241148845129), ('infographics_w_ocr/answer_type_question_span_score', 0.7228597257443411), ('infographics_w_ocr/answer_type_single_span_score', 0.655476301762224), ('infographics_w_ocr/evidence_type_figure_score', 0.6327841762111491), ('infographics_w_ocr/evidence_type_map_score', 0.5816038334602691), ('infographics_w_ocr/evidence_type_table_list_score', 0.6337104729359286), ('infographics_w_ocr/evidence_type_text_score', 0.6790192802343173), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5583750370514137), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6263908751066284), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5450205332919241), ('infographics_w_ocr/reasoning_type_counting_score', 0.631118881118881)]\n",
      "Got result for infographics - 600: [('infographics/anls_total_score', 0.5387890745367289), ('infographics/mllm_evaluation_anls_score', 0.47306837557220427), ('infographics/answer_type_multi_span_score', 0.40358057113388285), ('infographics/answer_type_non_extractive_score', 0.542125535706006), ('infographics/answer_type_question_span_score', 0.6984407953157952), ('infographics/answer_type_single_span_score', 0.5456866014396378), ('infographics/evidence_type_figure_score', 0.5318784259066155), ('infographics/evidence_type_map_score', 0.48573335274703944), ('infographics/evidence_type_table_list_score', 0.5046150246320811), ('infographics/evidence_type_text_score', 0.5690155534867277), ('infographics/evidence_type_visual_layout_score', 0.47527225655468797), ('infographics/reasoning_type_arithmetic_score', 0.4932491595162826), ('infographics/reasoning_type_comparison_score', 0.4520703552380929), ('infographics/reasoning_type_counting_score', 0.5745920745920745)]\n",
      "Got result for mmbench - 600: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5121951219512195), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7369986371617516)]\n",
      "600\n",
      "Got result for mmmu - 800: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.7333333333333333), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.4), ('mmmu/geography', 0.43333333333333335), ('mmmu/history', 0.7), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.5333333333333333), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5622222222222222), ('mmmu/mllm_eval_accuracy', 0.5755555555555556)]\n",
      "Got result for docvqa - 800: [('docvqa/anls_total_score', 0.6861651868027577), ('docvqa/mllm_evaluation_anls_score', 0.6849665255082817), ('docvqa/mmllm_fixed_anls_score', 0.720501167940428)]\n",
      "Got result for mathvista - 800: [('mathvista/accuracy', 0.435)]\n",
      "Got result for ai2d - 800: [('ai2d/accuracy', 0.8257772020725389)]\n",
      "Got result for chartqa - 800: [('chartqa/accuracy', 0.5386423595754996)]\n",
      "Got result for vqa - 800: [('vqa/accuracy', 0.7527439999999767), ('vqa/recall', 0.7740319999999759), ('vqa/bleu', 0.04545086994767189), ('vqa/mllm_evaluation_accuracy', 0.7709879999999767)]\n",
      "Got result for textvqa - 800: [('textvqa/accuracy', 70.53000000000036), ('textvqa/mllm_eval_accuracy', 74.64600000000038)]\n",
      "Got result for infographics_w_ocr - 800: [('infographics_w_ocr/anls_total_score', 0.6472951114150778), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5973177554748575), ('infographics_w_ocr/answer_type_multi_span_score', 0.503323993986201), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6466639398375387), ('infographics_w_ocr/answer_type_question_span_score', 0.7170577254570468), ('infographics_w_ocr/answer_type_single_span_score', 0.6575469899729094), ('infographics_w_ocr/evidence_type_figure_score', 0.6270001994179981), ('infographics_w_ocr/evidence_type_map_score', 0.608501523229246), ('infographics_w_ocr/evidence_type_table_list_score', 0.6534260886396096), ('infographics_w_ocr/evidence_type_text_score', 0.692856945110573), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5523783679965781), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.652300771906936), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5411386472815425), ('infographics_w_ocr/reasoning_type_counting_score', 0.6284965034965035)]\n",
      "Got result for infographics - 800: [('infographics/anls_total_score', 0.5584713034221711), ('infographics/mllm_evaluation_anls_score', 0.4991246766993003), ('infographics/answer_type_multi_span_score', 0.3821692950630614), ('infographics/answer_type_non_extractive_score', 0.5402885139774832), ('infographics/answer_type_question_span_score', 0.694036465671081), ('infographics/answer_type_single_span_score', 0.5761809310517206), ('infographics/evidence_type_figure_score', 0.5486501418344766), ('infographics/evidence_type_map_score', 0.5589467317772501), ('infographics/evidence_type_table_list_score', 0.524121350581692), ('infographics/evidence_type_text_score', 0.5997578810480703), ('infographics/evidence_type_visual_layout_score', 0.5009458084050376), ('infographics/reasoning_type_arithmetic_score', 0.4844048455349825), ('infographics/reasoning_type_comparison_score', 0.4813541596300644), ('infographics/reasoning_type_counting_score', 0.5786713286713286)]\n",
      "Got result for mmbench - 800: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.9166666666666666), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6753246753246753), ('mmbench/overall', 0.750688446377466)]\n",
      "800\n",
      "Got result for mmmu - 1000: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5333333333333333), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.6), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.4666666666666667), ('mmmu/energy_and_power', 0.36666666666666664), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.43333333333333335), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.6333333333333333), ('mmmu/materials', 0.4), ('mmmu/math', 0.6), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5511111111111111), ('mmmu/mllm_eval_accuracy', 0.5666666666666667)]\n",
      "Got result for docvqa - 1000: [('docvqa/anls_total_score', 0.6914809452898901), ('docvqa/mllm_evaluation_anls_score', 0.6910337685765978), ('docvqa/mmllm_fixed_anls_score', 0.7266098512822747)]\n",
      "Got result for mathvista - 1000: [('mathvista/accuracy', 0.441)]\n",
      "Got result for ai2d - 1000: [('ai2d/accuracy', 0.8228626943005182)]\n",
      "Got result for chartqa - 1000: [('chartqa/accuracy', 0.5446625354709164)]\n",
      "Got result for vqa - 1000: [('vqa/accuracy', 0.7517119999999763), ('vqa/recall', 0.7744439999999746), ('vqa/bleu', 0.03847002610564232), ('vqa/mllm_evaluation_accuracy', 0.7715879999999756)]\n",
      "Got result for textvqa - 1000: [('textvqa/accuracy', 70.04600000000036), ('textvqa/mllm_eval_accuracy', 74.15000000000038)]\n",
      "Got result for infographics_w_ocr - 1000: [('infographics_w_ocr/anls_total_score', 0.6547973502072989), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6048048545226773), ('infographics_w_ocr/answer_type_multi_span_score', 0.503602583214263), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6551580125721177), ('infographics_w_ocr/answer_type_question_span_score', 0.7178612519958674), ('infographics_w_ocr/answer_type_single_span_score', 0.6665106562611337), ('infographics_w_ocr/evidence_type_figure_score', 0.6354380330971015), ('infographics_w_ocr/evidence_type_map_score', 0.5891945925361767), ('infographics_w_ocr/evidence_type_table_list_score', 0.6538581681693623), ('infographics_w_ocr/evidence_type_text_score', 0.7021856039991057), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5823333066147026), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.65662915851272), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5496088289389683), ('infographics_w_ocr/reasoning_type_counting_score', 0.6491841491841491)]\n",
      "Got result for infographics - 1000: [('infographics/anls_total_score', 0.5557274631323967), ('infographics/mllm_evaluation_anls_score', 0.48987016150522056), ('infographics/answer_type_multi_span_score', 0.35456009292459895), ('infographics/answer_type_non_extractive_score', 0.5457338923885037), ('infographics/answer_type_question_span_score', 0.6946469662815817), ('infographics/answer_type_single_span_score', 0.5732867615153577), ('infographics/evidence_type_figure_score', 0.5433856843943352), ('infographics/evidence_type_map_score', 0.5117018932850822), ('infographics/evidence_type_table_list_score', 0.5286447006472307), ('infographics/evidence_type_text_score', 0.5924922026101045), ('infographics/evidence_type_visual_layout_score', 0.5158001118642461), ('infographics/reasoning_type_arithmetic_score', 0.49825630990014547), ('infographics/reasoning_type_comparison_score', 0.47883552954855163), ('infographics/reasoning_type_counting_score', 0.5812937062937062)]\n",
      "Got result for mmbench - 1000: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.5185185185185185), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7557467484518534)]\n",
      "1000\n",
      "Got result for mmmu - 1200: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.5), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.5666666666666667), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.6333333333333333), ('mmmu/marketing', 0.4), ('mmmu/materials', 0.26666666666666666), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5777777777777777), ('mmmu/mllm_eval_accuracy', 0.5833333333333334)]\n",
      "Got result for docvqa - 1200: [('docvqa/anls_total_score', 0.688159004463097), ('docvqa/mllm_evaluation_anls_score', 0.687887849937589), ('docvqa/mmllm_fixed_anls_score', 0.7248432274205141)]\n",
      "Got result for mathvista - 1200: [('mathvista/accuracy', 0.443)]\n",
      "Got result for ai2d - 1200: [('ai2d/accuracy', 0.8251295336787565)]\n",
      "Got result for chartqa - 1200: [('chartqa/accuracy', 0.5338041688877015)]\n",
      "Got result for vqa - 1200: [('vqa/accuracy', 0.7530399999999744), ('vqa/recall', 0.7722879999999732), ('vqa/bleu', 0.042081840336322784), ('vqa/mllm_evaluation_accuracy', 0.7701319999999744)]\n",
      "Got result for textvqa - 1200: [('textvqa/accuracy', 69.95600000000036), ('textvqa/mllm_eval_accuracy', 74.09000000000042)]\n",
      "Got result for infographics_w_ocr - 1200: [('infographics_w_ocr/anls_total_score', 0.6508986976368093), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5996206615245506), ('infographics_w_ocr/answer_type_multi_span_score', 0.5430357056045166), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6591065053090372), ('infographics_w_ocr/answer_type_question_span_score', 0.7079765288758502), ('infographics_w_ocr/answer_type_single_span_score', 0.6569890842073937), ('infographics_w_ocr/evidence_type_figure_score', 0.6324564858914928), ('infographics_w_ocr/evidence_type_map_score', 0.6080064737242955), ('infographics_w_ocr/evidence_type_table_list_score', 0.646020786828472), ('infographics_w_ocr/evidence_type_text_score', 0.6924020368555358), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5810923917569475), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6589699683877763), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5452584973158334), ('infographics_w_ocr/reasoning_type_counting_score', 0.6386946386946387)]\n",
      "Got result for infographics - 1200: [('infographics/anls_total_score', 0.5538923675620057), ('infographics/mllm_evaluation_anls_score', 0.49112106346003614), ('infographics/answer_type_multi_span_score', 0.39007474218426597), ('infographics/answer_type_non_extractive_score', 0.5509604620816195), ('infographics/answer_type_question_span_score', 0.7153276744622898), ('infographics/answer_type_single_span_score', 0.5647906983169795), ('infographics/evidence_type_figure_score', 0.54006393378161), ('infographics/evidence_type_map_score', 0.49796591220988223), ('infographics/evidence_type_table_list_score', 0.5145635853743483), ('infographics/evidence_type_text_score', 0.6015344858227378), ('infographics/evidence_type_visual_layout_score', 0.5387917275282756), ('infographics/reasoning_type_arithmetic_score', 0.5091819709970394), ('infographics/reasoning_type_comparison_score', 0.4477963166018653), ('infographics/reasoning_type_counting_score', 0.5775058275058275)]\n",
      "Got result for mmbench - 1200: [('mmbench/attribute_comparison', 0.6590909090909091), ('mmbench/attribute_recognition', 0.9014084507042254), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.7494843283432574)]\n",
      "1200\n",
      "Got result for mmmu - 1400: [('mmmu/accounting', 0.43333333333333335), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.7333333333333333), ('mmmu/basic_medical_science', 0.5666666666666667), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.3333333333333333), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.4666666666666667), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.4666666666666667), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.5666666666666667), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5577777777777778), ('mmmu/mllm_eval_accuracy', 0.5633333333333334)]\n",
      "Got result for docvqa - 1400: [('docvqa/anls_total_score', 0.6576122486567785), ('docvqa/mllm_evaluation_anls_score', 0.6566612892873913), ('docvqa/mmllm_fixed_anls_score', 0.6934407806760226)]\n",
      "Got result for mathvista - 1400: [('mathvista/accuracy', 0.39)]\n",
      "Got result for ai2d - 1400: [('ai2d/accuracy', 0.8014896373056994)]\n",
      "Got result for chartqa - 1400: [('chartqa/accuracy', 0.5053164478963773)]\n",
      "Got result for vqa - 1400: [('vqa/accuracy', 0.7249079999999761), ('vqa/recall', 0.7543519999999746), ('vqa/bleu', 0.030893689021468163), ('vqa/mllm_evaluation_accuracy', 0.7496639999999758)]\n",
      "Got result for textvqa - 1400: [('textvqa/accuracy', 68.46800000000027), ('textvqa/mllm_eval_accuracy', 73.00600000000034)]\n",
      "Got result for infographics_w_ocr - 1400: [('infographics_w_ocr/anls_total_score', 0.6369640423630668), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5829732966226111), ('infographics_w_ocr/answer_type_multi_span_score', 0.532606442211179), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6343472832170846), ('infographics_w_ocr/answer_type_question_span_score', 0.7184717526063681), ('infographics_w_ocr/answer_type_single_span_score', 0.6437379416417038), ('infographics_w_ocr/evidence_type_figure_score', 0.6145073401890329), ('infographics_w_ocr/evidence_type_map_score', 0.5446401370906321), ('infographics_w_ocr/evidence_type_table_list_score', 0.6425165028947857), ('infographics_w_ocr/evidence_type_text_score', 0.6775024510267813), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5382672591430826), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6234955968688843), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5446248559956286), ('infographics_w_ocr/reasoning_type_counting_score', 0.6243006993006992)]\n",
      "Got result for infographics - 1400: [('infographics/anls_total_score', 0.544698776916589), ('infographics/mllm_evaluation_anls_score', 0.4768804440485371), ('infographics/answer_type_multi_span_score', 0.3876663178392642), ('infographics/answer_type_non_extractive_score', 0.537036411448708), ('infographics/answer_type_question_span_score', 0.6925609113109114), ('infographics/answer_type_single_span_score', 0.5579228911168002), ('infographics/evidence_type_figure_score', 0.5337600881331508), ('infographics/evidence_type_map_score', 0.46992766503541117), ('infographics/evidence_type_table_list_score', 0.5152377475690411), ('infographics/evidence_type_text_score', 0.5866106648369201), ('infographics/evidence_type_visual_layout_score', 0.5032870488300436), ('infographics/reasoning_type_arithmetic_score', 0.49987831802900295), ('infographics/reasoning_type_comparison_score', 0.4659806705776409), ('infographics/reasoning_type_counting_score', 0.562062937062937)]\n",
      "Got result for mmbench - 1400: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.4567901234567901), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7389200432584532)]\n",
      "1400\n",
      "Got result for mmmu - 1600: [('mmmu/accounting', 0.5666666666666667), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.7333333333333333), ('mmmu/electronics', 0.26666666666666666), ('mmmu/energy_and_power', 0.6333333333333333), ('mmmu/finance', 0.5), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.43333333333333335), ('mmmu/materials', 0.4), ('mmmu/math', 0.6666666666666666), ('mmmu/mechanical_engineering', 0.3), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5677777777777778), ('mmmu/mllm_eval_accuracy', 0.5811111111111111)]\n",
      "Got result for docvqa - 1600: [('docvqa/anls_total_score', 0.6633950645141165), ('docvqa/mllm_evaluation_anls_score', 0.6621732866379921), ('docvqa/mmllm_fixed_anls_score', 0.7018963353076103)]\n",
      "Got result for mathvista - 1600: [('mathvista/accuracy', 0.387)]\n",
      "Got result for ai2d - 1600: [('ai2d/accuracy', 0.8018134715025906)]\n",
      "Got result for chartqa - 1600: [('chartqa/accuracy', 0.501776176087941)]\n",
      "Got result for vqa - 1600: [('vqa/accuracy', 0.7266159999999767), ('vqa/recall', 0.7553039999999749), ('vqa/bleu', 0.022225230932235718), ('vqa/mllm_evaluation_accuracy', 0.7506439999999759)]\n",
      "Got result for textvqa - 1600: [('textvqa/accuracy', 68.8460000000003), ('textvqa/mllm_eval_accuracy', 73.40600000000033)]\n",
      "Got result for infographics_w_ocr - 1600: [('infographics_w_ocr/anls_total_score', 0.6457023973979295), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5932751674743095), ('infographics_w_ocr/answer_type_multi_span_score', 0.5497372595047608), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6206169809696033), ('infographics_w_ocr/answer_type_question_span_score', 0.7205321921668075), ('infographics_w_ocr/answer_type_single_span_score', 0.660572165359513), ('infographics_w_ocr/evidence_type_figure_score', 0.6278178162645915), ('infographics_w_ocr/evidence_type_map_score', 0.594640137090632), ('infographics_w_ocr/evidence_type_table_list_score', 0.642145557871301), ('infographics_w_ocr/evidence_type_text_score', 0.6875644335225235), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5671063387468899), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6081090998043052), ('infographics_w_ocr/reasoning_type_comparison_score', 0.552772060569731), ('infographics_w_ocr/reasoning_type_counting_score', 0.6180069930069929)]\n",
      "Got result for infographics - 1600: [('infographics/anls_total_score', 0.5495695703528087), ('infographics/mllm_evaluation_anls_score', 0.484123202352065), ('infographics/answer_type_multi_span_score', 0.4011947975827871), ('infographics/answer_type_non_extractive_score', 0.5340847017158049), ('infographics/answer_type_question_span_score', 0.681049452684068), ('infographics/answer_type_single_span_score', 0.5635314091849846), ('infographics/evidence_type_figure_score', 0.5365480551777553), ('infographics/evidence_type_map_score', 0.5282933046250156), ('infographics/evidence_type_table_list_score', 0.5134098914630114), ('infographics/evidence_type_text_score', 0.5994608136374032), ('infographics/evidence_type_visual_layout_score', 0.5114665083623571), ('infographics/reasoning_type_arithmetic_score', 0.48055158312007623), ('infographics/reasoning_type_comparison_score', 0.4764306825263266), ('infographics/reasoning_type_counting_score', 0.5786713286713286)]\n",
      "Got result for mmbench - 1600: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8354430379746836), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8490566037735849), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6753246753246753), ('mmbench/overall', 0.7391074399707203)]\n",
      "1600\n",
      "Got result for mmmu - 1800: [('mmmu/accounting', 0.5333333333333333), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.4666666666666667), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.6333333333333333), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.6), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.4), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.8), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.57), ('mmmu/mllm_eval_accuracy', 0.5822222222222222)]\n",
      "Got result for docvqa - 1800: [('docvqa/anls_total_score', 0.6843588119133742), ('docvqa/mllm_evaluation_anls_score', 0.684458344039418), ('docvqa/mmllm_fixed_anls_score', 0.7224850768908115)]\n",
      "Got result for mathvista - 1800: [('mathvista/accuracy', 0.479)]\n",
      "Got result for ai2d - 1800: [('ai2d/accuracy', 0.8173575129533679)]\n",
      "Got result for vqa - 1800: [('vqa/accuracy', 0.7503119999999752), ('vqa/recall', 0.7713479999999748), ('vqa/bleu', 0.037375714629888535), ('vqa/mllm_evaluation_accuracy', 0.7684439999999758)]\n",
      "Got result for textvqa - 1800: [('textvqa/accuracy', 70.31600000000034), ('textvqa/mllm_eval_accuracy', 74.32400000000037)]\n",
      "Got result for infographics_w_ocr - 1800: [('infographics_w_ocr/anls_total_score', 0.6526072837441547), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5975676709033039), ('infographics_w_ocr/answer_type_multi_span_score', 0.5427162487344895), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6393067450933637), ('infographics_w_ocr/answer_type_question_span_score', 0.714310472709794), ('infographics_w_ocr/answer_type_single_span_score', 0.664294212037485), ('infographics_w_ocr/evidence_type_figure_score', 0.634466514054076), ('infographics_w_ocr/evidence_type_map_score', 0.6252231953964628), ('infographics_w_ocr/evidence_type_table_list_score', 0.6469956922890664), ('infographics_w_ocr/evidence_type_text_score', 0.7085295462550012), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5564597474059932), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6558332078880021), ('infographics_w_ocr/reasoning_type_comparison_score', 0.542371694441681), ('infographics_w_ocr/reasoning_type_counting_score', 0.6127622377622378)]\n",
      "Got result for infographics - 1800: [('infographics/anls_total_score', 0.5585906774640167), ('infographics/mllm_evaluation_anls_score', 0.4935091271938477), ('infographics/answer_type_multi_span_score', 0.3778451767180989), ('infographics/answer_type_non_extractive_score', 0.5463894353586939), ('infographics/answer_type_question_span_score', 0.7421515450361604), ('infographics/answer_type_single_span_score', 0.5719845867047859), ('infographics/evidence_type_figure_score', 0.545387545278541), ('infographics/evidence_type_map_score', 0.5535764732659879), ('infographics/evidence_type_table_list_score', 0.5195161862322201), ('infographics/evidence_type_text_score', 0.5951663128244008), ('infographics/evidence_type_visual_layout_score', 0.5096500748563842), ('infographics/reasoning_type_arithmetic_score', 0.5097717731279374), ('infographics/reasoning_type_comparison_score', 0.48293097685132386), ('infographics/reasoning_type_counting_score', 0.5772144522144521)]\n",
      "Got result for mmbench - 1800: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.5), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6103896103896104), ('mmbench/overall', 0.7516339808169767)]\n",
      "1800\n",
      "Got result for mmmu - 2000: [('mmmu/accounting', 0.4), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.3333333333333333), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.6333333333333333), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.4), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.7333333333333333), ('mmmu/public_health', 0.7), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.5433333333333333), ('mmmu/mllm_eval_accuracy', 0.5522222222222222)]\n",
      "Got result for docvqa - 2000: [('docvqa/anls_total_score', 0.6627813263594035), ('docvqa/mllm_evaluation_anls_score', 0.6624657918881379), ('docvqa/mmllm_fixed_anls_score', 0.7000551475450792)]\n",
      "Got result for mathvista - 2000: [('mathvista/accuracy', 0.378)]\n",
      "Got result for ai2d - 2000: [('ai2d/accuracy', 0.8021373056994818)]\n",
      "Got result for chartqa - 2000: [('chartqa/accuracy', 0.5066379600544395)]\n",
      "Got result for vqa - 2000: [('vqa/accuracy', 0.7267079999999778), ('vqa/recall', 0.755859999999976), ('vqa/bleu', 0.028023675084114075), ('vqa/mllm_evaluation_accuracy', 0.7509839999999772)]\n",
      "Got result for textvqa - 2000: [('textvqa/accuracy', 68.4740000000003), ('textvqa/mllm_eval_accuracy', 72.94000000000037)]\n",
      "Got result for infographics_w_ocr - 2000: [('infographics_w_ocr/anls_total_score', 0.6420000592264764), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5881253633181263), ('infographics_w_ocr/answer_type_multi_span_score', 0.538551872230699), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6317661241711875), ('infographics_w_ocr/answer_type_question_span_score', 0.6880230346576501), ('infographics_w_ocr/answer_type_single_span_score', 0.6557812322531061), ('infographics_w_ocr/evidence_type_figure_score', 0.6238483673800576), ('infographics_w_ocr/evidence_type_map_score', 0.6055312261995429), ('infographics_w_ocr/evidence_type_table_list_score', 0.6355094520789897), ('infographics_w_ocr/evidence_type_text_score', 0.6921671264690452), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.545479832830823), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6246575342465752), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5360197634885258), ('infographics_w_ocr/reasoning_type_counting_score', 0.6349067599067598)]\n",
      "Got result for infographics - 2000: [('infographics/anls_total_score', 0.5559116590884859), ('infographics/mllm_evaluation_anls_score', 0.4922647155064431), ('infographics/answer_type_multi_span_score', 0.40190802622300553), ('infographics/answer_type_non_extractive_score', 0.5323262833208585), ('infographics/answer_type_question_span_score', 0.7382339882339882), ('infographics/answer_type_single_span_score', 0.5707760379116904), ('infographics/evidence_type_figure_score', 0.5461863530518095), ('infographics/evidence_type_map_score', 0.4773830324208892), ('infographics/evidence_type_table_list_score', 0.5306038912986368), ('infographics/evidence_type_text_score', 0.5889429538375155), ('infographics/evidence_type_visual_layout_score', 0.5069370005363462), ('infographics/reasoning_type_arithmetic_score', 0.5054558265174702), ('infographics/reasoning_type_comparison_score', 0.47156348737507503), ('infographics/reasoning_type_counting_score', 0.5577505827505826)]\n",
      "Got result for mmbench - 2000: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7360026953942295)]\n",
      "2000\n",
      "Got result for mmmu - 2200: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.6333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.3333333333333333), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.9), ('mmmu/manage', 0.6), ('mmmu/marketing', 0.43333333333333335), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7333333333333333), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5655555555555556), ('mmmu/mllm_eval_accuracy', 0.5744444444444444)]\n",
      "Got result for docvqa - 2200: [('docvqa/anls_total_score', 0.6694143120287874), ('docvqa/mllm_evaluation_anls_score', 0.6698830748144646), ('docvqa/mmllm_fixed_anls_score', 0.7089631949147517)]\n",
      "Got result for mathvista - 2200: [('mathvista/accuracy', 0.394)]\n",
      "Got result for ai2d - 2200: [('ai2d/accuracy', 0.7943652849740933)]\n",
      "Got result for chartqa - 2200: [('chartqa/accuracy', 0.5129263856993226)]\n",
      "Got result for vqa - 2200: [('vqa/accuracy', 0.7266359999999765), ('vqa/recall', 0.7551799999999745), ('vqa/bleu', 0.03049882873892784), ('vqa/mllm_evaluation_accuracy', 0.7500119999999759)]\n",
      "Got result for textvqa - 2200: [('textvqa/accuracy', 68.84600000000026), ('textvqa/mllm_eval_accuracy', 73.47200000000032)]\n",
      "Got result for infographics_w_ocr - 2200: [('infographics_w_ocr/anls_total_score', 0.6432614946945413), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5898985995137185), ('infographics_w_ocr/answer_type_multi_span_score', 0.5609760024596901), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6271207996343621), ('infographics_w_ocr/answer_type_question_span_score', 0.7017118480998673), ('infographics_w_ocr/answer_type_single_span_score', 0.6541986604117707), ('infographics_w_ocr/evidence_type_figure_score', 0.6196211386623298), ('infographics_w_ocr/evidence_type_map_score', 0.604873586906202), ('infographics_w_ocr/evidence_type_table_list_score', 0.6486229501939967), ('infographics_w_ocr/evidence_type_text_score', 0.6843432069801682), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5501802028066695), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6098098248783178), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5325424540774624), ('infographics_w_ocr/reasoning_type_counting_score', 0.6267482517482517)]\n",
      "Got result for infographics - 2200: [('infographics/anls_total_score', 0.5439055755378869), ('infographics/mllm_evaluation_anls_score', 0.4804403199543547), ('infographics/answer_type_multi_span_score', 0.40435471419347996), ('infographics/answer_type_non_extractive_score', 0.5377988196252211), ('infographics/answer_type_question_span_score', 0.7116529304029304), ('infographics/answer_type_single_span_score', 0.5534635948693101), ('infographics/evidence_type_figure_score', 0.5323806197783365), ('infographics/evidence_type_map_score', 0.5120273875579316), ('infographics/evidence_type_table_list_score', 0.5094437698041304), ('infographics/evidence_type_text_score', 0.5876053942462758), ('infographics/evidence_type_visual_layout_score', 0.4987959308190131), ('infographics/reasoning_type_arithmetic_score', 0.4924751618244768), ('infographics/reasoning_type_comparison_score', 0.47829183989204593), ('infographics/reasoning_type_counting_score', 0.5775641025641025)]\n",
      "Got result for mmbench - 2200: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8227848101265823), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9423076923076923), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.4583333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.7421294421341338)]\n",
      "2200\n",
      "Got result for mmmu - 2400: [('mmmu/accounting', 0.5666666666666667), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.6), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.7333333333333333), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.5666666666666667), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.4), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.7666666666666667), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.57), ('mmmu/mllm_eval_accuracy', 0.5755555555555556)]\n",
      "Got result for docvqa - 2400: [('docvqa/anls_total_score', 0.6676134044250891), ('docvqa/mllm_evaluation_anls_score', 0.6670305691017008), ('docvqa/mmllm_fixed_anls_score', 0.7057330541748089)]\n",
      "Got result for mathvista - 2400: [('mathvista/accuracy', 0.398)]\n",
      "Got result for ai2d - 2400: [('ai2d/accuracy', 0.7979274611398963)]\n",
      "Got result for chartqa - 2400: [('chartqa/accuracy', 0.5218296375019621)]\n",
      "Got result for vqa - 2400: [('vqa/accuracy', 0.7270279999999776), ('vqa/recall', 0.7573719999999755), ('vqa/bleu', 0.02922874130308628), ('vqa/mllm_evaluation_accuracy', 0.751963999999977)]\n",
      "Got result for textvqa - 2400: [('textvqa/accuracy', 68.84400000000032), ('textvqa/mllm_eval_accuracy', 73.39000000000034)]\n",
      "Got result for infographics_w_ocr - 2400: [('infographics_w_ocr/anls_total_score', 0.6400734706551193), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.585863259324377), ('infographics_w_ocr/answer_type_multi_span_score', 0.5417920903366263), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6338675489670067), ('infographics_w_ocr/answer_type_question_span_score', 0.7080475785158653), ('infographics_w_ocr/answer_type_single_span_score', 0.6491220125524187), ('infographics_w_ocr/evidence_type_figure_score', 0.6199140081783565), ('infographics_w_ocr/evidence_type_map_score', 0.5909161160398784), ('infographics_w_ocr/evidence_type_table_list_score', 0.6407164745338039), ('infographics_w_ocr/evidence_type_text_score', 0.6857923617404651), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5831775879892245), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6181350042651409), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5459358815316889), ('infographics_w_ocr/reasoning_type_counting_score', 0.636072261072261)]\n",
      "Got result for infographics - 2400: [('infographics/anls_total_score', 0.544973853529598), ('infographics/mllm_evaluation_anls_score', 0.4838577817721142), ('infographics/answer_type_multi_span_score', 0.38867847013850343), ('infographics/answer_type_non_extractive_score', 0.5411728014802157), ('infographics/answer_type_question_span_score', 0.7359868763714917), ('infographics/answer_type_single_span_score', 0.553158868574983), ('infographics/evidence_type_figure_score', 0.5362464378933626), ('infographics/evidence_type_map_score', 0.5250805894542943), ('infographics/evidence_type_table_list_score', 0.513031272764928), ('infographics/evidence_type_text_score', 0.5837412142641485), ('infographics/evidence_type_visual_layout_score', 0.5033652960913895), ('infographics/reasoning_type_arithmetic_score', 0.484367211935705), ('infographics/reasoning_type_comparison_score', 0.46061815255837635), ('infographics/reasoning_type_counting_score', 0.5921328671328671)]\n",
      "Got result for mmbench - 2400: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.4444444444444444), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7414569084930678)]\n",
      "2400\n",
      "Got result for mmmu - 2600: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.7), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.5333333333333333), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.5333333333333333), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.3), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.4666666666666667), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.6), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.7666666666666667), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.7333333333333333), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.56), ('mmmu/mllm_eval_accuracy', 0.5755555555555556)]\n",
      "Got result for docvqa - 2600: [('docvqa/anls_total_score', 0.6672832913320215), ('docvqa/mllm_evaluation_anls_score', 0.6673006278030491), ('docvqa/mmllm_fixed_anls_score', 0.7044682330535644)]\n",
      "Got result for mathvista - 2600: [('mathvista/accuracy', 0.399)]\n",
      "Got result for ai2d - 2600: [('ai2d/accuracy', 0.8031088082901554)]\n",
      "Got result for chartqa - 2600: [('chartqa/accuracy', 0.5075724195093866)]\n",
      "Got result for vqa - 2600: [('vqa/accuracy', 0.7276079999999777), ('vqa/recall', 0.7578119999999756), ('vqa/bleu', 0.025044534355401993), ('vqa/mllm_evaluation_accuracy', 0.752243999999977)]\n",
      "Got result for textvqa - 2600: [('textvqa/accuracy', 69.08600000000031), ('textvqa/mllm_eval_accuracy', 73.62400000000038)]\n",
      "Got result for infographics_w_ocr - 2600: [('infographics_w_ocr/anls_total_score', 0.6391358859067168), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.58720405521593), ('infographics_w_ocr/answer_type_multi_span_score', 0.5183272630795799), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6373589942306038), ('infographics_w_ocr/answer_type_question_span_score', 0.6850774930457799), ('infographics_w_ocr/answer_type_single_span_score', 0.650204492794494), ('infographics_w_ocr/evidence_type_figure_score', 0.618197567547635), ('infographics_w_ocr/evidence_type_map_score', 0.567577430820005), ('infographics_w_ocr/evidence_type_table_list_score', 0.634900000476396), ('infographics_w_ocr/evidence_type_text_score', 0.6902275560916012), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5841190209004171), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6293134377038485), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5314698440141242), ('infographics_w_ocr/reasoning_type_counting_score', 0.629079254079254)]\n",
      "Got result for infographics - 2600: [('infographics/anls_total_score', 0.5483576725059027), ('infographics/mllm_evaluation_anls_score', 0.4845595042568303), ('infographics/answer_type_multi_span_score', 0.38210103376855065), ('infographics/answer_type_non_extractive_score', 0.545565977121131), ('infographics/answer_type_question_span_score', 0.6999177371773525), ('infographics/answer_type_single_span_score', 0.5598667452952026), ('infographics/evidence_type_figure_score', 0.5339724209563811), ('infographics/evidence_type_map_score', 0.5298752669384585), ('infographics/evidence_type_table_list_score', 0.513928773891564), ('infographics/evidence_type_text_score', 0.5853495720663191), ('infographics/evidence_type_visual_layout_score', 0.49361744114562295), ('infographics/reasoning_type_arithmetic_score', 0.511979401876662), ('infographics/reasoning_type_comparison_score', 0.46496719694322264), ('infographics/reasoning_type_counting_score', 0.5664918414918414)]\n",
      "Got result for mmbench - 2600: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.575), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4444444444444444), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6883116883116883), ('mmbench/overall', 0.7422790001338214)]\n",
      "2600\n",
      "Got result for mmmu - 2800: [('mmmu/accounting', 0.6), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.23333333333333334), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.5), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.6333333333333333), ('mmmu/finance', 0.4), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.6), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.2), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.6), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5544444444444444), ('mmmu/mllm_eval_accuracy', 0.5588888888888889)]\n",
      "Got result for docvqa - 2800: [('docvqa/anls_total_score', 0.66395975782975), ('docvqa/mllm_evaluation_anls_score', 0.6646201677106403), ('docvqa/mmllm_fixed_anls_score', 0.7016689256113126)]\n",
      "Got result for mathvista - 2800: [('mathvista/accuracy', 0.404)]\n",
      "Got result for ai2d - 2800: [('ai2d/accuracy', 0.7914507772020726)]\n",
      "Got result for chartqa - 2800: [('chartqa/accuracy', 0.49694777699724874)]\n",
      "Got result for vqa - 2800: [('vqa/accuracy', 0.7272559999999783), ('vqa/recall', 0.7581439999999756), ('vqa/bleu', 0.026664312928915024), ('vqa/mllm_evaluation_accuracy', 0.7524279999999774)]\n",
      "Got result for textvqa - 2800: [('textvqa/accuracy', 69.33200000000035), ('textvqa/mllm_eval_accuracy', 73.94000000000035)]\n",
      "Got result for infographics_w_ocr - 2800: [('infographics_w_ocr/anls_total_score', 0.6432594786147546), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5899130470744622), ('infographics_w_ocr/answer_type_multi_span_score', 0.5223361370497703), ('infographics_w_ocr/answer_type_non_extractive_score', 0.630910981724725), ('infographics_w_ocr/answer_type_question_span_score', 0.6947714347250156), ('infographics_w_ocr/answer_type_single_span_score', 0.6575083871676799), ('infographics_w_ocr/evidence_type_figure_score', 0.6260361349563158), ('infographics_w_ocr/evidence_type_map_score', 0.5789635694338664), ('infographics_w_ocr/evidence_type_table_list_score', 0.6410390694843162), ('infographics_w_ocr/evidence_type_text_score', 0.6852052266417392), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.561785131836383), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6281750213257061), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5453501433473905), ('infographics_w_ocr/reasoning_type_counting_score', 0.6264568764568765)]\n",
      "Got result for infographics - 2800: [('infographics/anls_total_score', 0.5571266134294298), ('infographics/mllm_evaluation_anls_score', 0.4941745670638031), ('infographics/answer_type_multi_span_score', 0.3930490997398355), ('infographics/answer_type_non_extractive_score', 0.5392835063269061), ('infographics/answer_type_question_span_score', 0.705545789680405), ('infographics/answer_type_single_span_score', 0.5731622156604242), ('infographics/evidence_type_figure_score', 0.5451085548933411), ('infographics/evidence_type_map_score', 0.5133124468633476), ('infographics/evidence_type_table_list_score', 0.5221442105952693), ('infographics/evidence_type_text_score', 0.6002041240064658), ('infographics/evidence_type_visual_layout_score', 0.5164415606965644), ('infographics/reasoning_type_arithmetic_score', 0.4932321198588321), ('infographics/reasoning_type_comparison_score', 0.4763705210815633), ('infographics/reasoning_type_counting_score', 0.5743589743589743)]\n",
      "Got result for mmbench - 2800: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.741590612788053)]\n",
      "2800\n",
      "3000\n",
      "Got result for mmmu - 3200: [('mmmu/accounting', 0.6333333333333333), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.6), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.3333333333333333), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.5), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.5), ('mmmu/math', 0.5666666666666667), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.7666666666666667), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5611111111111111), ('mmmu/mllm_eval_accuracy', 0.5688888888888889)]\n",
      "Got result for docvqa - 3200: [('docvqa/anls_total_score', 0.6640880634124493), ('docvqa/mllm_evaluation_anls_score', 0.6634133266479652), ('docvqa/mmllm_fixed_anls_score', 0.7021403504963396)]\n",
      "Got result for mathvista - 3200: [('mathvista/accuracy', 0.379)]\n",
      "Got result for ai2d - 3200: [('ai2d/accuracy', 0.7908031088082902)]\n",
      "Got result for chartqa - 3200: [('chartqa/accuracy', 0.5030746512008956)]\n",
      "Got result for vqa - 3200: [('vqa/accuracy', 0.7282199999999778), ('vqa/recall', 0.7576399999999741), ('vqa/bleu', 0.023279016837477684), ('vqa/mllm_evaluation_accuracy', 0.7522039999999759)]\n",
      "Got result for textvqa - 3200: [('textvqa/accuracy', 69.40600000000035), ('textvqa/mllm_eval_accuracy', 74.03000000000036)]\n",
      "Got result for infographics_w_ocr - 3200: [('infographics_w_ocr/anls_total_score', 0.6429400579120222), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5884253631895616), ('infographics_w_ocr/answer_type_multi_span_score', 0.5402503683724129), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6324377852406786), ('infographics_w_ocr/answer_type_question_span_score', 0.6962243921237135), ('infographics_w_ocr/answer_type_single_span_score', 0.6538366030276421), ('infographics_w_ocr/evidence_type_figure_score', 0.6214902776544455), ('infographics_w_ocr/evidence_type_map_score', 0.5762054364777137), ('infographics_w_ocr/evidence_type_table_list_score', 0.6436928996343314), ('infographics_w_ocr/evidence_type_text_score', 0.6936491906854457), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5767280602473487), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6210779517286364), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5125325401030927), ('infographics_w_ocr/reasoning_type_counting_score', 0.6277972027972027)]\n",
      "Got result for infographics - 3200: [('infographics/anls_total_score', 0.5442911927818022), ('infographics/mllm_evaluation_anls_score', 0.48164092110639845), ('infographics/answer_type_multi_span_score', 0.36575683119083485), ('infographics/answer_type_non_extractive_score', 0.5436266165018426), ('infographics/answer_type_question_span_score', 0.6816990879490878), ('infographics/answer_type_single_span_score', 0.5579889843295365), ('infographics/evidence_type_figure_score', 0.5314863415557626), ('infographics/evidence_type_map_score', 0.5370796835781139), ('infographics/evidence_type_table_list_score', 0.5050513875051371), ('infographics/evidence_type_text_score', 0.5819960900001305), ('infographics/evidence_type_visual_layout_score', 0.5087834108404902), ('infographics/reasoning_type_arithmetic_score', 0.5032837406125076), ('infographics/reasoning_type_comparison_score', 0.4798490257466455), ('infographics/reasoning_type_counting_score', 0.5682400932400932)]\n",
      "Got result for mmbench - 3200: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9191919191919192), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7392020300278188)]\n",
      "3200\n",
      "Got result for mmmu - 3400: [('mmmu/accounting', 0.36666666666666664), ('mmmu/agriculture', 0.5), ('mmmu/architecture_and_engineering', 0.5), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.7333333333333333), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.7), ('mmmu/computer_science', 0.7333333333333333), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.5), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.6666666666666666), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.36666666666666664), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5622222222222222), ('mmmu/mllm_eval_accuracy', 0.5766666666666667)]\n",
      "Got result for docvqa - 3400: [('docvqa/anls_total_score', 0.6662842594918789), ('docvqa/mllm_evaluation_anls_score', 0.6652847094750739), ('docvqa/mmllm_fixed_anls_score', 0.7038201111199454)]\n",
      "Got result for mathvista - 3400: [('mathvista/accuracy', 0.393)]\n",
      "Got result for ai2d - 3400: [('ai2d/accuracy', 0.7924222797927462)]\n",
      "Got result for chartqa - 3400: [('chartqa/accuracy', 0.5096414068533369)]\n",
      "Got result for vqa - 3400: [('vqa/accuracy', 0.7282879999999772), ('vqa/recall', 0.7565879999999746), ('vqa/bleu', 0.0), ('vqa/mllm_evaluation_accuracy', 0.7515359999999767)]\n",
      "Got result for textvqa - 3400: [('textvqa/accuracy', 68.54800000000031), ('textvqa/mllm_eval_accuracy', 73.23600000000035)]\n",
      "Got result for infographics_w_ocr - 3400: [('infographics_w_ocr/anls_total_score', 0.6471130009735296), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5970165747874483), ('infographics_w_ocr/answer_type_multi_span_score', 0.5277120512481788), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6390618272625507), ('infographics_w_ocr/answer_type_question_span_score', 0.6982436569472379), ('infographics_w_ocr/answer_type_single_span_score', 0.659930244634008), ('infographics_w_ocr/evidence_type_figure_score', 0.6248130697058113), ('infographics_w_ocr/evidence_type_map_score', 0.5709190149784209), ('infographics_w_ocr/evidence_type_table_list_score', 0.6465013479258935), ('infographics_w_ocr/evidence_type_text_score', 0.6959895225650007), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5823225535681384), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.624204990215264), ('infographics_w_ocr/reasoning_type_comparison_score', 0.541245895871541), ('infographics_w_ocr/reasoning_type_counting_score', 0.6452797202797204)]\n",
      "Got result for infographics - 3400: [('infographics/anls_total_score', 0.5471520650683339), ('infographics/mllm_evaluation_anls_score', 0.4829317131228439), ('infographics/answer_type_multi_span_score', 0.4026163000965545), ('infographics/answer_type_non_extractive_score', 0.5401711609668212), ('infographics/answer_type_question_span_score', 0.7024932866279021), ('infographics/answer_type_single_span_score', 0.5578557144446681), ('infographics/evidence_type_figure_score', 0.5357241352483342), ('infographics/evidence_type_map_score', 0.49143320011514335), ('infographics/evidence_type_table_list_score', 0.5112195185734529), ('infographics/evidence_type_text_score', 0.5846294022832487), ('infographics/evidence_type_visual_layout_score', 0.5070947987109717), ('infographics/reasoning_type_arithmetic_score', 0.49051821466205014), ('infographics/reasoning_type_comparison_score', 0.48506441211224355), ('infographics/reasoning_type_counting_score', 0.580128205128205)]\n",
      "Got result for mmbench - 3400: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4567901234567901), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6753246753246753), ('mmbench/overall', 0.7446592804291632)]\n",
      "3400\n",
      "Got result for mmmu - 3600: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.6), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.4666666666666667), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8), ('mmmu/literature', 0.9), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.6), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.7), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5455555555555556), ('mmmu/mllm_eval_accuracy', 0.5466666666666666)]\n",
      "Got result for docvqa - 3600: [('docvqa/anls_total_score', 0.6673320053456222), ('docvqa/mllm_evaluation_anls_score', 0.66831914101801), ('docvqa/mmllm_fixed_anls_score', 0.7068702981118987)]\n",
      "Got result for mathvista - 3600: [('mathvista/accuracy', 0.39)]\n",
      "Got result for ai2d - 3600: [('ai2d/accuracy', 0.8069948186528497)]\n",
      "Got result for chartqa - 3600: [('chartqa/accuracy', 0.5095766623262165)]\n",
      "Got result for vqa - 3600: [('vqa/accuracy', 0.7248559999999767), ('vqa/recall', 0.7542519999999742), ('vqa/bleu', 0.027971595525741577), ('vqa/mllm_evaluation_accuracy', 0.7488479999999759)]\n",
      "Got result for textvqa - 3600: [('textvqa/accuracy', 68.49600000000034), ('textvqa/mllm_eval_accuracy', 73.19000000000035)]\n",
      "Got result for infographics_w_ocr - 3600: [('infographics_w_ocr/anls_total_score', 0.6484489445356911), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5965838510927669), ('infographics_w_ocr/answer_type_multi_span_score', 0.5357902204494556), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6404180659605616), ('infographics_w_ocr/answer_type_question_span_score', 0.7011794466477335), ('infographics_w_ocr/answer_type_single_span_score', 0.6604707771186143), ('infographics_w_ocr/evidence_type_figure_score', 0.6247968013105916), ('infographics_w_ocr/evidence_type_map_score', 0.5896708901659397), ('infographics_w_ocr/evidence_type_table_list_score', 0.6551548544784905), ('infographics_w_ocr/evidence_type_text_score', 0.6966182941099536), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5722394099010877), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6259743966079581), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5448196517831402), ('infographics_w_ocr/reasoning_type_counting_score', 0.6404428904428904)]\n",
      "Got result for infographics - 3600: [('infographics/anls_total_score', 0.5496827239760246), ('infographics/mllm_evaluation_anls_score', 0.4876385734970513), ('infographics/answer_type_multi_span_score', 0.3991648633927753), ('infographics/answer_type_non_extractive_score', 0.5463191118706491), ('infographics/answer_type_question_span_score', 0.6709304050650204), ('infographics/answer_type_single_span_score', 0.5620574935011181), ('infographics/evidence_type_figure_score', 0.5381051174079604), ('infographics/evidence_type_map_score', 0.5299723184988182), ('infographics/evidence_type_table_list_score', 0.5186647897424309), ('infographics/evidence_type_text_score', 0.5916394584698487), ('infographics/evidence_type_visual_layout_score', 0.4699430454156177), ('infographics/reasoning_type_arithmetic_score', 0.4979376787595964), ('infographics/reasoning_type_comparison_score', 0.49499427146429703), ('infographics/reasoning_type_counting_score', 0.5825174825174824)]\n",
      "Got result for mmbench - 3600: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7708333333333334), ('mmbench/object_localization', 0.4444444444444444), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7346576364351671)]\n",
      "3600\n",
      "Got result for mmmu - 3800: [('mmmu/accounting', 0.4), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.3333333333333333), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.6666666666666666), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.5666666666666667), ('mmmu/finance', 0.4), ('mmmu/geography', 0.6), ('mmmu/history', 0.8), ('mmmu/literature', 0.9), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.5), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7), ('mmmu/sociology', 0.7666666666666667), ('mmmu/accuracy', 0.5677777777777778), ('mmmu/mllm_eval_accuracy', 0.5844444444444444)]\n",
      "Got result for docvqa - 3800: [('docvqa/anls_total_score', 0.6616529263076446), ('docvqa/mllm_evaluation_anls_score', 0.6633989390774641), ('docvqa/mmllm_fixed_anls_score', 0.7019972711244351)]\n",
      "Got result for mathvista - 3800: [('mathvista/accuracy', 0.387)]\n",
      "Got result for ai2d - 3800: [('ai2d/accuracy', 0.7924222797927462)]\n",
      "Got result for chartqa - 3800: [('chartqa/accuracy', 0.5189239580937337)]\n",
      "Got result for vqa - 3800: [('vqa/accuracy', 0.7256639999999778), ('vqa/recall', 0.7551119999999752), ('vqa/bleu', 0.021624036133289337), ('vqa/mllm_evaluation_accuracy', 0.7498759999999771)]\n",
      "Got result for textvqa - 3800: [('textvqa/accuracy', 68.29400000000028), ('textvqa/mllm_eval_accuracy', 72.97200000000034)]\n",
      "Got result for infographics_w_ocr - 3800: [('infographics_w_ocr/anls_total_score', 0.6465777910239343), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5956844353866315), ('infographics_w_ocr/answer_type_multi_span_score', 0.5321523182587027), ('infographics_w_ocr/answer_type_non_extractive_score', 0.638779815723758), ('infographics_w_ocr/answer_type_question_span_score', 0.6960717669710882), ('infographics_w_ocr/answer_type_single_span_score', 0.6598077568921017), ('infographics_w_ocr/evidence_type_figure_score', 0.627244964614269), ('infographics_w_ocr/evidence_type_map_score', 0.5873794110180248), ('infographics_w_ocr/evidence_type_table_list_score', 0.6430205201144769), ('infographics_w_ocr/evidence_type_text_score', 0.7007880602575256), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5689540112345787), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6256115459882582), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5353694962307665), ('infographics_w_ocr/reasoning_type_counting_score', 0.6344988344988345)]\n",
      "Got result for infographics - 3800: [('infographics/anls_total_score', 0.5462979824952026), ('infographics/mllm_evaluation_anls_score', 0.4831643531581884), ('infographics/answer_type_multi_span_score', 0.3811784973782347), ('infographics/answer_type_non_extractive_score', 0.5436857897979058), ('infographics/answer_type_question_span_score', 0.6914723631069785), ('infographics/answer_type_single_span_score', 0.5585001179081762), ('infographics/evidence_type_figure_score', 0.5339068782328559), ('infographics/evidence_type_map_score', 0.5076392933410988), ('infographics/evidence_type_table_list_score', 0.505617724731024), ('infographics/evidence_type_text_score', 0.5899017580794799), ('infographics/evidence_type_visual_layout_score', 0.5166286708060807), ('infographics/reasoning_type_arithmetic_score', 0.5059642982588186), ('infographics/reasoning_type_comparison_score', 0.4820025249966273), ('infographics/reasoning_type_counting_score', 0.5829170829170829)]\n",
      "Got result for mmbench - 3800: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4567901234567901), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6133333333333333), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7414173634937057)]\n",
      "3800\n",
      "Got result for mmmu - 4000: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.23333333333333334), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.7333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.3), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.6), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.4), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.5333333333333333), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.55), ('mmmu/mllm_eval_accuracy', 0.5655555555555556)]\n",
      "Got result for docvqa - 4000: [('docvqa/anls_total_score', 0.6638353050311572), ('docvqa/mllm_evaluation_anls_score', 0.6635608945241366), ('docvqa/mmllm_fixed_anls_score', 0.7007195639643756)]\n",
      "Got result for mathvista - 4000: [('mathvista/accuracy', 0.397)]\n",
      "Got result for ai2d - 4000: [('ai2d/accuracy', 0.7908031088082902)]\n",
      "Got result for chartqa - 4000: [('chartqa/accuracy', 0.5120386010834749)]\n",
      "Got result for vqa - 4000: [('vqa/accuracy', 0.7281759999999776), ('vqa/recall', 0.7565479999999756), ('vqa/bleu', 0.023881062865257263), ('vqa/mllm_evaluation_accuracy', 0.751255999999977)]\n",
      "Got result for textvqa - 4000: [('textvqa/accuracy', 68.79800000000034), ('textvqa/mllm_eval_accuracy', 73.24600000000038)]\n",
      "Got result for infographics_w_ocr - 4000: [('infographics_w_ocr/anls_total_score', 0.639619658756057), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5871979608324914), ('infographics_w_ocr/answer_type_multi_span_score', 0.5167015009137605), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6394733355854516), ('infographics_w_ocr/answer_type_question_span_score', 0.6793956824647361), ('infographics_w_ocr/answer_type_single_span_score', 0.6514077794543481), ('infographics_w_ocr/evidence_type_figure_score', 0.6222197764554883), ('infographics_w_ocr/evidence_type_map_score', 0.586554328509774), ('infographics_w_ocr/evidence_type_table_list_score', 0.6389060223586145), ('infographics_w_ocr/evidence_type_text_score', 0.6818067502022553), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.53589167955617), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6249272417080635), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5264517082179011), ('infographics_w_ocr/reasoning_type_counting_score', 0.6286713286713287)]\n",
      "Got result for infographics - 4000: [('infographics/anls_total_score', 0.553377349607767), ('infographics/mllm_evaluation_anls_score', 0.49012452921508914), ('infographics/answer_type_multi_span_score', 0.3789344164972565), ('infographics/answer_type_non_extractive_score', 0.5550825003808731), ('infographics/answer_type_question_span_score', 0.6676841854726471), ('infographics/answer_type_single_span_score', 0.5671087354420487), ('infographics/evidence_type_figure_score', 0.5460545267846331), ('infographics/evidence_type_map_score', 0.5520251055486932), ('infographics/evidence_type_table_list_score', 0.5221322629807379), ('infographics/evidence_type_text_score', 0.5897991623267063), ('infographics/evidence_type_visual_layout_score', 0.5085972289449961), ('infographics/reasoning_type_arithmetic_score', 0.5058582969541873), ('infographics/reasoning_type_comparison_score', 0.45285362534681767), ('infographics/reasoning_type_counting_score', 0.5944638694638694)]\n",
      "Got result for mmbench - 4000: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8354430379746836), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7394341674884854)]\n",
      "4000\n",
      "Got result for mmmu - 4200: [('mmmu/accounting', 0.5333333333333333), ('mmmu/agriculture', 0.5333333333333333), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.5333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.6666666666666666), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.5333333333333333), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.5), ('mmmu/sociology', 0.5666666666666667), ('mmmu/accuracy', 0.5444444444444444), ('mmmu/mllm_eval_accuracy', 0.5411111111111111)]\n",
      "Got result for docvqa - 4200: [('docvqa/anls_total_score', 0.6654590780706102), ('docvqa/mllm_evaluation_anls_score', 0.6634939321224607), ('docvqa/mmllm_fixed_anls_score', 0.7014731985007547)]\n",
      "Got result for mathvista - 4200: [('mathvista/accuracy', 0.385)]\n",
      "Got result for ai2d - 4200: [('ai2d/accuracy', 0.7943652849740933)]\n",
      "Got result for chartqa - 4200: [('chartqa/accuracy', 0.5135012984250816)]\n",
      "Got result for vqa - 4200: [('vqa/accuracy', 0.7239439999999766), ('vqa/recall', 0.7538879999999742), ('vqa/bleu', 0.023069756105542183), ('vqa/mllm_evaluation_accuracy', 0.7478439999999751)]\n",
      "Got result for textvqa - 4200: [('textvqa/accuracy', 69.11800000000038), ('textvqa/mllm_eval_accuracy', 73.70000000000041)]\n",
      "Got result for infographics_w_ocr - 4200: [('infographics_w_ocr/anls_total_score', 0.6480012644357671), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5943138508713856), ('infographics_w_ocr/answer_type_multi_span_score', 0.5395363560727504), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6394816154309827), ('infographics_w_ocr/answer_type_question_span_score', 0.7004215838209051), ('infographics_w_ocr/answer_type_single_span_score', 0.6603548033746832), ('infographics_w_ocr/evidence_type_figure_score', 0.6262296275973861), ('infographics_w_ocr/evidence_type_map_score', 0.5936500380807311), ('infographics_w_ocr/evidence_type_table_list_score', 0.6547058701066608), ('infographics_w_ocr/evidence_type_text_score', 0.6926244806151663), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5633052172612127), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6367579908675798), ('infographics_w_ocr/reasoning_type_comparison_score', 0.544022030878238), ('infographics_w_ocr/reasoning_type_counting_score', 0.6311188811188811)]\n",
      "Got result for infographics - 4200: [('infographics/anls_total_score', 0.5425830774462975), ('infographics/mllm_evaluation_anls_score', 0.4797971188207303), ('infographics/answer_type_multi_span_score', 0.3900616811709289), ('infographics/answer_type_non_extractive_score', 0.544308434181852), ('infographics/answer_type_question_span_score', 0.6514284326784328), ('infographics/answer_type_single_span_score', 0.5543465593999105), ('infographics/evidence_type_figure_score', 0.5320936165918101), ('infographics/evidence_type_map_score', 0.5010386332750922), ('infographics/evidence_type_table_list_score', 0.49799591553747263), ('infographics/evidence_type_text_score', 0.5837858283938127), ('infographics/evidence_type_visual_layout_score', 0.514470924619566), ('infographics/reasoning_type_arithmetic_score', 0.5099973656480504), ('infographics/reasoning_type_comparison_score', 0.44134443464461104), ('infographics/reasoning_type_counting_score', 0.5716783216783217)]\n",
      "Got result for mmbench - 4200: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.575), ('mmbench/image_quality', 0.5121951219512195), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6753246753246753), ('mmbench/overall', 0.7373213318922573)]\n",
      "4200\n",
      "Got result for mmmu - 4400: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.4666666666666667), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.9), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.5333333333333333), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.3333333333333333), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5), ('mmmu/history', 0.8), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.4), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.2), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5477777777777778), ('mmmu/mllm_eval_accuracy', 0.55)]\n",
      "Got result for docvqa - 4400: [('docvqa/anls_total_score', 0.6577835036688473), ('docvqa/mllm_evaluation_anls_score', 0.6582541095077417), ('docvqa/mmllm_fixed_anls_score', 0.6963264761710404)]\n",
      "Got result for mathvista - 4400: [('mathvista/accuracy', 0.384)]\n",
      "Got result for ai2d - 4400: [('ai2d/accuracy', 0.8005181347150259)]\n",
      "Got result for chartqa - 4400: [('chartqa/accuracy', 0.4991762741209335)]\n",
      "Got result for vqa - 4400: [('vqa/accuracy', 0.7234039999999777), ('vqa/recall', 0.7527559999999749), ('vqa/bleu', 0.01938352733850479), ('vqa/mllm_evaluation_accuracy', 0.7470599999999753)]\n",
      "Got result for textvqa - 4400: [('textvqa/accuracy', 68.61400000000032), ('textvqa/mllm_eval_accuracy', 73.15600000000038)]\n",
      "Got result for infographics_w_ocr - 4400: [('infographics_w_ocr/anls_total_score', 0.6441712683161956), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.592151199201444), ('infographics_w_ocr/answer_type_multi_span_score', 0.542195857955229), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6345314269816984), ('infographics_w_ocr/answer_type_question_span_score', 0.6972164556157769), ('infographics_w_ocr/answer_type_single_span_score', 0.6570649688924293), ('infographics_w_ocr/evidence_type_figure_score', 0.6223226352659003), ('infographics_w_ocr/evidence_type_map_score', 0.5568513582127443), ('infographics_w_ocr/evidence_type_table_list_score', 0.6485196267161363), ('infographics_w_ocr/evidence_type_text_score', 0.6850861006639555), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5905931652218812), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6239584901399967), ('infographics_w_ocr/reasoning_type_comparison_score', 0.553092497102961), ('infographics_w_ocr/reasoning_type_counting_score', 0.6328671328671329)]\n",
      "Got result for infographics - 4400: [('infographics/anls_total_score', 0.5427382446906195), ('infographics/mllm_evaluation_anls_score', 0.4805578932362299), ('infographics/answer_type_multi_span_score', 0.3943458312960511), ('infographics/answer_type_non_extractive_score', 0.5389742264968306), ('infographics/answer_type_question_span_score', 0.67802747466209), ('infographics/answer_type_single_span_score', 0.5535217133635124), ('infographics/evidence_type_figure_score', 0.5281696747337464), ('infographics/evidence_type_map_score', 0.5429002416156735), ('infographics/evidence_type_table_list_score', 0.5099792044432347), ('infographics/evidence_type_text_score', 0.5845648038746332), ('infographics/evidence_type_visual_layout_score', 0.5284417721023548), ('infographics/reasoning_type_arithmetic_score', 0.4996098650208238), ('infographics/reasoning_type_comparison_score', 0.455277659407749), ('infographics/reasoning_type_counting_score', 0.5635198135198136)]\n",
      "Got result for mmbench - 4400: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6103896103896104), ('mmbench/overall', 0.7404643794534864)]\n",
      "4400\n",
      "Got result for mmmu - 4600: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.5), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.3), ('mmmu/energy_and_power', 0.36666666666666664), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.6333333333333333), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.9), ('mmmu/manage', 0.6), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.5), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.5333333333333333), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5655555555555556), ('mmmu/mllm_eval_accuracy', 0.5833333333333334)]\n",
      "Got result for docvqa - 4600: [('docvqa/anls_total_score', 0.6649139118450663), ('docvqa/mllm_evaluation_anls_score', 0.6664944150599198), ('docvqa/mmllm_fixed_anls_score', 0.7036872840615839)]\n",
      "Got result for mathvista - 4600: [('mathvista/accuracy', 0.396)]\n",
      "Got result for ai2d - 4600: [('ai2d/accuracy', 0.7865932642487047)]\n",
      "Got result for chartqa - 4600: [('chartqa/accuracy', 0.5067031887108536)]\n",
      "Got result for vqa - 4600: [('vqa/accuracy', 0.7231519999999776), ('vqa/recall', 0.752971999999975), ('vqa/bleu', 0.028394801542162895), ('vqa/mllm_evaluation_accuracy', 0.7475439999999761)]\n",
      "Got result for textvqa - 4600: [('textvqa/accuracy', 68.3180000000003), ('textvqa/mllm_eval_accuracy', 72.77000000000035)]\n",
      "Got result for infographics_w_ocr - 4600: [('infographics_w_ocr/anls_total_score', 0.6433603004791333), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5912851946288605), ('infographics_w_ocr/answer_type_multi_span_score', 0.5177825851123312), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6307091190906744), ('infographics_w_ocr/answer_type_question_span_score', 0.6975598622091835), ('infographics_w_ocr/answer_type_single_span_score', 0.6566383586330448), ('infographics_w_ocr/evidence_type_figure_score', 0.6236438921927718), ('infographics_w_ocr/evidence_type_map_score', 0.5573464077176947), ('infographics_w_ocr/evidence_type_table_list_score', 0.6365635749629702), ('infographics_w_ocr/evidence_type_text_score', 0.6895555646007121), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5868064096004332), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6095278864970644), ('infographics_w_ocr/reasoning_type_comparison_score', 0.538524967067676), ('infographics_w_ocr/reasoning_type_counting_score', 0.6416083916083916)]\n",
      "Got result for infographics - 4600: [('infographics/anls_total_score', 0.5465704977363636), ('infographics/mllm_evaluation_anls_score', 0.48339642069589106), ('infographics/answer_type_multi_span_score', 0.3552702863567235), ('infographics/answer_type_non_extractive_score', 0.5460481953248681), ('infographics/answer_type_question_span_score', 0.7087509178855332), ('infographics/answer_type_single_span_score', 0.5580794874915083), ('infographics/evidence_type_figure_score', 0.5357241784906924), ('infographics/evidence_type_map_score', 0.551946224034168), ('infographics/evidence_type_table_list_score', 0.5176125767609223), ('infographics/evidence_type_text_score', 0.5782280019500535), ('infographics/evidence_type_visual_layout_score', 0.49255177096170427), ('infographics/reasoning_type_arithmetic_score', 0.5163173014200411), ('infographics/reasoning_type_comparison_score', 0.46498154371650674), ('infographics/reasoning_type_counting_score', 0.5685314685314685)]\n",
      "Got result for mmbench - 4600: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4444444444444444), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7417084303855573)]\n",
      "4600\n",
      "Got result for mmmu - 4800: [('mmmu/accounting', 0.4), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.43333333333333335), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.5), ('mmmu/clinical_medicine', 0.5333333333333333), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.7), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.6666666666666666), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.6), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5811111111111111), ('mmmu/mllm_eval_accuracy', 0.5911111111111111)]\n",
      "Got result for docvqa - 4800: [('docvqa/anls_total_score', 0.6656203103508205), ('docvqa/mllm_evaluation_anls_score', 0.6646199881328083), ('docvqa/mmllm_fixed_anls_score', 0.7012822552137186)]\n",
      "Got result for mathvista - 4800: [('mathvista/accuracy', 0.384)]\n",
      "Got result for ai2d - 4800: [('ai2d/accuracy', 0.7891839378238342)]\n",
      "Got result for chartqa - 4800: [('chartqa/accuracy', 0.5180198972882273)]\n",
      "Got result for vqa - 4800: [('vqa/accuracy', 0.7249759999999768), ('vqa/recall', 0.7533599999999744), ('vqa/bleu', 0.030800124630331993), ('vqa/mllm_evaluation_accuracy', 0.7481919999999755)]\n",
      "Got result for textvqa - 4800: [('textvqa/accuracy', 68.86200000000034), ('textvqa/mllm_eval_accuracy', 73.45800000000037)]\n",
      "Got result for infographics_w_ocr - 4800: [('infographics_w_ocr/anls_total_score', 0.6461024126003606), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.593693098418034), ('infographics_w_ocr/answer_type_multi_span_score', 0.5505649040052891), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6318737621630931), ('infographics_w_ocr/answer_type_question_span_score', 0.6885572226918381), ('infographics_w_ocr/answer_type_single_span_score', 0.6594201869657167), ('infographics_w_ocr/evidence_type_figure_score', 0.6292872406604259), ('infographics_w_ocr/evidence_type_map_score', 0.6053662096978929), ('infographics_w_ocr/evidence_type_table_list_score', 0.6403314300016312), ('infographics_w_ocr/evidence_type_text_score', 0.701623764565011), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5297610549227327), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6311399217221133), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5328399597858798), ('infographics_w_ocr/reasoning_type_counting_score', 0.6175990675990678)]\n",
      "Got result for infographics - 4800: [('infographics/anls_total_score', 0.5529616122127399), ('infographics/mllm_evaluation_anls_score', 0.48861416586293815), ('infographics/answer_type_multi_span_score', 0.3598815257435469), ('infographics/answer_type_non_extractive_score', 0.5428128732830362), ('infographics/answer_type_question_span_score', 0.6904867746213901), ('infographics/answer_type_single_span_score', 0.5702777400696709), ('infographics/evidence_type_figure_score', 0.5439963428025235), ('infographics/evidence_type_map_score', 0.5443871989008856), ('infographics/evidence_type_table_list_score', 0.5173909875716537), ('infographics/evidence_type_text_score', 0.5913938033564236), ('infographics/evidence_type_visual_layout_score', 0.489056743627179), ('infographics/reasoning_type_arithmetic_score', 0.4954641059093113), ('infographics/reasoning_type_comparison_score', 0.482062192335367), ('infographics/reasoning_type_counting_score', 0.5807109557109557)]\n",
      "Got result for mmbench - 4800: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7434887286382224)]\n",
      "4800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.5444</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.7937</td>\n",
       "      <td>0.4937</td>\n",
       "      <td>0.7255</td>\n",
       "      <td>0.68126</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.5379</td>\n",
       "      <td>0.7381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.6534</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.7895</td>\n",
       "      <td>0.4979</td>\n",
       "      <td>0.7258</td>\n",
       "      <td>0.68664</td>\n",
       "      <td>0.6334</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.7353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.5567</td>\n",
       "      <td>0.5433</td>\n",
       "      <td>0.6576</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.7947</td>\n",
       "      <td>0.4977</td>\n",
       "      <td>0.7262</td>\n",
       "      <td>0.68338</td>\n",
       "      <td>0.6451</td>\n",
       "      <td>0.5388</td>\n",
       "      <td>0.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.5756</td>\n",
       "      <td>0.5622</td>\n",
       "      <td>0.6862</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.8258</td>\n",
       "      <td>0.5386</td>\n",
       "      <td>0.7527</td>\n",
       "      <td>0.7053</td>\n",
       "      <td>0.6473</td>\n",
       "      <td>0.5585</td>\n",
       "      <td>0.7507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.5511</td>\n",
       "      <td>0.6915</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.5447</td>\n",
       "      <td>0.7517</td>\n",
       "      <td>0.70046</td>\n",
       "      <td>0.6548</td>\n",
       "      <td>0.5557</td>\n",
       "      <td>0.7557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>0.5833</td>\n",
       "      <td>0.5778</td>\n",
       "      <td>0.6882</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.8251</td>\n",
       "      <td>0.5338</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.69956</td>\n",
       "      <td>0.6509</td>\n",
       "      <td>0.5539</td>\n",
       "      <td>0.7495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>0.5633</td>\n",
       "      <td>0.5578</td>\n",
       "      <td>0.6576</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.5053</td>\n",
       "      <td>0.7249</td>\n",
       "      <td>0.68468</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.5447</td>\n",
       "      <td>0.7389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>0.5811</td>\n",
       "      <td>0.5678</td>\n",
       "      <td>0.6634</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.5018</td>\n",
       "      <td>0.7266</td>\n",
       "      <td>0.68846</td>\n",
       "      <td>0.6457</td>\n",
       "      <td>0.5496</td>\n",
       "      <td>0.7391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>0.5822</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.6844</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.8174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7503</td>\n",
       "      <td>0.70316</td>\n",
       "      <td>0.6526</td>\n",
       "      <td>0.5586</td>\n",
       "      <td>0.7516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.5522</td>\n",
       "      <td>0.5433</td>\n",
       "      <td>0.6628</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.8021</td>\n",
       "      <td>0.5066</td>\n",
       "      <td>0.7267</td>\n",
       "      <td>0.68474</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.5559</td>\n",
       "      <td>0.736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>0.5744</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.6694</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.7266</td>\n",
       "      <td>0.68846</td>\n",
       "      <td>0.6433</td>\n",
       "      <td>0.5439</td>\n",
       "      <td>0.7421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>0.5756</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.6676</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.7979</td>\n",
       "      <td>0.5218</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.68844</td>\n",
       "      <td>0.6401</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.7415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>0.5756</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.6673</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.8031</td>\n",
       "      <td>0.5076</td>\n",
       "      <td>0.7276</td>\n",
       "      <td>0.69086</td>\n",
       "      <td>0.6391</td>\n",
       "      <td>0.5484</td>\n",
       "      <td>0.7423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>0.5589</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.7915</td>\n",
       "      <td>0.4969</td>\n",
       "      <td>0.7273</td>\n",
       "      <td>0.69332</td>\n",
       "      <td>0.6433</td>\n",
       "      <td>0.5571</td>\n",
       "      <td>0.7416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.5611</td>\n",
       "      <td>0.6641</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.7908</td>\n",
       "      <td>0.5031</td>\n",
       "      <td>0.7282</td>\n",
       "      <td>0.69406</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.5443</td>\n",
       "      <td>0.7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>0.5767</td>\n",
       "      <td>0.5622</td>\n",
       "      <td>0.6663</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.7924</td>\n",
       "      <td>0.5096</td>\n",
       "      <td>0.7283</td>\n",
       "      <td>0.68548</td>\n",
       "      <td>0.6471</td>\n",
       "      <td>0.5472</td>\n",
       "      <td>0.7447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>0.5467</td>\n",
       "      <td>0.5456</td>\n",
       "      <td>0.6673</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.5096</td>\n",
       "      <td>0.7249</td>\n",
       "      <td>0.68496</td>\n",
       "      <td>0.6484</td>\n",
       "      <td>0.5497</td>\n",
       "      <td>0.7347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.5678</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.7924</td>\n",
       "      <td>0.5189</td>\n",
       "      <td>0.7257</td>\n",
       "      <td>0.68294</td>\n",
       "      <td>0.6466</td>\n",
       "      <td>0.5463</td>\n",
       "      <td>0.7414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.7908</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.7282</td>\n",
       "      <td>0.68798</td>\n",
       "      <td>0.6396</td>\n",
       "      <td>0.5534</td>\n",
       "      <td>0.7394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>0.5411</td>\n",
       "      <td>0.5444</td>\n",
       "      <td>0.6655</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.5135</td>\n",
       "      <td>0.7239</td>\n",
       "      <td>0.69118</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.5426</td>\n",
       "      <td>0.7373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.6578</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.8005</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.7234</td>\n",
       "      <td>0.68614</td>\n",
       "      <td>0.6442</td>\n",
       "      <td>0.5427</td>\n",
       "      <td>0.7405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.5833</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.6649</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.7866</td>\n",
       "      <td>0.5067</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>0.68318</td>\n",
       "      <td>0.6434</td>\n",
       "      <td>0.5466</td>\n",
       "      <td>0.7417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>0.5911</td>\n",
       "      <td>0.5811</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.68862</td>\n",
       "      <td>0.6461</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.7435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1  docvqa mathvista    ai2d chartqa     vqa  textvqa  \\\n",
       "200   0.5556  0.5444   0.649     0.391  0.7937  0.4937  0.7255  0.68126   \n",
       "400     0.57  0.5656  0.6534     0.399  0.7895  0.4979  0.7258  0.68664   \n",
       "600   0.5567  0.5433  0.6576     0.379  0.7947  0.4977  0.7262  0.68338   \n",
       "800   0.5756  0.5622  0.6862     0.435  0.8258  0.5386  0.7527   0.7053   \n",
       "1000  0.5667  0.5511  0.6915     0.441  0.8229  0.5447  0.7517  0.70046   \n",
       "1200  0.5833  0.5778  0.6882     0.443  0.8251  0.5338   0.753  0.69956   \n",
       "1400  0.5633  0.5578  0.6576      0.39  0.8015  0.5053  0.7249  0.68468   \n",
       "1600  0.5811  0.5678  0.6634     0.387  0.8018  0.5018  0.7266  0.68846   \n",
       "1800  0.5822    0.57  0.6844     0.479  0.8174     NaN  0.7503  0.70316   \n",
       "2000  0.5522  0.5433  0.6628     0.378  0.8021  0.5066  0.7267  0.68474   \n",
       "2200  0.5744  0.5656  0.6694     0.394  0.7944  0.5129  0.7266  0.68846   \n",
       "2400  0.5756    0.57  0.6676     0.398  0.7979  0.5218   0.727  0.68844   \n",
       "2600  0.5756    0.56  0.6673     0.399  0.8031  0.5076  0.7276  0.69086   \n",
       "2800  0.5589  0.5544   0.664     0.404  0.7915  0.4969  0.7273  0.69332   \n",
       "3200  0.5689  0.5611  0.6641     0.379  0.7908  0.5031  0.7282  0.69406   \n",
       "3400  0.5767  0.5622  0.6663     0.393  0.7924  0.5096  0.7283  0.68548   \n",
       "3600  0.5467  0.5456  0.6673      0.39   0.807  0.5096  0.7249  0.68496   \n",
       "3800  0.5844  0.5678  0.6617     0.387  0.7924  0.5189  0.7257  0.68294   \n",
       "4000  0.5656    0.55  0.6638     0.397  0.7908   0.512  0.7282  0.68798   \n",
       "4200  0.5411  0.5444  0.6655     0.385  0.7944  0.5135  0.7239  0.69118   \n",
       "4400    0.55  0.5478  0.6578     0.384  0.8005  0.4992  0.7234  0.68614   \n",
       "4600  0.5833  0.5656  0.6649     0.396  0.7866  0.5067  0.7232  0.68318   \n",
       "4800  0.5911  0.5811  0.6656     0.384  0.7892   0.518   0.725  0.68862   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "200               0.641       0.5379  0.7381  \n",
       "400              0.6334       0.5391  0.7353  \n",
       "600              0.6451       0.5388   0.737  \n",
       "800              0.6473       0.5585  0.7507  \n",
       "1000             0.6548       0.5557  0.7557  \n",
       "1200             0.6509       0.5539  0.7495  \n",
       "1400              0.637       0.5447  0.7389  \n",
       "1600             0.6457       0.5496  0.7391  \n",
       "1800             0.6526       0.5586  0.7516  \n",
       "2000              0.642       0.5559   0.736  \n",
       "2200             0.6433       0.5439  0.7421  \n",
       "2400             0.6401        0.545  0.7415  \n",
       "2600             0.6391       0.5484  0.7423  \n",
       "2800             0.6433       0.5571  0.7416  \n",
       "3200             0.6429       0.5443  0.7392  \n",
       "3400             0.6471       0.5472  0.7447  \n",
       "3600             0.6484       0.5497  0.7347  \n",
       "3800             0.6466       0.5463  0.7414  \n",
       "4000             0.6396       0.5534  0.7394  \n",
       "4200              0.648       0.5426  0.7373  \n",
       "4400             0.6442       0.5427  0.7405  \n",
       "4600             0.6434       0.5466  0.7417  \n",
       "4800             0.6461        0.553  0.7435  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores_all(\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New checkpoints: [6000]\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp28/checkpoint-6000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp28/evals/eval_jobs_checkpoint-6000.json\n"
     ]
    }
   ],
   "source": [
    "eval_helper.run_eval_sweep(\n",
    "    output_dir=f\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp28\",\n",
    "    eval_sbatch=EVAL_SBATCH,\n",
    "    eval_config_dir=EVAL_CONFIG_DIR,\n",
    "    aligner_parent_dir=ALIGNER_CODE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2600, 2800, 3000, 3200, 3400, 3600, 3800, 4000, 4200, 4400, 4600, 4800, 5000, 5200, 5400, 5600, 5800, 6000]\n",
      "Got result for mmmu - 200: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.43333333333333335), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.5333333333333333), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.5666666666666667), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.8), ('mmmu/accuracy', 0.5577777777777778), ('mmmu/mllm_eval_accuracy', 0.5688888888888889)]\n",
      "Got result for docvqa - 200: [('docvqa/anls_total_score', 0.6851530745117566), ('docvqa/mllm_evaluation_anls_score', 0.6844053884846867), ('docvqa/mmllm_fixed_anls_score', 0.7223642689741734)]\n",
      "Got result for mathvista - 200: [('mathvista/accuracy', 0.411)]\n",
      "Got result for ai2d - 200: [('ai2d/accuracy', 0.8173575129533679)]\n",
      "Got result for chartqa - 200: [('chartqa/accuracy', 0.518923699207068)]\n",
      "Got result for vqa - 200: [('vqa/accuracy', 0.7468399999999757), ('vqa/recall', 0.7745999999999753), ('vqa/bleu', 0.03156115487217903), ('vqa/mllm_evaluation_accuracy', 0.7705199999999759)]\n",
      "Got result for textvqa - 200: [('textvqa/accuracy', 69.68800000000036), ('textvqa/mllm_eval_accuracy', 73.83400000000039)]\n",
      "Got result for infographics_w_ocr - 200: [('infographics_w_ocr/anls_total_score', 0.6447185406390223), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5938385075230164), ('infographics_w_ocr/answer_type_multi_span_score', 0.5503691938469056), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6404162443945446), ('infographics_w_ocr/answer_type_question_span_score', 0.7162564434057648), ('infographics_w_ocr/answer_type_single_span_score', 0.6523644566708748), ('infographics_w_ocr/evidence_type_figure_score', 0.6264635524853811), ('infographics_w_ocr/evidence_type_map_score', 0.6020658796648897), ('infographics_w_ocr/evidence_type_table_list_score', 0.6435081450113043), ('infographics_w_ocr/evidence_type_text_score', 0.6909712626814268), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5598980170463517), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6296239148978872), ('infographics_w_ocr/reasoning_type_comparison_score', 0.54614763522812), ('infographics_w_ocr/reasoning_type_counting_score', 0.6343240093240093)]\n",
      "Got result for infographics - 200: [('infographics/anls_total_score', 0.5586695556928462), ('infographics/mllm_evaluation_anls_score', 0.49428537870339406), ('infographics/answer_type_multi_span_score', 0.39405287016749907), ('infographics/answer_type_non_extractive_score', 0.5501736559602743), ('infographics/answer_type_question_span_score', 0.6900682117028271), ('infographics/answer_type_single_span_score', 0.5732820784316588), ('infographics/evidence_type_figure_score', 0.5410299600923666), ('infographics/evidence_type_map_score', 0.5383822615371227), ('infographics/evidence_type_table_list_score', 0.5363205973740994), ('infographics/evidence_type_text_score', 0.5959148594734742), ('infographics/evidence_type_visual_layout_score', 0.5204674401109819), ('infographics/reasoning_type_arithmetic_score', 0.5001576429658621), ('infographics/reasoning_type_comparison_score', 0.48523278426784316), ('infographics/reasoning_type_counting_score', 0.5865384615384616)]\n",
      "Got result for mmbench - 200: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.5121951219512195), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7430291030449682)]\n",
      "200\n",
      "Got result for mmmu - 400: [('mmmu/accounting', 0.43333333333333335), ('mmmu/agriculture', 0.5), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.9), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.6666666666666666), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.7), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.5), ('mmmu/geography', 0.5), ('mmmu/history', 0.8), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.3), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7666666666666667), ('mmmu/accuracy', 0.5544444444444444), ('mmmu/mllm_eval_accuracy', 0.5766666666666667)]\n",
      "Got result for docvqa - 400: [('docvqa/anls_total_score', 0.694550159268945), ('docvqa/mllm_evaluation_anls_score', 0.6942296440524113), ('docvqa/mmllm_fixed_anls_score', 0.7304485879581767)]\n",
      "Got result for mathvista - 400: [('mathvista/accuracy', 0.42)]\n",
      "Got result for ai2d - 400: [('ai2d/accuracy', 0.8180051813471503)]\n",
      "Got result for chartqa - 400: [('chartqa/accuracy', 0.5345047208714526)]\n",
      "Got result for vqa - 400: [('vqa/accuracy', 0.7513399999999752), ('vqa/recall', 0.7769959999999749), ('vqa/bleu', 0.038769494742155075), ('vqa/mllm_evaluation_accuracy', 0.7737159999999762)]\n",
      "Got result for textvqa - 400: [('textvqa/accuracy', 70.67000000000037), ('textvqa/mllm_eval_accuracy', 74.85400000000044)]\n",
      "Got result for infographics_w_ocr - 400: [('infographics_w_ocr/anls_total_score', 0.6488734426426309), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5961101948309698), ('infographics_w_ocr/answer_type_multi_span_score', 0.517284821077321), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6378799621114272), ('infographics_w_ocr/answer_type_question_span_score', 0.7106474690467903), ('infographics_w_ocr/answer_type_single_span_score', 0.6632571398308017), ('infographics_w_ocr/evidence_type_figure_score', 0.6325310760629169), ('infographics_w_ocr/evidence_type_map_score', 0.5698876618431074), ('infographics_w_ocr/evidence_type_table_list_score', 0.6525122710947869), ('infographics_w_ocr/evidence_type_text_score', 0.6869910781037137), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5879749226782212), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6350945857795169), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5523507971019843), ('infographics_w_ocr/reasoning_type_counting_score', 0.6255827505827506)]\n",
      "Got result for infographics - 400: [('infographics/anls_total_score', 0.5459186927299082), ('infographics/mllm_evaluation_anls_score', 0.4839021941403154), ('infographics/answer_type_multi_span_score', 0.3500961852690957), ('infographics/answer_type_non_extractive_score', 0.5448996151527796), ('infographics/answer_type_question_span_score', 0.6931397928994083), ('infographics/answer_type_single_span_score', 0.5603107000740348), ('infographics/evidence_type_figure_score', 0.5339557398702707), ('infographics/evidence_type_map_score', 0.5209845941685759), ('infographics/evidence_type_table_list_score', 0.5138916064950364), ('infographics/evidence_type_text_score', 0.5846427918166275), ('infographics/evidence_type_visual_layout_score', 0.4980518059551699), ('infographics/reasoning_type_arithmetic_score', 0.49713294696171395), ('infographics/reasoning_type_comparison_score', 0.4810125595603904), ('infographics/reasoning_type_counting_score', 0.5818764568764568)]\n",
      "Got result for mmbench - 400: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.9166666666666666), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.7142857142857143), ('mmbench/overall', 0.7529135901708386)]\n",
      "400\n",
      "Got result for mmmu - 600: [('mmmu/accounting', 0.43333333333333335), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.7), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.6), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.8), ('mmmu/sociology', 0.7666666666666667), ('mmmu/accuracy', 0.5688888888888889), ('mmmu/mllm_eval_accuracy', 0.58)]\n",
      "Got result for docvqa - 600: [('docvqa/anls_total_score', 0.6835048246017047), ('docvqa/mllm_evaluation_anls_score', 0.6836101710515065), ('docvqa/mmllm_fixed_anls_score', 0.7210585631034013)]\n",
      "Got result for mathvista - 600: [('mathvista/accuracy', 0.432)]\n",
      "Got result for ai2d - 600: [('ai2d/accuracy', 0.8235103626943006)]\n",
      "Got result for chartqa - 600: [('chartqa/accuracy', 0.5300919808810313)]\n",
      "Got result for vqa - 600: [('vqa/accuracy', 0.7490999999999756), ('vqa/recall', 0.7748799999999743), ('vqa/bleu', 0.026821967214345932), ('vqa/mllm_evaluation_accuracy', 0.7706199999999754)]\n",
      "Got result for textvqa - 600: [('textvqa/accuracy', 70.01200000000031), ('textvqa/mllm_eval_accuracy', 74.30600000000035)]\n",
      "Got result for infographics_w_ocr - 600: [('infographics_w_ocr/anls_total_score', 0.6515065401246783), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6014267269645682), ('infographics_w_ocr/answer_type_multi_span_score', 0.507566383835137), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6516980307215392), ('infographics_w_ocr/answer_type_question_span_score', 0.7197538038884194), ('infographics_w_ocr/answer_type_single_span_score', 0.6616220186156623), ('infographics_w_ocr/evidence_type_figure_score', 0.6285914637931848), ('infographics_w_ocr/evidence_type_map_score', 0.5881656661141226), ('infographics_w_ocr/evidence_type_table_list_score', 0.6571715774068906), ('infographics_w_ocr/evidence_type_text_score', 0.6951097903193735), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5605610061452192), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6561267499623661), ('infographics_w_ocr/reasoning_type_comparison_score', 0.557049368550685), ('infographics_w_ocr/reasoning_type_counting_score', 0.6378205128205128)]\n",
      "Got result for infographics - 600: [('infographics/anls_total_score', 0.5574156881593187), ('infographics/mllm_evaluation_anls_score', 0.49462318399407484), ('infographics/answer_type_multi_span_score', 0.39092636618510734), ('infographics/answer_type_non_extractive_score', 0.5554227468332352), ('infographics/answer_type_question_span_score', 0.699454658589274), ('infographics/answer_type_single_span_score', 0.5691865031249267), ('infographics/evidence_type_figure_score', 0.5415843319005622), ('infographics/evidence_type_map_score', 0.5580465782780866), ('infographics/evidence_type_table_list_score', 0.5314606966184777), ('infographics/evidence_type_text_score', 0.5992339394229137), ('infographics/evidence_type_visual_layout_score', 0.5035985331204011), ('infographics/reasoning_type_arithmetic_score', 0.516833718032348), ('infographics/reasoning_type_comparison_score', 0.4657378681362684), ('infographics/reasoning_type_counting_score', 0.5865384615384615)]\n",
      "Got result for mmbench - 600: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.743029470672095)]\n",
      "600\n",
      "Got result for mmmu - 800: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.7333333333333333), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.4), ('mmmu/geography', 0.43333333333333335), ('mmmu/history', 0.7), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.5333333333333333), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5622222222222222), ('mmmu/mllm_eval_accuracy', 0.5755555555555556)]\n",
      "Got result for docvqa - 800: [('docvqa/anls_total_score', 0.6861651868027577), ('docvqa/mllm_evaluation_anls_score', 0.6849665255082817), ('docvqa/mmllm_fixed_anls_score', 0.720501167940428)]\n",
      "Got result for mathvista - 800: [('mathvista/accuracy', 0.435)]\n",
      "Got result for ai2d - 800: [('ai2d/accuracy', 0.8257772020725389)]\n",
      "Got result for chartqa - 800: [('chartqa/accuracy', 0.5386423595754996)]\n",
      "Got result for vqa - 800: [('vqa/accuracy', 0.7527439999999767), ('vqa/recall', 0.7740319999999759), ('vqa/bleu', 0.04545086994767189), ('vqa/mllm_evaluation_accuracy', 0.7709879999999767)]\n",
      "Got result for textvqa - 800: [('textvqa/accuracy', 70.53000000000036), ('textvqa/mllm_eval_accuracy', 74.64600000000038)]\n",
      "Got result for infographics_w_ocr - 800: [('infographics_w_ocr/anls_total_score', 0.6472951114150778), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5973177554748575), ('infographics_w_ocr/answer_type_multi_span_score', 0.503323993986201), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6466639398375387), ('infographics_w_ocr/answer_type_question_span_score', 0.7170577254570468), ('infographics_w_ocr/answer_type_single_span_score', 0.6575469899729094), ('infographics_w_ocr/evidence_type_figure_score', 0.6270001994179981), ('infographics_w_ocr/evidence_type_map_score', 0.608501523229246), ('infographics_w_ocr/evidence_type_table_list_score', 0.6534260886396096), ('infographics_w_ocr/evidence_type_text_score', 0.692856945110573), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5523783679965781), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.652300771906936), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5411386472815425), ('infographics_w_ocr/reasoning_type_counting_score', 0.6284965034965035)]\n",
      "Got result for infographics - 800: [('infographics/anls_total_score', 0.5584713034221711), ('infographics/mllm_evaluation_anls_score', 0.4991246766993003), ('infographics/answer_type_multi_span_score', 0.3821692950630614), ('infographics/answer_type_non_extractive_score', 0.5402885139774832), ('infographics/answer_type_question_span_score', 0.694036465671081), ('infographics/answer_type_single_span_score', 0.5761809310517206), ('infographics/evidence_type_figure_score', 0.5486501418344766), ('infographics/evidence_type_map_score', 0.5589467317772501), ('infographics/evidence_type_table_list_score', 0.524121350581692), ('infographics/evidence_type_text_score', 0.5997578810480703), ('infographics/evidence_type_visual_layout_score', 0.5009458084050376), ('infographics/reasoning_type_arithmetic_score', 0.4844048455349825), ('infographics/reasoning_type_comparison_score', 0.4813541596300644), ('infographics/reasoning_type_counting_score', 0.5786713286713286)]\n",
      "Got result for mmbench - 800: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.9166666666666666), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6753246753246753), ('mmbench/overall', 0.750688446377466)]\n",
      "800\n",
      "Got result for mmmu - 1000: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5333333333333333), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.6), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.4666666666666667), ('mmmu/energy_and_power', 0.36666666666666664), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.43333333333333335), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.6333333333333333), ('mmmu/materials', 0.4), ('mmmu/math', 0.6), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5511111111111111), ('mmmu/mllm_eval_accuracy', 0.5666666666666667)]\n",
      "Got result for docvqa - 1000: [('docvqa/anls_total_score', 0.6914809452898901), ('docvqa/mllm_evaluation_anls_score', 0.6910337685765978), ('docvqa/mmllm_fixed_anls_score', 0.7266098512822747)]\n",
      "Got result for mathvista - 1000: [('mathvista/accuracy', 0.441)]\n",
      "Got result for ai2d - 1000: [('ai2d/accuracy', 0.8228626943005182)]\n",
      "Got result for chartqa - 1000: [('chartqa/accuracy', 0.5446625354709164)]\n",
      "Got result for vqa - 1000: [('vqa/accuracy', 0.7517119999999763), ('vqa/recall', 0.7744439999999746), ('vqa/bleu', 0.03847002610564232), ('vqa/mllm_evaluation_accuracy', 0.7715879999999756)]\n",
      "Got result for textvqa - 1000: [('textvqa/accuracy', 70.04600000000036), ('textvqa/mllm_eval_accuracy', 74.15000000000038)]\n",
      "Got result for infographics_w_ocr - 1000: [('infographics_w_ocr/anls_total_score', 0.6547973502072989), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6048048545226773), ('infographics_w_ocr/answer_type_multi_span_score', 0.503602583214263), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6551580125721177), ('infographics_w_ocr/answer_type_question_span_score', 0.7178612519958674), ('infographics_w_ocr/answer_type_single_span_score', 0.6665106562611337), ('infographics_w_ocr/evidence_type_figure_score', 0.6354380330971015), ('infographics_w_ocr/evidence_type_map_score', 0.5891945925361767), ('infographics_w_ocr/evidence_type_table_list_score', 0.6538581681693623), ('infographics_w_ocr/evidence_type_text_score', 0.7021856039991057), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5823333066147026), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.65662915851272), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5496088289389683), ('infographics_w_ocr/reasoning_type_counting_score', 0.6491841491841491)]\n",
      "Got result for infographics - 1000: [('infographics/anls_total_score', 0.5557274631323967), ('infographics/mllm_evaluation_anls_score', 0.48987016150522056), ('infographics/answer_type_multi_span_score', 0.35456009292459895), ('infographics/answer_type_non_extractive_score', 0.5457338923885037), ('infographics/answer_type_question_span_score', 0.6946469662815817), ('infographics/answer_type_single_span_score', 0.5732867615153577), ('infographics/evidence_type_figure_score', 0.5433856843943352), ('infographics/evidence_type_map_score', 0.5117018932850822), ('infographics/evidence_type_table_list_score', 0.5286447006472307), ('infographics/evidence_type_text_score', 0.5924922026101045), ('infographics/evidence_type_visual_layout_score', 0.5158001118642461), ('infographics/reasoning_type_arithmetic_score', 0.49825630990014547), ('infographics/reasoning_type_comparison_score', 0.47883552954855163), ('infographics/reasoning_type_counting_score', 0.5812937062937062)]\n",
      "Got result for mmbench - 1000: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.5185185185185185), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7557467484518534)]\n",
      "1000\n",
      "Got result for mmmu - 1200: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.5), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.5666666666666667), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.6333333333333333), ('mmmu/marketing', 0.4), ('mmmu/materials', 0.26666666666666666), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5777777777777777), ('mmmu/mllm_eval_accuracy', 0.5833333333333334)]\n",
      "Got result for docvqa - 1200: [('docvqa/anls_total_score', 0.688159004463097), ('docvqa/mllm_evaluation_anls_score', 0.687887849937589), ('docvqa/mmllm_fixed_anls_score', 0.7248432274205141)]\n",
      "Got result for mathvista - 1200: [('mathvista/accuracy', 0.443)]\n",
      "Got result for ai2d - 1200: [('ai2d/accuracy', 0.8251295336787565)]\n",
      "Got result for chartqa - 1200: [('chartqa/accuracy', 0.5338041688877015)]\n",
      "Got result for vqa - 1200: [('vqa/accuracy', 0.7530399999999744), ('vqa/recall', 0.7722879999999732), ('vqa/bleu', 0.042081840336322784), ('vqa/mllm_evaluation_accuracy', 0.7701319999999744)]\n",
      "Got result for textvqa - 1200: [('textvqa/accuracy', 69.95600000000036), ('textvqa/mllm_eval_accuracy', 74.09000000000042)]\n",
      "Got result for infographics_w_ocr - 1200: [('infographics_w_ocr/anls_total_score', 0.6508986976368093), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5996206615245506), ('infographics_w_ocr/answer_type_multi_span_score', 0.5430357056045166), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6591065053090372), ('infographics_w_ocr/answer_type_question_span_score', 0.7079765288758502), ('infographics_w_ocr/answer_type_single_span_score', 0.6569890842073937), ('infographics_w_ocr/evidence_type_figure_score', 0.6324564858914928), ('infographics_w_ocr/evidence_type_map_score', 0.6080064737242955), ('infographics_w_ocr/evidence_type_table_list_score', 0.646020786828472), ('infographics_w_ocr/evidence_type_text_score', 0.6924020368555358), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5810923917569475), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6589699683877763), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5452584973158334), ('infographics_w_ocr/reasoning_type_counting_score', 0.6386946386946387)]\n",
      "Got result for infographics - 1200: [('infographics/anls_total_score', 0.5538923675620057), ('infographics/mllm_evaluation_anls_score', 0.49112106346003614), ('infographics/answer_type_multi_span_score', 0.39007474218426597), ('infographics/answer_type_non_extractive_score', 0.5509604620816195), ('infographics/answer_type_question_span_score', 0.7153276744622898), ('infographics/answer_type_single_span_score', 0.5647906983169795), ('infographics/evidence_type_figure_score', 0.54006393378161), ('infographics/evidence_type_map_score', 0.49796591220988223), ('infographics/evidence_type_table_list_score', 0.5145635853743483), ('infographics/evidence_type_text_score', 0.6015344858227378), ('infographics/evidence_type_visual_layout_score', 0.5387917275282756), ('infographics/reasoning_type_arithmetic_score', 0.5091819709970394), ('infographics/reasoning_type_comparison_score', 0.4477963166018653), ('infographics/reasoning_type_counting_score', 0.5775058275058275)]\n",
      "Got result for mmbench - 1200: [('mmbench/attribute_comparison', 0.6590909090909091), ('mmbench/attribute_recognition', 0.9014084507042254), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.7494843283432574)]\n",
      "1200\n",
      "Got result for mmmu - 1400: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5333333333333333), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.8), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.7333333333333333), ('mmmu/electronics', 0.5333333333333333), ('mmmu/energy_and_power', 0.5666666666666667), ('mmmu/finance', 0.5666666666666667), ('mmmu/geography', 0.6), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.6), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.8), ('mmmu/accuracy', 0.5944444444444444), ('mmmu/mllm_eval_accuracy', 0.5966666666666667)]\n",
      "Got result for docvqa - 1400: [('docvqa/anls_total_score', 0.6930404464982352), ('docvqa/mllm_evaluation_anls_score', 0.6927990890884685), ('docvqa/mmllm_fixed_anls_score', 0.729133455674974)]\n",
      "Got result for mathvista - 1400: [('mathvista/accuracy', 0.449)]\n",
      "Got result for ai2d - 1400: [('ai2d/accuracy', 0.8189766839378239)]\n",
      "Got result for chartqa - 1400: [('chartqa/accuracy', 0.5360363717944453)]\n",
      "Got result for vqa - 1400: [('vqa/accuracy', 0.7513519999999766), ('vqa/recall', 0.7735159999999754), ('vqa/bleu', 0.03825804218649864), ('vqa/mllm_evaluation_accuracy', 0.7701839999999763)]\n",
      "Got result for textvqa - 1400: [('textvqa/accuracy', 70.01000000000035), ('textvqa/mllm_eval_accuracy', 73.9480000000004)]\n",
      "Got result for infographics_w_ocr - 1400: [('infographics_w_ocr/anls_total_score', 0.6499254459825651), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5994573807403643), ('infographics_w_ocr/answer_type_multi_span_score', 0.5539215005211982), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6378966873994001), ('infographics_w_ocr/answer_type_question_span_score', 0.7172714006707219), ('infographics_w_ocr/answer_type_single_span_score', 0.6611922024724622), ('infographics_w_ocr/evidence_type_figure_score', 0.6279122413414823), ('infographics_w_ocr/evidence_type_map_score', 0.6209732967223848), ('infographics_w_ocr/evidence_type_table_list_score', 0.641478413065964), ('infographics_w_ocr/evidence_type_text_score', 0.701503783852162), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5746796895374385), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6325577675748906), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5264763709147012), ('infographics_w_ocr/reasoning_type_counting_score', 0.6311188811188811)]\n",
      "Got result for infographics - 1400: [('infographics/anls_total_score', 0.5688860273165649), ('infographics/mllm_evaluation_anls_score', 0.5073781456586762), ('infographics/answer_type_multi_span_score', 0.39297840984481774), ('infographics/answer_type_non_extractive_score', 0.5489512195660479), ('infographics/answer_type_question_span_score', 0.7260877477223631), ('infographics/answer_type_single_span_score', 0.5855738858885448), ('infographics/evidence_type_figure_score', 0.555901520109363), ('infographics/evidence_type_map_score', 0.5659534612489847), ('infographics/evidence_type_table_list_score', 0.5360086889679313), ('infographics/evidence_type_text_score', 0.6162989623287427), ('infographics/evidence_type_visual_layout_score', 0.5324573180136688), ('infographics/reasoning_type_arithmetic_score', 0.5050914078311337), ('infographics/reasoning_type_comparison_score', 0.5023171452819319), ('infographics/reasoning_type_counting_score', 0.5932400932400932)]\n",
      "Got result for mmbench - 1400: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.7510641699138133)]\n",
      "1400\n",
      "Got result for mmmu - 1600: [('mmmu/accounting', 0.5333333333333333), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.8), ('mmmu/biology', 0.4666666666666667), ('mmmu/chemistry', 0.5), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.7), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.6333333333333333), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.7666666666666667), ('mmmu/accuracy', 0.5833333333333334), ('mmmu/mllm_eval_accuracy', 0.59)]\n",
      "Got result for docvqa - 1600: [('docvqa/anls_total_score', 0.6895491186186158), ('docvqa/mllm_evaluation_anls_score', 0.6894087841419814), ('docvqa/mmllm_fixed_anls_score', 0.7256799113549901)]\n",
      "Got result for mathvista - 1600: [('mathvista/accuracy', 0.462)]\n",
      "Got result for ai2d - 1600: [('ai2d/accuracy', 0.8167098445595855)]\n",
      "Got result for chartqa - 1600: [('chartqa/accuracy', 0.5228806060184572)]\n",
      "Got result for vqa - 1600: [('vqa/accuracy', 0.7529279999999762), ('vqa/recall', 0.7734919999999755), ('vqa/bleu', 0.04158240929245949), ('vqa/mllm_evaluation_accuracy', 0.7710599999999765)]\n",
      "Got result for textvqa - 1600: [('textvqa/accuracy', 69.82600000000032), ('textvqa/mllm_eval_accuracy', 73.9440000000004)]\n",
      "Got result for infographics_w_ocr - 1600: [('infographics_w_ocr/anls_total_score', 0.6523406873214214), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6013651697734672), ('infographics_w_ocr/answer_type_multi_span_score', 0.5173389492531302), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6550700806125765), ('infographics_w_ocr/answer_type_question_span_score', 0.6839380673373886), ('infographics_w_ocr/answer_type_single_span_score', 0.6647116277133357), ('infographics_w_ocr/evidence_type_figure_score', 0.6346603727706368), ('infographics_w_ocr/evidence_type_map_score', 0.5943101040873318), ('infographics_w_ocr/evidence_type_table_list_score', 0.6483615753947224), ('infographics_w_ocr/evidence_type_text_score', 0.6955143605705562), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5588978694301922), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6600014426213051), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5605359506802862), ('infographics_w_ocr/reasoning_type_counting_score', 0.6375291375291375)]\n",
      "Got result for infographics - 1600: [('infographics/anls_total_score', 0.5537297630445153), ('infographics/mllm_evaluation_anls_score', 0.4899764745382736), ('infographics/answer_type_multi_span_score', 0.36928185097523347), ('infographics/answer_type_non_extractive_score', 0.5412542751602428), ('infographics/answer_type_question_span_score', 0.7004056306940922), ('infographics/answer_type_single_span_score', 0.5701383665801465), ('infographics/evidence_type_figure_score', 0.5448171919394893), ('infographics/evidence_type_map_score', 0.5520504927027933), ('infographics/evidence_type_table_list_score', 0.5141683598078697), ('infographics/evidence_type_text_score', 0.601637879789039), ('infographics/evidence_type_visual_layout_score', 0.4912309098181136), ('infographics/reasoning_type_arithmetic_score', 0.48395073343703465), ('infographics/reasoning_type_comparison_score', 0.46657430730600613), ('infographics/reasoning_type_counting_score', 0.5956293706293705)]\n",
      "Got result for mmbench - 1600: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6266666666666667), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7524386672207198)]\n",
      "1600\n",
      "Got result for mmmu - 1800: [('mmmu/accounting', 0.5333333333333333), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.4666666666666667), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.6333333333333333), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.6), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.4), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.8), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.57), ('mmmu/mllm_eval_accuracy', 0.5822222222222222)]\n",
      "Got result for docvqa - 1800: [('docvqa/anls_total_score', 0.6843588119133742), ('docvqa/mllm_evaluation_anls_score', 0.684458344039418), ('docvqa/mmllm_fixed_anls_score', 0.7224850768908115)]\n",
      "Got result for mathvista - 1800: [('mathvista/accuracy', 0.479)]\n",
      "Got result for ai2d - 1800: [('ai2d/accuracy', 0.8173575129533679)]\n",
      "Got result for vqa - 1800: [('vqa/accuracy', 0.7503119999999752), ('vqa/recall', 0.7713479999999748), ('vqa/bleu', 0.037375714629888535), ('vqa/mllm_evaluation_accuracy', 0.7684439999999758)]\n",
      "Got result for textvqa - 1800: [('textvqa/accuracy', 70.31600000000034), ('textvqa/mllm_eval_accuracy', 74.32400000000037)]\n",
      "Got result for infographics_w_ocr - 1800: [('infographics_w_ocr/anls_total_score', 0.6526072837441547), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5975676709033039), ('infographics_w_ocr/answer_type_multi_span_score', 0.5427162487344895), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6393067450933637), ('infographics_w_ocr/answer_type_question_span_score', 0.714310472709794), ('infographics_w_ocr/answer_type_single_span_score', 0.664294212037485), ('infographics_w_ocr/evidence_type_figure_score', 0.634466514054076), ('infographics_w_ocr/evidence_type_map_score', 0.6252231953964628), ('infographics_w_ocr/evidence_type_table_list_score', 0.6469956922890664), ('infographics_w_ocr/evidence_type_text_score', 0.7085295462550012), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5564597474059932), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6558332078880021), ('infographics_w_ocr/reasoning_type_comparison_score', 0.542371694441681), ('infographics_w_ocr/reasoning_type_counting_score', 0.6127622377622378)]\n",
      "Got result for infographics - 1800: [('infographics/anls_total_score', 0.5585906774640167), ('infographics/mllm_evaluation_anls_score', 0.4935091271938477), ('infographics/answer_type_multi_span_score', 0.3778451767180989), ('infographics/answer_type_non_extractive_score', 0.5463894353586939), ('infographics/answer_type_question_span_score', 0.7421515450361604), ('infographics/answer_type_single_span_score', 0.5719845867047859), ('infographics/evidence_type_figure_score', 0.545387545278541), ('infographics/evidence_type_map_score', 0.5535764732659879), ('infographics/evidence_type_table_list_score', 0.5195161862322201), ('infographics/evidence_type_text_score', 0.5951663128244008), ('infographics/evidence_type_visual_layout_score', 0.5096500748563842), ('infographics/reasoning_type_arithmetic_score', 0.5097717731279374), ('infographics/reasoning_type_comparison_score', 0.48293097685132386), ('infographics/reasoning_type_counting_score', 0.5772144522144521)]\n",
      "Got result for mmbench - 1800: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.5), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6103896103896104), ('mmbench/overall', 0.7516339808169767)]\n",
      "1800\n",
      "Got result for mmmu - 2000: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.5333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.26666666666666666), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.8), ('mmmu/sociology', 0.8), ('mmmu/accuracy', 0.5655555555555556), ('mmmu/mllm_eval_accuracy', 0.5811111111111111)]\n",
      "Got result for docvqa - 2000: [('docvqa/anls_total_score', 0.6899574637869383), ('docvqa/mllm_evaluation_anls_score', 0.6898156482086736), ('docvqa/mmllm_fixed_anls_score', 0.7262972673636102)]\n",
      "Got result for mathvista - 2000: [('mathvista/accuracy', 0.465)]\n",
      "Got result for ai2d - 2000: [('ai2d/accuracy', 0.8189766839378239)]\n",
      "Got result for chartqa - 2000: [('chartqa/accuracy', 0.5427615168847472)]\n",
      "Got result for vqa - 2000: [('vqa/accuracy', 0.7523479999999746), ('vqa/recall', 0.7745559999999742), ('vqa/bleu', 0.03253495320677757), ('vqa/mllm_evaluation_accuracy', 0.771683999999975)]\n",
      "Got result for textvqa - 2000: [('textvqa/accuracy', 70.33400000000033), ('textvqa/mllm_eval_accuracy', 74.4280000000004)]\n",
      "Got result for infographics_w_ocr - 2000: [('infographics_w_ocr/anls_total_score', 0.6562296117789616), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6045113204547179), ('infographics_w_ocr/answer_type_multi_span_score', 0.5408687933693975), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6511757380654309), ('infographics_w_ocr/answer_type_question_span_score', 0.7020241479234692), ('infographics_w_ocr/answer_type_single_span_score', 0.6674771707181895), ('infographics_w_ocr/evidence_type_figure_score', 0.632734958589794), ('infographics_w_ocr/evidence_type_map_score', 0.5919998730642295), ('infographics_w_ocr/evidence_type_table_list_score', 0.6616010535428127), ('infographics_w_ocr/evidence_type_text_score', 0.6993930197273216), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5689288603291461), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6440645541672936), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5584972618671974), ('infographics_w_ocr/reasoning_type_counting_score', 0.6480186480186481)]\n",
      "Got result for infographics - 2000: [('infographics/anls_total_score', 0.5637488889001301), ('infographics/mllm_evaluation_anls_score', 0.49996109506085806), ('infographics/answer_type_multi_span_score', 0.4020599697203107), ('infographics/answer_type_non_extractive_score', 0.5517984928473174), ('infographics/answer_type_question_span_score', 0.7089174180520335), ('infographics/answer_type_single_span_score', 0.5774753394974147), ('infographics/evidence_type_figure_score', 0.5555100740191045), ('infographics/evidence_type_map_score', 0.5700781234310044), ('infographics/evidence_type_table_list_score', 0.5235230656604243), ('infographics/evidence_type_text_score', 0.5978651676113317), ('infographics/evidence_type_visual_layout_score', 0.5138399457639763), ('infographics/reasoning_type_arithmetic_score', 0.5072873283489722), ('infographics/reasoning_type_comparison_score', 0.4857721513532679), ('infographics/reasoning_type_counting_score', 0.5920745920745921)]\n",
      "Got result for mmbench - 2000: [('mmbench/attribute_comparison', 0.6590909090909091), ('mmbench/attribute_recognition', 0.9014084507042254), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.5185185185185185), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7527252344913367)]\n",
      "2000\n",
      "Got result for mmmu - 2200: [('mmmu/accounting', 0.7333333333333333), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7666666666666667), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.6666666666666666), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.7333333333333333), ('mmmu/electronics', 0.3333333333333333), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.6), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.5333333333333333), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.5), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.7666666666666667), ('mmmu/accuracy', 0.5955555555555555), ('mmmu/mllm_eval_accuracy', 0.5966666666666667)]\n",
      "Got result for docvqa - 2200: [('docvqa/anls_total_score', 0.6922914190279911), ('docvqa/mllm_evaluation_anls_score', 0.6911179245164714), ('docvqa/mmllm_fixed_anls_score', 0.7283848687729948)]\n",
      "Got result for mathvista - 2200: [('mathvista/accuracy', 0.479)]\n",
      "Got result for ai2d - 2200: [('ai2d/accuracy', 0.8196243523316062)]\n",
      "Got result for chartqa - 2200: [('chartqa/accuracy', 0.5444108819674306)]\n",
      "Got result for vqa - 2200: [('vqa/accuracy', 0.7515439999999755), ('vqa/recall', 0.7717879999999748), ('vqa/bleu', 0.0398288257420063), ('vqa/mllm_evaluation_accuracy', 0.7689359999999761)]\n",
      "Got result for textvqa - 2200: [('textvqa/accuracy', 69.97200000000032), ('textvqa/mllm_eval_accuracy', 74.03600000000041)]\n",
      "Got result for infographics_w_ocr - 2200: [('infographics_w_ocr/anls_total_score', 0.6550591443045888), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6044840273997697), ('infographics_w_ocr/answer_type_multi_span_score', 0.5582450594392037), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6471497459743394), ('infographics_w_ocr/answer_type_question_span_score', 0.7106474690467903), ('infographics_w_ocr/answer_type_single_span_score', 0.6646520567500551), ('infographics_w_ocr/evidence_type_figure_score', 0.6378259399773384), ('infographics_w_ocr/evidence_type_map_score', 0.6151493308671525), ('infographics_w_ocr/evidence_type_table_list_score', 0.6478554446518198), ('infographics_w_ocr/evidence_type_text_score', 0.6893174769062259), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.575264602052212), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6398646444879319), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5689727738266208), ('infographics_w_ocr/reasoning_type_counting_score', 0.6452797202797204)]\n",
      "Got result for infographics - 2200: [('infographics/anls_total_score', 0.5597182089159649), ('infographics/mllm_evaluation_anls_score', 0.49565477722568685), ('infographics/answer_type_multi_span_score', 0.3724094870637148), ('infographics/answer_type_non_extractive_score', 0.5421696948821723), ('infographics/answer_type_question_span_score', 0.7482183948530102), ('infographics/answer_type_single_span_score', 0.5740709828417373), ('infographics/evidence_type_figure_score', 0.5491495980033484), ('infographics/evidence_type_map_score', 0.5814642620448657), ('infographics/evidence_type_table_list_score', 0.5235315391499971), ('infographics/evidence_type_text_score', 0.5908737240147329), ('infographics/evidence_type_visual_layout_score', 0.5217730708301629), ('infographics/reasoning_type_arithmetic_score', 0.500524570558817), ('infographics/reasoning_type_comparison_score', 0.47951700544005643), ('infographics/reasoning_type_counting_score', 0.5705710955710955)]\n",
      "Got result for mmbench - 2200: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.9014084507042254), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.25), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.742391080083362)]\n",
      "2200\n",
      "Got result for mmmu - 2400: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.23333333333333334), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.5), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.4), ('mmmu/geography', 0.5), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.7333333333333333), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5644444444444444), ('mmmu/mllm_eval_accuracy', 0.5866666666666667)]\n",
      "Got result for docvqa - 2400: [('docvqa/anls_total_score', 0.6955738547753733), ('docvqa/mllm_evaluation_anls_score', 0.6952901555391034), ('docvqa/mmllm_fixed_anls_score', 0.7341761510206308)]\n",
      "Got result for mathvista - 2400: [('mathvista/accuracy', 0.467)]\n",
      "Got result for ai2d - 2400: [('ai2d/accuracy', 0.8251295336787565)]\n",
      "Got result for chartqa - 2400: [('chartqa/accuracy', 0.5427929311938108)]\n",
      "Got result for vqa - 2400: [('vqa/accuracy', 0.7522399999999761), ('vqa/recall', 0.7724039999999743), ('vqa/bleu', 0.0341523103415966), ('vqa/mllm_evaluation_accuracy', 0.7697839999999759)]\n",
      "Got result for textvqa - 2400: [('textvqa/accuracy', 71.00000000000037), ('textvqa/mllm_eval_accuracy', 75.18400000000042)]\n",
      "Got result for infographics_w_ocr - 2400: [('infographics_w_ocr/anls_total_score', 0.6606641518011869), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6086817445911927), ('infographics_w_ocr/answer_type_multi_span_score', 0.5224283464538997), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6460753532182106), ('infographics_w_ocr/answer_type_question_span_score', 0.7320149904143117), ('infographics_w_ocr/answer_type_single_span_score', 0.6742958949726682), ('infographics_w_ocr/evidence_type_figure_score', 0.6417492057183953), ('infographics_w_ocr/evidence_type_map_score', 0.626653338410764), ('infographics_w_ocr/evidence_type_table_list_score', 0.6590362453335669), ('infographics_w_ocr/evidence_type_text_score', 0.7077916775898918), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5628328344128831), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6458207888002406), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5783300207435994), ('infographics_w_ocr/reasoning_type_counting_score', 0.633916083916084)]\n",
      "Got result for infographics - 2400: [('infographics/anls_total_score', 0.560204735984656), ('infographics/mllm_evaluation_anls_score', 0.49773481712398854), ('infographics/answer_type_multi_span_score', 0.39924180236048384), ('infographics/answer_type_non_extractive_score', 0.5462003236867613), ('infographics/answer_type_question_span_score', 0.7151339579224194), ('infographics/answer_type_single_span_score', 0.5739897261324877), ('infographics/evidence_type_figure_score', 0.5543242736495142), ('infographics/evidence_type_map_score', 0.5631220432075974), ('infographics/evidence_type_table_list_score', 0.5212411681694655), ('infographics/evidence_type_text_score', 0.6088891955161908), ('infographics/evidence_type_visual_layout_score', 0.4818020442753523), ('infographics/reasoning_type_arithmetic_score', 0.5174615719136266), ('infographics/reasoning_type_comparison_score', 0.47793978932395753), ('infographics/reasoning_type_counting_score', 0.5670163170163169)]\n",
      "Got result for mmbench - 2400: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9423076923076923), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.5974025974025974), ('mmbench/overall', 0.7429375017100799)]\n",
      "2400\n",
      "Got result for mmmu - 2600: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.5), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.6333333333333333), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.7), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.4), ('mmmu/geography', 0.6), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.26666666666666666), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.5666666666666667), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.57), ('mmmu/mllm_eval_accuracy', 0.5788888888888889)]\n",
      "Got result for docvqa - 2600: [('docvqa/anls_total_score', 0.6954683645192988), ('docvqa/mllm_evaluation_anls_score', 0.6942062397260953), ('docvqa/mmllm_fixed_anls_score', 0.7308686722779693)]\n",
      "Got result for mathvista - 2600: [('mathvista/accuracy', 0.473)]\n",
      "Got result for ai2d - 2600: [('ai2d/accuracy', 0.8309585492227979)]\n",
      "Got result for chartqa - 2600: [('chartqa/accuracy', 0.5408773089242913)]\n",
      "Got result for vqa - 2600: [('vqa/accuracy', 0.7553119999999753), ('vqa/recall', 0.7759119999999748), ('vqa/bleu', 0.03813748061656952), ('vqa/mllm_evaluation_accuracy', 0.773291999999976)]\n",
      "Got result for textvqa - 2600: [('textvqa/accuracy', 70.18000000000038), ('textvqa/mllm_eval_accuracy', 74.16800000000043)]\n",
      "Got result for infographics_w_ocr - 2600: [('infographics_w_ocr/anls_total_score', 0.651164724928565), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6013482421058045), ('infographics_w_ocr/answer_type_multi_span_score', 0.5294426409286509), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6439028873477339), ('infographics_w_ocr/answer_type_question_span_score', 0.7077984661977874), ('infographics_w_ocr/answer_type_single_span_score', 0.6636168014911752), ('infographics_w_ocr/evidence_type_figure_score', 0.6277577706777167), ('infographics_w_ocr/evidence_type_map_score', 0.5909161160398783), ('infographics_w_ocr/evidence_type_table_list_score', 0.6500928113222694), ('infographics_w_ocr/evidence_type_text_score', 0.6991038018012479), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5959493163988981), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6465010161071801), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5506955322634512), ('infographics_w_ocr/reasoning_type_counting_score', 0.6258741258741258)]\n",
      "Got result for infographics - 2600: [('infographics/anls_total_score', 0.5619697467117214), ('infographics/mllm_evaluation_anls_score', 0.4980160659819635), ('infographics/answer_type_multi_span_score', 0.39913251393046323), ('infographics/answer_type_non_extractive_score', 0.5500272682912828), ('infographics/answer_type_question_span_score', 0.7284607753357754), ('infographics/answer_type_single_span_score', 0.5738720440481327), ('infographics/evidence_type_figure_score', 0.547114900359438), ('infographics/evidence_type_map_score', 0.5254432753308097), ('infographics/evidence_type_table_list_score', 0.5310237024243538), ('infographics/evidence_type_text_score', 0.6104759745010804), ('infographics/evidence_type_visual_layout_score', 0.5460960893108907), ('infographics/reasoning_type_arithmetic_score', 0.5014785823005), ('infographics/reasoning_type_comparison_score', 0.4698321601005313), ('infographics/reasoning_type_counting_score', 0.5891608391608392)]\n",
      "Got result for mmbench - 2600: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.55), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.4583333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6103896103896104), ('mmbench/overall', 0.7475072074801593)]\n",
      "2600\n",
      "Got result for mmmu - 2800: [('mmmu/accounting', 0.7), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.7333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.5), ('mmmu/geography', 0.6666666666666666), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.5333333333333333), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.8), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5844444444444444), ('mmmu/mllm_eval_accuracy', 0.5944444444444444)]\n",
      "Got result for docvqa - 2800: [('docvqa/anls_total_score', 0.6976693869159347), ('docvqa/mllm_evaluation_anls_score', 0.6977460875410136), ('docvqa/mmllm_fixed_anls_score', 0.736589807646552)]\n",
      "Got result for mathvista - 2800: [('mathvista/accuracy', 0.457)]\n",
      "Got result for ai2d - 2800: [('ai2d/accuracy', 0.8228626943005182)]\n",
      "Got result for chartqa - 2800: [('chartqa/accuracy', 0.5281946093052187)]\n",
      "Got result for vqa - 2800: [('vqa/accuracy', 0.7512839999999753), ('vqa/recall', 0.7739279999999741), ('vqa/bleu', 0.027705319225788116), ('vqa/mllm_evaluation_accuracy', 0.7707359999999759)]\n",
      "Got result for textvqa - 2800: [('textvqa/accuracy', 70.66800000000038), ('textvqa/mllm_eval_accuracy', 74.80000000000044)]\n",
      "Got result for infographics_w_ocr - 2800: [('infographics_w_ocr/anls_total_score', 0.6531590609303768), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6018078191959201), ('infographics_w_ocr/answer_type_multi_span_score', 0.5239336808881732), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6392575628109085), ('infographics_w_ocr/answer_type_question_span_score', 0.7090449049442262), ('infographics_w_ocr/answer_type_single_span_score', 0.6672114646029506), ('infographics_w_ocr/evidence_type_figure_score', 0.6341152637744527), ('infographics_w_ocr/evidence_type_map_score', 0.6179074638233054), ('infographics_w_ocr/evidence_type_table_list_score', 0.656370440292388), ('infographics_w_ocr/evidence_type_text_score', 0.6951664809076201), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5573929783707291), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.628799425460384), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5582325241961964), ('infographics_w_ocr/reasoning_type_counting_score', 0.6413170163170162)]\n",
      "Got result for infographics - 2800: [('infographics/anls_total_score', 0.550622000958938), ('infographics/mllm_evaluation_anls_score', 0.4931451983890792), ('infographics/answer_type_multi_span_score', 0.376389502056889), ('infographics/answer_type_non_extractive_score', 0.5522363310790074), ('infographics/answer_type_question_span_score', 0.6960088522588523), ('infographics/answer_type_single_span_score', 0.5615219833702023), ('infographics/evidence_type_figure_score', 0.5411372190688342), ('infographics/evidence_type_map_score', 0.5812660004604655), ('infographics/evidence_type_table_list_score', 0.5207637487848971), ('infographics/evidence_type_text_score', 0.5824082087759574), ('infographics/evidence_type_visual_layout_score', 0.5091396442761439), ('infographics/reasoning_type_arithmetic_score', 0.5070320471005402), ('infographics/reasoning_type_comparison_score', 0.44567070537587594), ('infographics/reasoning_type_counting_score', 0.5888694638694638)]\n",
      "Got result for mmbench - 2800: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.5), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7571794299865611)]\n",
      "2800\n",
      "Got result for mmmu - 3000: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.6666666666666666), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.6), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.5), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7), ('mmmu/diagnostics_and_laboratory_medicine', 0.3333333333333333), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.4666666666666667), ('mmmu/energy_and_power', 0.6333333333333333), ('mmmu/finance', 0.4), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.7), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.5666666666666667), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.8666666666666667), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5711111111111111), ('mmmu/mllm_eval_accuracy', 0.58)]\n",
      "Got result for docvqa - 3000: [('docvqa/anls_total_score', 0.6854900885286632), ('docvqa/mllm_evaluation_anls_score', 0.6844136228000655), ('docvqa/mmllm_fixed_anls_score', 0.7223934884987383)]\n",
      "Got result for mathvista - 3000: [('mathvista/accuracy', 0.482)]\n",
      "Got result for ai2d - 3000: [('ai2d/accuracy', 0.8244818652849741)]\n",
      "Got result for chartqa - 3000: [('chartqa/accuracy', 0.5383905956706266)]\n",
      "Got result for vqa - 3000: [('vqa/accuracy', 0.7551679999999747), ('vqa/recall', 0.7745479999999736), ('vqa/bleu', 0.0471799261868), ('vqa/mllm_evaluation_accuracy', 0.7725599999999748)]\n",
      "Got result for textvqa - 3000: [('textvqa/accuracy', 70.89600000000037), ('textvqa/mllm_eval_accuracy', 74.85400000000043)]\n",
      "Got result for infographics_w_ocr - 3000: [('infographics_w_ocr/anls_total_score', 0.6471103621065909), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5940251732315768), ('infographics_w_ocr/answer_type_multi_span_score', 0.5305115866254025), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6333591664513909), ('infographics_w_ocr/answer_type_question_span_score', 0.7215754299747512), ('infographics_w_ocr/answer_type_single_span_score', 0.6598633424504501), ('infographics_w_ocr/evidence_type_figure_score', 0.627550796673074), ('infographics_w_ocr/evidence_type_map_score', 0.6357763935734232), ('infographics_w_ocr/evidence_type_table_list_score', 0.6438686695747734), ('infographics_w_ocr/evidence_type_text_score', 0.6974593683302565), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5314446412894178), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6320123939986951), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5410602019159332), ('infographics_w_ocr/reasoning_type_counting_score', 0.6203379953379953)]\n",
      "Got result for infographics - 3000: [('infographics/anls_total_score', 0.5547711623340286), ('infographics/mllm_evaluation_anls_score', 0.4929341511735586), ('infographics/answer_type_multi_span_score', 0.3897428668607784), ('infographics/answer_type_non_extractive_score', 0.5338115772112156), ('infographics/answer_type_question_span_score', 0.7501420587959049), ('infographics/answer_type_single_span_score', 0.5673327543490795), ('infographics/evidence_type_figure_score', 0.5479717393199617), ('infographics/evidence_type_map_score', 0.5369796226814282), ('infographics/evidence_type_table_list_score', 0.5151056857905315), ('infographics/evidence_type_text_score', 0.5893852537548887), ('infographics/evidence_type_visual_layout_score', 0.5121682194999595), ('infographics/reasoning_type_arithmetic_score', 0.4890335691705555), ('infographics/reasoning_type_comparison_score', 0.48278496980994257), ('infographics/reasoning_type_counting_score', 0.574009324009324)]\n",
      "Got result for mmbench - 3000: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.9014084507042254), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6933333333333334), ('mmbench/physical_relation', 0.5), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6753246753246753), ('mmbench/overall', 0.7600706385215489)]\n",
      "3000\n",
      "Got result for mmmu - 3200: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.5666666666666667), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.6), ('mmmu/materials', 0.5), ('mmmu/math', 0.36666666666666664), ('mmmu/mechanical_engineering', 0.23333333333333334), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.7333333333333333), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5566666666666666), ('mmmu/mllm_eval_accuracy', 0.5677777777777778)]\n",
      "Got result for docvqa - 3200: [('docvqa/anls_total_score', 0.6884175614823189), ('docvqa/mllm_evaluation_anls_score', 0.6864299815427379), ('docvqa/mmllm_fixed_anls_score', 0.7239035558099187)]\n",
      "Got result for mathvista - 3200: [('mathvista/accuracy', 0.466)]\n",
      "Got result for ai2d - 3200: [('ai2d/accuracy', 0.8244818652849741)]\n",
      "Got result for chartqa - 3200: [('chartqa/accuracy', 0.5424814537419674)]\n",
      "Got result for vqa - 3200: [('vqa/accuracy', 0.7527399999999749), ('vqa/recall', 0.7726199999999747), ('vqa/bleu', 0.04370268061757088), ('vqa/mllm_evaluation_accuracy', 0.7700399999999754)]\n",
      "Got result for textvqa - 3200: [('textvqa/accuracy', 70.49400000000034), ('textvqa/mllm_eval_accuracy', 74.49600000000042)]\n",
      "Got result for infographics_w_ocr - 3200: [('infographics_w_ocr/anls_total_score', 0.652866522133115), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6020094594467755), ('infographics_w_ocr/answer_type_multi_span_score', 0.5300797866485243), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6575841729096706), ('infographics_w_ocr/answer_type_question_span_score', 0.7048499577345732), ('infographics_w_ocr/answer_type_single_span_score', 0.66191766580243), ('infographics_w_ocr/evidence_type_figure_score', 0.6322301243040545), ('infographics_w_ocr/evidence_type_map_score', 0.6239435937520877), ('infographics_w_ocr/evidence_type_table_list_score', 0.6490357967175435), ('infographics_w_ocr/evidence_type_text_score', 0.6979294761976276), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.572896105671597), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6533471950424002), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5348780193596397), ('infographics_w_ocr/reasoning_type_counting_score', 0.6401515151515151)]\n",
      "Got result for infographics - 3200: [('infographics/anls_total_score', 0.5580369040368354), ('infographics/mllm_evaluation_anls_score', 0.4922397548697688), ('infographics/answer_type_multi_span_score', 0.39734432821950927), ('infographics/answer_type_non_extractive_score', 0.5399067177548191), ('infographics/answer_type_question_span_score', 0.720631398516014), ('infographics/answer_type_single_span_score', 0.5729569186766982), ('infographics/evidence_type_figure_score', 0.5484160959838147), ('infographics/evidence_type_map_score', 0.5818768032989912), ('infographics/evidence_type_table_list_score', 0.5243729994886941), ('infographics/evidence_type_text_score', 0.5880637977591125), ('infographics/evidence_type_visual_layout_score', 0.5027206473250383), ('infographics/reasoning_type_arithmetic_score', 0.5058849825973112), ('infographics/reasoning_type_comparison_score', 0.5128840851798792), ('infographics/reasoning_type_counting_score', 0.5641608391608391)]\n",
      "Got result for mmbench - 3200: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6266666666666667), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.744608635227715)]\n",
      "3200\n",
      "Got result for mmmu - 3400: [('mmmu/accounting', 0.6), ('mmmu/agriculture', 0.6666666666666666), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.3), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.3333333333333333), ('mmmu/economics', 0.5), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.5), ('mmmu/finance', 0.5), ('mmmu/geography', 0.43333333333333335), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.26666666666666666), ('mmmu/math', 0.6), ('mmmu/mechanical_engineering', 0.26666666666666666), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.5666666666666667), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7666666666666667), ('mmmu/accuracy', 0.5544444444444444), ('mmmu/mllm_eval_accuracy', 0.5688888888888889)]\n",
      "Got result for docvqa - 3400: [('docvqa/anls_total_score', 0.6920595635982579), ('docvqa/mllm_evaluation_anls_score', 0.6919151292039054), ('docvqa/mmllm_fixed_anls_score', 0.731105033051952)]\n",
      "Got result for mathvista - 3400: [('mathvista/accuracy', 0.485)]\n",
      "Got result for ai2d - 3400: [('ai2d/accuracy', 0.819300518134715)]\n",
      "Got result for chartqa - 3400: [('chartqa/accuracy', 0.5320210262027238)]\n",
      "Got result for vqa - 3400: [('vqa/accuracy', 0.7519959999999745), ('vqa/recall', 0.7713239999999741), ('vqa/bleu', 0.04670434445142746), ('vqa/mllm_evaluation_accuracy', 0.7694439999999754)]\n",
      "Got result for textvqa - 3400: [('textvqa/accuracy', 70.15600000000035), ('textvqa/mllm_eval_accuracy', 74.21200000000039)]\n",
      "Got result for infographics_w_ocr - 3400: [('infographics_w_ocr/anls_total_score', 0.6522810064646545), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5990834132139811), ('infographics_w_ocr/answer_type_multi_span_score', 0.5338257551027108), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6467316137750136), ('infographics_w_ocr/answer_type_question_span_score', 0.7310251714097868), ('infographics_w_ocr/answer_type_single_span_score', 0.6637450724193565), ('infographics_w_ocr/evidence_type_figure_score', 0.6315288521593815), ('infographics_w_ocr/evidence_type_map_score', 0.6296949939276671), ('infographics_w_ocr/evidence_type_table_list_score', 0.6554964226747275), ('infographics_w_ocr/evidence_type_text_score', 0.6997925222454932), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5458192272987659), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6551115836218573), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5496486449585314), ('infographics_w_ocr/reasoning_type_counting_score', 0.6255827505827506)]\n",
      "Got result for infographics - 3400: [('infographics/anls_total_score', 0.5578659676448343), ('infographics/mllm_evaluation_anls_score', 0.49840432062246415), ('infographics/answer_type_multi_span_score', 0.38973175378381825), ('infographics/answer_type_non_extractive_score', 0.545390058003078), ('infographics/answer_type_question_span_score', 0.7124659528505682), ('infographics/answer_type_single_span_score', 0.5717619075383957), ('infographics/evidence_type_figure_score', 0.5513982372533459), ('infographics/evidence_type_map_score', 0.5514784063164357), ('infographics/evidence_type_table_list_score', 0.5210992588409312), ('infographics/evidence_type_text_score', 0.5963277370090824), ('infographics/evidence_type_visual_layout_score', 0.5024110793585861), ('infographics/reasoning_type_arithmetic_score', 0.5016576555103951), ('infographics/reasoning_type_comparison_score', 0.47542669233864104), ('infographics/reasoning_type_counting_score', 0.5726107226107225)]\n",
      "Got result for mmbench - 3400: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.625), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7601470565840391)]\n",
      "3400\n",
      "Got result for mmmu - 3600: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.7666666666666667), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.4666666666666667), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.5), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.8), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.6333333333333333), ('mmmu/materials', 0.26666666666666666), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5688888888888889), ('mmmu/mllm_eval_accuracy', 0.5744444444444444)]\n",
      "Got result for docvqa - 3600: [('docvqa/anls_total_score', 0.6990456918028961), ('docvqa/mllm_evaluation_anls_score', 0.6984530184091614), ('docvqa/mmllm_fixed_anls_score', 0.7358869951384686)]\n",
      "Got result for mathvista - 3600: [('mathvista/accuracy', 0.487)]\n",
      "Got result for ai2d - 3600: [('ai2d/accuracy', 0.8280440414507773)]\n",
      "Got result for chartqa - 3600: [('chartqa/accuracy', 0.5595985179835642)]\n",
      "Got result for vqa - 3600: [('vqa/accuracy', 0.7528999999999746), ('vqa/recall', 0.7742439999999746), ('vqa/bleu', 0.045805346220731735), ('vqa/mllm_evaluation_accuracy', 0.7712079999999755)]\n",
      "Got result for textvqa - 3600: [('textvqa/accuracy', 69.80800000000033), ('textvqa/mllm_eval_accuracy', 74.09000000000037)]\n",
      "Got result for infographics_w_ocr - 3600: [('infographics_w_ocr/anls_total_score', 0.6530705749222349), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5995033460946686), ('infographics_w_ocr/answer_type_multi_span_score', 0.52920165540935), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6424669965357128), ('infographics_w_ocr/answer_type_question_span_score', 0.7270187611533766), ('infographics_w_ocr/answer_type_single_span_score', 0.6645258146587915), ('infographics_w_ocr/evidence_type_figure_score', 0.6388948497303798), ('infographics_w_ocr/evidence_type_map_score', 0.6112679763451457), ('infographics_w_ocr/evidence_type_table_list_score', 0.6450198037322603), ('infographics_w_ocr/evidence_type_text_score', 0.6987870181203262), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5631503294950186), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6380739123889807), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5482505209949791), ('infographics_w_ocr/reasoning_type_counting_score', 0.6349067599067598)]\n",
      "Got result for infographics - 3600: [('infographics/anls_total_score', 0.5580228017628018), ('infographics/mllm_evaluation_anls_score', 0.497256049589339), ('infographics/answer_type_multi_span_score', 0.36771584580512506), ('infographics/answer_type_non_extractive_score', 0.5543664593393347), ('infographics/answer_type_question_span_score', 0.7458527049873204), ('infographics/answer_type_single_span_score', 0.5690640269545204), ('infographics/evidence_type_figure_score', 0.5465349527564461), ('infographics/evidence_type_map_score', 0.5550476045420114), ('infographics/evidence_type_table_list_score', 0.5219323097454984), ('infographics/evidence_type_text_score', 0.6036297170852951), ('infographics/evidence_type_visual_layout_score', 0.5059497684427389), ('infographics/reasoning_type_arithmetic_score', 0.495426890461137), ('infographics/reasoning_type_comparison_score', 0.4579134140620034), ('infographics/reasoning_type_counting_score', 0.5964452214452214)]\n",
      "Got result for mmbench - 3600: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7522218380507352)]\n",
      "3600\n",
      "Got result for mmmu - 3800: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.4666666666666667), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.6), ('mmmu/biology', 0.4666666666666667), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5333333333333333), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.3), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.16666666666666666), ('mmmu/pharmacy', 0.6), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.7), ('mmmu/sociology', 0.7666666666666667), ('mmmu/accuracy', 0.5633333333333334), ('mmmu/mllm_eval_accuracy', 0.5666666666666667)]\n",
      "Got result for docvqa - 3800: [('docvqa/anls_total_score', 0.6920638334513775), ('docvqa/mllm_evaluation_anls_score', 0.6911548393759033), ('docvqa/mmllm_fixed_anls_score', 0.7283273600620433)]\n",
      "Got result for mathvista - 3800: [('mathvista/accuracy', 0.489)]\n",
      "Got result for ai2d - 3800: [('ai2d/accuracy', 0.8293393782383419)]\n",
      "Got result for chartqa - 3800: [('chartqa/accuracy', 0.5441613461705116)]\n",
      "Got result for vqa - 3800: [('vqa/accuracy', 0.7538279999999754), ('vqa/recall', 0.7744479999999747), ('vqa/bleu', 0.0488370917737484), ('vqa/mllm_evaluation_accuracy', 0.7718399999999757)]\n",
      "Got result for textvqa - 3800: [('textvqa/accuracy', 70.78400000000035), ('textvqa/mllm_eval_accuracy', 74.95200000000042)]\n",
      "Got result for infographics_w_ocr - 3800: [('infographics_w_ocr/anls_total_score', 0.6554426110524546), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6027773076894009), ('infographics_w_ocr/answer_type_multi_span_score', 0.5408017447028147), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6481809179367952), ('infographics_w_ocr/answer_type_question_span_score', 0.6916097257443412), ('infographics_w_ocr/answer_type_single_span_score', 0.6683206807714822), ('infographics_w_ocr/evidence_type_figure_score', 0.640820626600638), ('infographics_w_ocr/evidence_type_map_score', 0.6761582889058135), ('infographics_w_ocr/evidence_type_table_list_score', 0.649744304967638), ('infographics_w_ocr/evidence_type_text_score', 0.7033787063863371), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5647286591219027), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6589408023483363), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5572993120357926), ('infographics_w_ocr/reasoning_type_counting_score', 0.6197552447552448)]\n",
      "Got result for infographics - 3800: [('infographics/anls_total_score', 0.5633542383955912), ('infographics/mllm_evaluation_anls_score', 0.4976516791616558), ('infographics/answer_type_multi_span_score', 0.414230358323126), ('infographics/answer_type_non_extractive_score', 0.5551618213010618), ('infographics/answer_type_question_span_score', 0.70290537453999), ('infographics/answer_type_single_span_score', 0.57483458030508), ('infographics/evidence_type_figure_score', 0.5532512430758209), ('infographics/evidence_type_map_score', 0.5570064591217163), ('infographics/evidence_type_table_list_score', 0.5370266622109579), ('infographics/evidence_type_text_score', 0.605010198597999), ('infographics/evidence_type_visual_layout_score', 0.5024688354562336), ('infographics/reasoning_type_arithmetic_score', 0.5109171935370563), ('infographics/reasoning_type_comparison_score', 0.4692879414209123), ('infographics/reasoning_type_counting_score', 0.594114219114219)]\n",
      "Got result for mmbench - 3800: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.5432098765432098), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.7547104793823574)]\n",
      "3800\n",
      "Got result for mmmu - 4000: [('mmmu/accounting', 0.6333333333333333), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.9333333333333333), ('mmmu/basic_medical_science', 0.7666666666666667), ('mmmu/biology', 0.4666666666666667), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.4666666666666667), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.6), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7333333333333333), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.5911111111111111), ('mmmu/mllm_eval_accuracy', 0.5944444444444444)]\n",
      "Got result for docvqa - 4000: [('docvqa/anls_total_score', 0.6860768388956411), ('docvqa/mllm_evaluation_anls_score', 0.68520896948228), ('docvqa/mmllm_fixed_anls_score', 0.7230157412987918)]\n",
      "Got result for mathvista - 4000: [('mathvista/accuracy', 0.488)]\n",
      "Got result for ai2d - 4000: [('ai2d/accuracy', 0.8238341968911918)]\n",
      "Got result for chartqa - 4000: [('chartqa/accuracy', 0.5506320224850985)]\n",
      "Got result for vqa - 4000: [('vqa/accuracy', 0.7519879999999751), ('vqa/recall', 0.7734279999999746), ('vqa/bleu', 0.04753536358475685), ('vqa/mllm_evaluation_accuracy', 0.7707319999999758)]\n",
      "Got result for textvqa - 4000: [('textvqa/accuracy', 70.56600000000037), ('textvqa/mllm_eval_accuracy', 74.54000000000042)]\n",
      "Got result for infographics_w_ocr - 4000: [('infographics_w_ocr/anls_total_score', 0.6530764427705144), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6005057891678327), ('infographics_w_ocr/answer_type_multi_span_score', 0.5368301074996702), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6460586881472962), ('infographics_w_ocr/answer_type_question_span_score', 0.7205833664826878), ('infographics_w_ocr/answer_type_single_span_score', 0.662973870385425), ('infographics_w_ocr/evidence_type_figure_score', 0.6283901732816229), ('infographics_w_ocr/evidence_type_map_score', 0.6383695100279257), ('infographics_w_ocr/evidence_type_table_list_score', 0.6628347362154484), ('infographics_w_ocr/evidence_type_text_score', 0.6970006803570912), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5687935990004084), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6452184516396842), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5562953848434483), ('infographics_w_ocr/reasoning_type_counting_score', 0.6366550116550116)]\n",
      "Got result for infographics - 4000: [('infographics/anls_total_score', 0.5615054244784362), ('infographics/mllm_evaluation_anls_score', 0.4939268392192973), ('infographics/answer_type_multi_span_score', 0.40748464442552734), ('infographics/answer_type_non_extractive_score', 0.5461232107253808), ('infographics/answer_type_question_span_score', 0.7422202263548418), ('infographics/answer_type_single_span_score', 0.5730403589469466), ('infographics/evidence_type_figure_score', 0.5437115338668302), ('infographics/evidence_type_map_score', 0.5281386342968662), ('infographics/evidence_type_table_list_score', 0.5435541824785185), ('infographics/evidence_type_text_score', 0.5969694222943102), ('infographics/evidence_type_visual_layout_score', 0.5145574490088946), ('infographics/reasoning_type_arithmetic_score', 0.4998497792162174), ('infographics/reasoning_type_comparison_score', 0.484593639311937), ('infographics/reasoning_type_counting_score', 0.5952797202797202)]\n",
      "Got result for mmbench - 4000: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.5185185185185185), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.7066666666666667), ('mmbench/physical_relation', 0.4583333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.5974025974025974), ('mmbench/overall', 0.7591957972071346)]\n",
      "4000\n",
      "Got result for mmmu - 4200: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.9), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.7), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.4), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.6333333333333333), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.3), ('mmmu/music', 0.2), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.8333333333333334), ('mmmu/sociology', 0.5666666666666667), ('mmmu/accuracy', 0.5622222222222222), ('mmmu/mllm_eval_accuracy', 0.5722222222222222)]\n",
      "Got result for docvqa - 4200: [('docvqa/anls_total_score', 0.6909663819942671), ('docvqa/mllm_evaluation_anls_score', 0.6890248476177889), ('docvqa/mmllm_fixed_anls_score', 0.7247965743751706)]\n",
      "Got result for mathvista - 4200: [('mathvista/accuracy', 0.472)]\n",
      "Got result for ai2d - 4200: [('ai2d/accuracy', 0.8254533678756477)]\n",
      "Got result for chartqa - 4200: [('chartqa/accuracy', 0.5546165839617534)]\n",
      "Got result for vqa - 4200: [('vqa/accuracy', 0.7541879999999751), ('vqa/recall', 0.7757559999999741), ('vqa/bleu', 0.04487704113125801), ('vqa/mllm_evaluation_accuracy', 0.7731199999999752)]\n",
      "Got result for textvqa - 4200: [('textvqa/accuracy', 70.55200000000039), ('textvqa/mllm_eval_accuracy', 74.56800000000044)]\n",
      "Got result for infographics_w_ocr - 4200: [('infographics_w_ocr/anls_total_score', 0.6554969780193748), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6035238223347938), ('infographics_w_ocr/answer_type_multi_span_score', 0.525866019058106), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6564341023653866), ('infographics_w_ocr/answer_type_question_span_score', 0.7215220111713325), ('infographics_w_ocr/answer_type_single_span_score', 0.6655972338335036), ('infographics_w_ocr/evidence_type_figure_score', 0.6352453504489496), ('infographics_w_ocr/evidence_type_map_score', 0.6253332063975627), ('infographics_w_ocr/evidence_type_table_list_score', 0.6625141628715077), ('infographics_w_ocr/evidence_type_text_score', 0.6996736298509091), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5621706640929052), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6591600180641273), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5464334005626414), ('infographics_w_ocr/reasoning_type_counting_score', 0.6424825174825175)]\n",
      "Got result for infographics - 4200: [('infographics/anls_total_score', 0.554261476452139), ('infographics/mllm_evaluation_anls_score', 0.49264494436226997), ('infographics/answer_type_multi_span_score', 0.39572288306518333), ('infographics/answer_type_non_extractive_score', 0.5356048261563634), ('infographics/answer_type_question_span_score', 0.6922049638395792), ('infographics/answer_type_single_span_score', 0.5686185912607391), ('infographics/evidence_type_figure_score', 0.5418123711190144), ('infographics/evidence_type_map_score', 0.5323349178419168), ('infographics/evidence_type_table_list_score', 0.5283550495801679), ('infographics/evidence_type_text_score', 0.6077979568910364), ('infographics/evidence_type_visual_layout_score', 0.5231948304061107), ('infographics/reasoning_type_arithmetic_score', 0.4914022906317425), ('infographics/reasoning_type_comparison_score', 0.46512953627105036), ('infographics/reasoning_type_counting_score', 0.5635198135198135)]\n",
      "Got result for mmbench - 4200: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9423076923076923), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.7066666666666667), ('mmbench/physical_relation', 0.5), ('mmbench/spatial_relationship', 0.5), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7597848193042702)]\n",
      "4200\n",
      "Got result for mmmu - 4400: [('mmmu/accounting', 0.5666666666666667), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.9), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.7333333333333333), ('mmmu/computer_science', 0.7), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.3333333333333333), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.4), ('mmmu/mechanical_engineering', 0.23333333333333334), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.5333333333333333), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.8), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.5622222222222222), ('mmmu/mllm_eval_accuracy', 0.5766666666666667)]\n",
      "Got result for docvqa - 4400: [('docvqa/anls_total_score', 0.687059863042456), ('docvqa/mllm_evaluation_anls_score', 0.6874393600193144), ('docvqa/mmllm_fixed_anls_score', 0.7259460025814127)]\n",
      "Got result for mathvista - 4400: [('mathvista/accuracy', 0.478)]\n",
      "Got result for ai2d - 4400: [('ai2d/accuracy', 0.8254533678756477)]\n",
      "Got result for chartqa - 4400: [('chartqa/accuracy', 0.5627272819115826)]\n",
      "Got result for vqa - 4400: [('vqa/accuracy', 0.7509559999999759), ('vqa/recall', 0.7733199999999747), ('vqa/bleu', 0.027833901345729828), ('vqa/mllm_evaluation_accuracy', 0.7699719999999762)]\n",
      "Got result for textvqa - 4400: [('textvqa/accuracy', 70.29200000000036), ('textvqa/mllm_eval_accuracy', 74.29200000000043)]\n",
      "Got result for infographics_w_ocr - 4400: [('infographics_w_ocr/anls_total_score', 0.6582673357826653), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6058047167865134), ('infographics_w_ocr/answer_type_multi_span_score', 0.5494791100047535), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6575845041034917), ('infographics_w_ocr/answer_type_question_span_score', 0.6938205459698673), ('infographics_w_ocr/answer_type_single_span_score', 0.6691927711208586), ('infographics_w_ocr/evidence_type_figure_score', 0.6391277717362602), ('infographics_w_ocr/evidence_type_map_score', 0.6123291011089852), ('infographics_w_ocr/evidence_type_table_list_score', 0.6628408411357791), ('infographics_w_ocr/evidence_type_text_score', 0.6979990367517522), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5743064096004332), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6515213382507901), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5532515800493557), ('infographics_w_ocr/reasoning_type_counting_score', 0.6465617715617715)]\n",
      "Got result for infographics - 4400: [('infographics/anls_total_score', 0.5655256587195344), ('infographics/mllm_evaluation_anls_score', 0.5065413974418381), ('infographics/answer_type_multi_span_score', 0.39956879881935636), ('infographics/answer_type_non_extractive_score', 0.5507064364207221), ('infographics/answer_type_question_span_score', 0.730704658589274), ('infographics/answer_type_single_span_score', 0.5792862106746477), ('infographics/evidence_type_figure_score', 0.5565226384171452), ('infographics/evidence_type_map_score', 0.5794664590893478), ('infographics/evidence_type_table_list_score', 0.5390906968531408), ('infographics/evidence_type_text_score', 0.6010565873204857), ('infographics/evidence_type_visual_layout_score', 0.5003763593360613), ('infographics/reasoning_type_arithmetic_score', 0.5098424406643585), ('infographics/reasoning_type_comparison_score', 0.48767322202911983), ('infographics/reasoning_type_counting_score', 0.5865384615384615)]\n",
      "Got result for mmbench - 4400: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.9014084507042254), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7551217318253529)]\n",
      "4400\n",
      "Got result for mmmu - 4600: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.5333333333333333), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.9), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.6666666666666666), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.5), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.6333333333333333), ('mmmu/history', 0.8), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5833333333333334), ('mmmu/mllm_eval_accuracy', 0.5955555555555555)]\n",
      "Got result for docvqa - 4600: [('docvqa/anls_total_score', 0.6892635424861194), ('docvqa/mllm_evaluation_anls_score', 0.6898767886197251), ('docvqa/mmllm_fixed_anls_score', 0.7297969406713206)]\n",
      "Got result for mathvista - 4600: [('mathvista/accuracy', 0.494)]\n",
      "Got result for ai2d - 4600: [('ai2d/accuracy', 0.8264248704663213)]\n",
      "Got result for chartqa - 4600: [('chartqa/accuracy', 0.5542431155094005)]\n",
      "Got result for vqa - 4600: [('vqa/accuracy', 0.7542599999999763), ('vqa/recall', 0.7750559999999753), ('vqa/bleu', 0.030386371538043022), ('vqa/mllm_evaluation_accuracy', 0.7720639999999767)]\n",
      "Got result for textvqa - 4600: [('textvqa/accuracy', 70.89600000000034), ('textvqa/mllm_eval_accuracy', 74.96200000000039)]\n",
      "Got result for infographics_w_ocr - 4600: [('infographics_w_ocr/anls_total_score', 0.649927777637822), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5966051694465504), ('infographics_w_ocr/answer_type_multi_span_score', 0.5269805118487447), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6491362465141853), ('infographics_w_ocr/answer_type_question_span_score', 0.7488419134912347), ('infographics_w_ocr/answer_type_single_span_score', 0.6574164477541107), ('infographics_w_ocr/evidence_type_figure_score', 0.6299696204872672), ('infographics_w_ocr/evidence_type_map_score', 0.6073464077176949), ('infographics_w_ocr/evidence_type_table_list_score', 0.656641099134377), ('infographics_w_ocr/evidence_type_text_score', 0.6865704961750319), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5912388868166718), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6509897636609961), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5471717249113014), ('infographics_w_ocr/reasoning_type_counting_score', 0.6305361305361306)]\n",
      "Got result for infographics - 4600: [('infographics/anls_total_score', 0.5539331586500253), ('infographics/mllm_evaluation_anls_score', 0.4928319784925895), ('infographics/answer_type_multi_span_score', 0.4101462395909851), ('infographics/answer_type_non_extractive_score', 0.5358730931515742), ('infographics/answer_type_question_span_score', 0.7413121066967221), ('infographics/answer_type_single_span_score', 0.5653900673860024), ('infographics/evidence_type_figure_score', 0.5380880462929105), ('infographics/evidence_type_map_score', 0.5386429019437172), ('infographics/evidence_type_table_list_score', 0.521022059968053), ('infographics/evidence_type_text_score', 0.5960328430591154), ('infographics/evidence_type_visual_layout_score', 0.5487849403939866), ('infographics/reasoning_type_arithmetic_score', 0.4741592026694766), ('infographics/reasoning_type_comparison_score', 0.47215999251425034), ('infographics/reasoning_type_counting_score', 0.5932400932400932)]\n",
      "Got result for mmbench - 4600: [('mmbench/attribute_comparison', 0.6590909090909091), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.8717948717948718), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.4583333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7635052848919529)]\n",
      "4600\n",
      "Got result for mmmu - 4800: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.4666666666666667), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5333333333333333), ('mmmu/design', 0.7), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.4666666666666667), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.7), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.5666666666666667), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.7333333333333333), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.56), ('mmmu/mllm_eval_accuracy', 0.5677777777777778)]\n",
      "Got result for docvqa - 4800: [('docvqa/anls_total_score', 0.6923626888708494), ('docvqa/mllm_evaluation_anls_score', 0.6929402773748311), ('docvqa/mmllm_fixed_anls_score', 0.73125470630584)]\n",
      "Got result for mathvista - 4800: [('mathvista/accuracy', 0.478)]\n",
      "Got result for ai2d - 4800: [('ai2d/accuracy', 0.8270725388601037)]\n",
      "Got result for chartqa - 4800: [('chartqa/accuracy', 0.56371189310048)]\n",
      "Got result for vqa - 4800: [('vqa/accuracy', 0.7547319999999766), ('vqa/recall', 0.7766159999999754), ('vqa/bleu', 0.03306329995393753), ('vqa/mllm_evaluation_accuracy', 0.7736279999999769)]\n",
      "Got result for textvqa - 4800: [('textvqa/accuracy', 70.63200000000039), ('textvqa/mllm_eval_accuracy', 74.79000000000042)]\n",
      "Got result for infographics_w_ocr - 4800: [('infographics_w_ocr/anls_total_score', 0.6501342941103906), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5993894935174388), ('infographics_w_ocr/answer_type_multi_span_score', 0.5409352914740743), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6490565944001753), ('infographics_w_ocr/answer_type_question_span_score', 0.7168691885038039), ('infographics_w_ocr/answer_type_single_span_score', 0.6584685186204431), ('infographics_w_ocr/evidence_type_figure_score', 0.6330029205743357), ('infographics_w_ocr/evidence_type_map_score', 0.6406347365505781), ('infographics_w_ocr/evidence_type_table_list_score', 0.6504141461572726), ('infographics_w_ocr/evidence_type_text_score', 0.6963029759410463), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.572060887788017), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6449599202167692), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5483516467436097), ('infographics_w_ocr/reasoning_type_counting_score', 0.6343240093240093)]\n",
      "Got result for infographics - 4800: [('infographics/anls_total_score', 0.5557553160110615), ('infographics/mllm_evaluation_anls_score', 0.49137741656313116), ('infographics/answer_type_multi_span_score', 0.4019411171619812), ('infographics/answer_type_non_extractive_score', 0.5412701724636628), ('infographics/answer_type_question_span_score', 0.7186091152437307), ('infographics/answer_type_single_span_score', 0.5693157307136697), ('infographics/evidence_type_figure_score', 0.541837718383754), ('infographics/evidence_type_map_score', 0.5484945732010685), ('infographics/evidence_type_table_list_score', 0.5292625033445473), ('infographics/evidence_type_text_score', 0.595096315108011), ('infographics/evidence_type_visual_layout_score', 0.5165222280144574), ('infographics/reasoning_type_arithmetic_score', 0.488261662234265), ('infographics/reasoning_type_comparison_score', 0.47973920588534846), ('infographics/reasoning_type_counting_score', 0.5935314685314683)]\n",
      "Got result for mmbench - 4800: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.7515880980086499)]\n",
      "4800\n",
      "Got result for mmmu - 5000: [('mmmu/accounting', 0.5666666666666667), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.4666666666666667), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.3333333333333333), ('mmmu/energy_and_power', 0.5666666666666667), ('mmmu/finance', 0.36666666666666664), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.6), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.3), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.8333333333333334), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5755555555555556), ('mmmu/mllm_eval_accuracy', 0.5866666666666667)]\n",
      "Got result for docvqa - 5000: [('docvqa/anls_total_score', 0.6933501113052803), ('docvqa/mllm_evaluation_anls_score', 0.6930542427910802), ('docvqa/mmllm_fixed_anls_score', 0.7328135620944147)]\n",
      "Got result for mathvista - 5000: [('mathvista/accuracy', 0.489)]\n",
      "Got result for ai2d - 5000: [('ai2d/accuracy', 0.8218911917098446)]\n",
      "Got result for chartqa - 5000: [('chartqa/accuracy', 0.5470226169509819)]\n",
      "Got result for vqa - 5000: [('vqa/accuracy', 0.7518359999999749), ('vqa/recall', 0.7750679999999739), ('vqa/bleu', 0.027631811797618866), ('vqa/mllm_evaluation_accuracy', 0.7715999999999754)]\n",
      "Got result for textvqa - 5000: [('textvqa/accuracy', 70.41600000000035), ('textvqa/mllm_eval_accuracy', 74.6060000000004)]\n",
      "Got result for infographics_w_ocr - 5000: [('infographics_w_ocr/anls_total_score', 0.6519914458469714), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5988744941070757), ('infographics_w_ocr/answer_type_multi_span_score', 0.5422063569828365), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6541447802749796), ('infographics_w_ocr/answer_type_question_span_score', 0.6920294449140604), ('infographics_w_ocr/answer_type_single_span_score', 0.6605171879816227), ('infographics_w_ocr/evidence_type_figure_score', 0.6355812273234858), ('infographics_w_ocr/evidence_type_map_score', 0.6082670260953221), ('infographics_w_ocr/evidence_type_table_list_score', 0.6458682000622288), ('infographics_w_ocr/evidence_type_text_score', 0.6980925456330691), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5858554196852831), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6542536420961074), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5516955329244927), ('infographics_w_ocr/reasoning_type_counting_score', 0.6413170163170163)]\n",
      "Got result for infographics - 5000: [('infographics/anls_total_score', 0.5615187695587205), ('infographics/mllm_evaluation_anls_score', 0.500880586596818), ('infographics/answer_type_multi_span_score', 0.39422995713180514), ('infographics/answer_type_non_extractive_score', 0.5562837403705398), ('infographics/answer_type_question_span_score', 0.7130353620738236), ('infographics/answer_type_single_span_score', 0.5734023355831113), ('infographics/evidence_type_figure_score', 0.5515254816899777), ('infographics/evidence_type_map_score', 0.5341111941062714), ('infographics/evidence_type_table_list_score', 0.5277132345189383), ('infographics/evidence_type_text_score', 0.6086290812239384), ('infographics/evidence_type_visual_layout_score', 0.4982179075621536), ('infographics/reasoning_type_arithmetic_score', 0.5152679512268551), ('infographics/reasoning_type_comparison_score', 0.48147483521189666), ('infographics/reasoning_type_counting_score', 0.5888694638694637)]\n",
      "Got result for mmbench - 5000: [('mmbench/attribute_comparison', 0.6590909090909091), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8611111111111112), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.72), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.7520414772016897)]\n",
      "5000\n",
      "Got result for mmmu - 5200: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.7666666666666667), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.4666666666666667), ('mmmu/design', 0.7), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.4666666666666667), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.4), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.6), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.8), ('mmmu/physics', 0.5333333333333333), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.7333333333333333), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5688888888888889), ('mmmu/mllm_eval_accuracy', 0.5755555555555556)]\n",
      "Got result for docvqa - 5200: [('docvqa/anls_total_score', 0.6900262209331468), ('docvqa/mllm_evaluation_anls_score', 0.6911725080284302), ('docvqa/mmllm_fixed_anls_score', 0.7322545211667849)]\n",
      "Got result for mathvista - 5200: [('mathvista/accuracy', 0.487)]\n",
      "Got result for ai2d - 5200: [('ai2d/accuracy', 0.8163860103626943)]\n",
      "Got result for chartqa - 5200: [('chartqa/accuracy', 0.570036011221631)]\n",
      "Got result for vqa - 5200: [('vqa/accuracy', 0.753887999999977), ('vqa/recall', 0.7732159999999756), ('vqa/bleu', 0.04276316612958908), ('vqa/mllm_evaluation_accuracy', 0.7710039999999767)]\n",
      "Got result for textvqa - 5200: [('textvqa/accuracy', 70.84600000000036), ('textvqa/mllm_eval_accuracy', 75.00400000000042)]\n",
      "Got result for infographics_w_ocr - 5200: [('infographics_w_ocr/anls_total_score', 0.6475439864773423), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5927731502118204), ('infographics_w_ocr/answer_type_multi_span_score', 0.5422372455955993), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6470822928327451), ('infographics_w_ocr/answer_type_question_span_score', 0.7087014983508196), ('infographics_w_ocr/answer_type_single_span_score', 0.6564080357300963), ('infographics_w_ocr/evidence_type_figure_score', 0.628364478210402), ('infographics_w_ocr/evidence_type_map_score', 0.5967465241999312), ('infographics_w_ocr/evidence_type_table_list_score', 0.6474746961288477), ('infographics_w_ocr/evidence_type_text_score', 0.6994784184158072), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5342495047035597), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6462437486410086), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5355541084013923), ('infographics_w_ocr/reasoning_type_counting_score', 0.6337412587412588)]\n",
      "Got result for infographics - 5200: [('infographics/anls_total_score', 0.5606378614327061), ('infographics/mllm_evaluation_anls_score', 0.4968640616673429), ('infographics/answer_type_multi_span_score', 0.3929423038599037), ('infographics/answer_type_non_extractive_score', 0.5544065337917057), ('infographics/answer_type_question_span_score', 0.7505077721423875), ('infographics/answer_type_single_span_score', 0.5705869537243295), ('infographics/evidence_type_figure_score', 0.5523808230479162), ('infographics/evidence_type_map_score', 0.5622163299921324), ('infographics/evidence_type_table_list_score', 0.5308380193891934), ('infographics/evidence_type_text_score', 0.6138701053166072), ('infographics/evidence_type_visual_layout_score', 0.5186672450009276), ('infographics/reasoning_type_arithmetic_score', 0.5000119173064377), ('infographics/reasoning_type_comparison_score', 0.48668763927551706), ('infographics/reasoning_type_counting_score', 0.5941142191142191)]\n",
      "Got result for mmbench - 5200: [('mmbench/attribute_comparison', 0.6590909090909091), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.7520383222700083)]\n",
      "5200\n",
      "Got result for mmmu - 5400: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.6), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.7333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.6333333333333333), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.26666666666666666), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.8), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.5722222222222222), ('mmmu/mllm_eval_accuracy', 0.5833333333333334)]\n",
      "Got result for docvqa - 5400: [('docvqa/anls_total_score', 0.687448292571438), ('docvqa/mllm_evaluation_anls_score', 0.6881498978705112), ('docvqa/mmllm_fixed_anls_score', 0.7285652619760637)]\n",
      "Got result for mathvista - 5400: [('mathvista/accuracy', 0.488)]\n",
      "Got result for ai2d - 5400: [('ai2d/accuracy', 0.8212435233160622)]\n",
      "Got result for chartqa - 5400: [('chartqa/accuracy', 0.5584110793282683)]\n",
      "Got result for vqa - 5400: [('vqa/accuracy', 0.7543399999999748), ('vqa/recall', 0.7746999999999739), ('vqa/bleu', 0.047432851046323776), ('vqa/mllm_evaluation_accuracy', 0.7722239999999752)]\n",
      "Got result for textvqa - 5400: [('textvqa/accuracy', 70.62000000000035), ('textvqa/mllm_eval_accuracy', 74.54200000000039)]\n",
      "Got result for infographics_w_ocr - 5400: [('infographics_w_ocr/anls_total_score', 0.6472864952659212), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5955129636089116), ('infographics_w_ocr/answer_type_multi_span_score', 0.5421720980264859), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6391163638451164), ('infographics_w_ocr/answer_type_question_span_score', 0.7002308023801237), ('infographics_w_ocr/answer_type_single_span_score', 0.6581589415759026), ('infographics_w_ocr/evidence_type_figure_score', 0.6268942320573199), ('infographics_w_ocr/evidence_type_map_score', 0.5902730932433902), ('infographics_w_ocr/evidence_type_table_list_score', 0.6528123776664972), ('infographics_w_ocr/evidence_type_text_score', 0.6917829780716183), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.566075231850327), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6292169493368122), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5602293854841695), ('infographics_w_ocr/reasoning_type_counting_score', 0.6255827505827506)]\n",
      "Got result for infographics - 5400: [('infographics/anls_total_score', 0.5521929771018448), ('infographics/mllm_evaluation_anls_score', 0.4865074175233719), ('infographics/answer_type_multi_span_score', 0.37555813076421324), ('infographics/answer_type_non_extractive_score', 0.533666514317509), ('infographics/answer_type_question_span_score', 0.7287939090823706), ('infographics/answer_type_single_span_score', 0.5677243479820872), ('infographics/evidence_type_figure_score', 0.5428025763669297), ('infographics/evidence_type_map_score', 0.5398965306110889), ('infographics/evidence_type_table_list_score', 0.5254291160200518), ('infographics/evidence_type_text_score', 0.5953795242893389), ('infographics/evidence_type_visual_layout_score', 0.5056541349929172), ('infographics/reasoning_type_arithmetic_score', 0.4812245973204877), ('infographics/reasoning_type_comparison_score', 0.4712005923993071), ('infographics/reasoning_type_counting_score', 0.5725524475524475)]\n",
      "Got result for mmbench - 5400: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8354430379746836), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8490566037735849), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.7443184534504432)]\n",
      "5400\n",
      "Got result for mmmu - 5600: [('mmmu/accounting', 0.43333333333333335), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6666666666666666), ('mmmu/design', 0.8333333333333334), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.3333333333333333), ('mmmu/energy_and_power', 0.5666666666666667), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.6666666666666666), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.5666666666666667), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7333333333333333), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.5711111111111111), ('mmmu/mllm_eval_accuracy', 0.5722222222222222)]\n",
      "Got result for docvqa - 5600: [('docvqa/anls_total_score', 0.6937561097791135), ('docvqa/mllm_evaluation_anls_score', 0.692350393421497), ('docvqa/mmllm_fixed_anls_score', 0.7329261273969528)]\n",
      "Got result for mathvista - 5600: [('mathvista/accuracy', 0.473)]\n",
      "Got result for ai2d - 5600: [('ai2d/accuracy', 0.8270725388601037)]\n",
      "Got result for chartqa - 5600: [('chartqa/accuracy', 0.565104688277239)]\n",
      "Got result for vqa - 5600: [('vqa/accuracy', 0.7509959999999751), ('vqa/recall', 0.7722799999999749), ('vqa/bleu', 0.03847870975732803), ('vqa/mllm_evaluation_accuracy', 0.7693679999999757)]\n",
      "Got result for textvqa - 5600: [('textvqa/accuracy', 70.89200000000032), ('textvqa/mllm_eval_accuracy', 75.00000000000041)]\n",
      "Got result for infographics_w_ocr - 5600: [('infographics_w_ocr/anls_total_score', 0.6504432852929701), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5986877580494319), ('infographics_w_ocr/answer_type_multi_span_score', 0.5226254167725024), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6494015327650049), ('infographics_w_ocr/answer_type_question_span_score', 0.706694477593799), ('infographics_w_ocr/answer_type_single_span_score', 0.6617176982364497), ('infographics_w_ocr/evidence_type_figure_score', 0.6283656640867235), ('infographics_w_ocr/evidence_type_map_score', 0.5971153846153845), ('infographics_w_ocr/evidence_type_table_list_score', 0.6526706792020109), ('infographics_w_ocr/evidence_type_text_score', 0.6968238407419125), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5616776577388036), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6501793868232222), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5617729173868063), ('infographics_w_ocr/reasoning_type_counting_score', 0.6349067599067599)]\n",
      "Got result for infographics - 5600: [('infographics/anls_total_score', 0.5621139845074372), ('infographics/mllm_evaluation_anls_score', 0.49736779255816754), ('infographics/answer_type_multi_span_score', 0.3997504066446949), ('infographics/answer_type_non_extractive_score', 0.5576509084646518), ('infographics/answer_type_question_span_score', 0.6860999577345732), ('infographics/answer_type_single_span_score', 0.5767602392319365), ('infographics/evidence_type_figure_score', 0.5524130321440853), ('infographics/evidence_type_map_score', 0.5135105980362712), ('infographics/evidence_type_table_list_score', 0.5237420027021616), ('infographics/evidence_type_text_score', 0.6145676122289675), ('infographics/evidence_type_visual_layout_score', 0.5268463104793364), ('infographics/reasoning_type_arithmetic_score', 0.5121493803000652), ('infographics/reasoning_type_comparison_score', 0.4885599674980059), ('infographics/reasoning_type_counting_score', 0.5946969696969695)]\n",
      "Got result for mmbench - 5600: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.9014084507042254), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9423076923076923), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7708333333333334), ('mmbench/object_localization', 0.4567901234567901), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7397817944318754)]\n",
      "5600\n",
      "Got result for mmmu - 5800: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7666666666666667), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6666666666666666), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.6333333333333333), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.6333333333333333), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.5666666666666667), ('mmmu/music', 0.5), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.6), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5855555555555556), ('mmmu/mllm_eval_accuracy', 0.6)]\n",
      "Got result for docvqa - 5800: [('docvqa/anls_total_score', 0.6986951070126822), ('docvqa/mllm_evaluation_anls_score', 0.697917887543483), ('docvqa/mmllm_fixed_anls_score', 0.73771105679636)]\n",
      "Got result for mathvista - 5800: [('mathvista/accuracy', 0.512)]\n",
      "Got result for ai2d - 5800: [('ai2d/accuracy', 0.8228626943005182)]\n",
      "Got result for chartqa - 5800: [('chartqa/accuracy', 0.5679283561234222)]\n",
      "Got result for vqa - 5800: [('vqa/accuracy', 0.7549239999999755), ('vqa/recall', 0.7742439999999745), ('vqa/bleu', 0.040698450058698654), ('vqa/mllm_evaluation_accuracy', 0.7715679999999758)]\n",
      "Got result for textvqa - 5800: [('textvqa/accuracy', 70.93800000000033), ('textvqa/mllm_eval_accuracy', 75.0120000000004)]\n",
      "Got result for infographics_w_ocr - 5800: [('infographics_w_ocr/anls_total_score', 0.6605073316978199), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6095811696260273), ('infographics_w_ocr/answer_type_multi_span_score', 0.5385818703140748), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6542684259682455), ('infographics_w_ocr/answer_type_question_span_score', 0.720262853662175), ('infographics_w_ocr/answer_type_single_span_score', 0.6719703714210838), ('infographics_w_ocr/evidence_type_figure_score', 0.6414240507405774), ('infographics_w_ocr/evidence_type_map_score', 0.5820988829652195), ('infographics_w_ocr/evidence_type_table_list_score', 0.6650707414586297), ('infographics_w_ocr/evidence_type_text_score', 0.6994024616298009), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.6083540077046272), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6487229665311853), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5550241001888098), ('infographics_w_ocr/reasoning_type_counting_score', 0.6494755244755245)]\n",
      "Got result for infographics - 5800: [('infographics/anls_total_score', 0.5581030726189555), ('infographics/mllm_evaluation_anls_score', 0.49670041679663884), ('infographics/answer_type_multi_span_score', 0.40119832190998156), ('infographics/answer_type_non_extractive_score', 0.5480343646708928), ('infographics/answer_type_question_span_score', 0.7089819902319903), ('infographics/answer_type_single_span_score', 0.5702817163002452), ('infographics/evidence_type_figure_score', 0.5434458650127849), ('infographics/evidence_type_map_score', 0.5518420508059719), ('infographics/evidence_type_table_list_score', 0.5199722818338348), ('infographics/evidence_type_text_score', 0.603789978644245), ('infographics/evidence_type_visual_layout_score', 0.5217792950441337), ('infographics/reasoning_type_arithmetic_score', 0.5062089166541219), ('infographics/reasoning_type_comparison_score', 0.4678986455944092), ('infographics/reasoning_type_counting_score', 0.5859557109557109)]\n",
      "Got result for mmbench - 5800: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6103896103896104), ('mmbench/overall', 0.7516422962545742)]\n",
      "5800\n",
      "Got result for docvqa - 6000: [('docvqa/anls_total_score', 0.6907244704126669), ('docvqa/mllm_evaluation_anls_score', 0.6901799468799896), ('docvqa/mmllm_fixed_anls_score', 0.7285612210971545)]\n",
      "Got result for mathvista - 6000: [('mathvista/accuracy', 0.489)]\n",
      "Got result for ai2d - 6000: [('ai2d/accuracy', 0.8303108808290155)]\n",
      "Got result for chartqa - 6000: [('chartqa/accuracy', 0.5716509407743837)]\n",
      "Got result for vqa - 6000: [('vqa/accuracy', 0.753231999999975), ('vqa/recall', 0.7747079999999741), ('vqa/bleu', 0.02703072316944599), ('vqa/mllm_evaluation_accuracy', 0.7717559999999755)]\n",
      "Got result for textvqa - 6000: [('textvqa/accuracy', 70.42400000000032), ('textvqa/mllm_eval_accuracy', 74.50800000000038)]\n",
      "Got result for infographics_w_ocr - 6000: [('infographics_w_ocr/anls_total_score', 0.6563114925477969), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6060670560012074), ('infographics_w_ocr/answer_type_multi_span_score', 0.5334219557626202), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6563420304830796), ('infographics_w_ocr/answer_type_question_span_score', 0.7277414861408074), ('infographics_w_ocr/answer_type_single_span_score', 0.6650461788528826), ('infographics_w_ocr/evidence_type_figure_score', 0.6368393499361902), ('infographics_w_ocr/evidence_type_map_score', 0.6037160446813912), ('infographics_w_ocr/evidence_type_table_list_score', 0.6565375500825521), ('infographics_w_ocr/evidence_type_text_score', 0.7066525848975213), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5949208546064717), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6549902152641877), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5528012404232342), ('infographics_w_ocr/reasoning_type_counting_score', 0.6423659673659674)]\n",
      "Got result for infographics - 6000: [('infographics/anls_total_score', 0.5620549507991315), ('infographics/mllm_evaluation_anls_score', 0.500426481705793), ('infographics/answer_type_multi_span_score', 0.40263019491645385), ('infographics/answer_type_non_extractive_score', 0.5573500740770182), ('infographics/answer_type_question_span_score', 0.7237220578566732), ('infographics/answer_type_single_span_score', 0.5727862897839269), ('infographics/evidence_type_figure_score', 0.5560101205611226), ('infographics/evidence_type_map_score', 0.561669114339393), ('infographics/evidence_type_table_list_score', 0.5254556034625901), ('infographics/evidence_type_text_score', 0.6136981950011163), ('infographics/evidence_type_visual_layout_score', 0.5288655196549934), ('infographics/reasoning_type_arithmetic_score', 0.5157653419639721), ('infographics/reasoning_type_comparison_score', 0.46664968926212813), ('infographics/reasoning_type_counting_score', 0.5824592074592074)]\n",
      "Got result for mmbench - 6000: [('mmbench/attribute_comparison', 0.6818181818181818), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6103896103896104), ('mmbench/overall', 0.7477992440097709)]\n",
      "6000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.5578</td>\n",
       "      <td>0.6852</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.8174</td>\n",
       "      <td>0.5189</td>\n",
       "      <td>0.7468</td>\n",
       "      <td>0.69688</td>\n",
       "      <td>0.6447</td>\n",
       "      <td>0.5587</td>\n",
       "      <td>0.743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.5767</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.5345</td>\n",
       "      <td>0.7513</td>\n",
       "      <td>0.7067</td>\n",
       "      <td>0.6489</td>\n",
       "      <td>0.5459</td>\n",
       "      <td>0.7529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.6835</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.8235</td>\n",
       "      <td>0.5301</td>\n",
       "      <td>0.7491</td>\n",
       "      <td>0.70012</td>\n",
       "      <td>0.6515</td>\n",
       "      <td>0.5574</td>\n",
       "      <td>0.743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.5756</td>\n",
       "      <td>0.5622</td>\n",
       "      <td>0.6862</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.8258</td>\n",
       "      <td>0.5386</td>\n",
       "      <td>0.7527</td>\n",
       "      <td>0.7053</td>\n",
       "      <td>0.6473</td>\n",
       "      <td>0.5585</td>\n",
       "      <td>0.7507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.5511</td>\n",
       "      <td>0.6915</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.5447</td>\n",
       "      <td>0.7517</td>\n",
       "      <td>0.70046</td>\n",
       "      <td>0.6548</td>\n",
       "      <td>0.5557</td>\n",
       "      <td>0.7557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>0.5833</td>\n",
       "      <td>0.5778</td>\n",
       "      <td>0.6882</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.8251</td>\n",
       "      <td>0.5338</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.69956</td>\n",
       "      <td>0.6509</td>\n",
       "      <td>0.5539</td>\n",
       "      <td>0.7495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>0.5967</td>\n",
       "      <td>0.5944</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.7514</td>\n",
       "      <td>0.7001</td>\n",
       "      <td>0.6499</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.7511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.5833</td>\n",
       "      <td>0.6895</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.5229</td>\n",
       "      <td>0.7529</td>\n",
       "      <td>0.69826</td>\n",
       "      <td>0.6523</td>\n",
       "      <td>0.5537</td>\n",
       "      <td>0.7524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>0.5822</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.6844</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.8174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7503</td>\n",
       "      <td>0.70316</td>\n",
       "      <td>0.6526</td>\n",
       "      <td>0.5586</td>\n",
       "      <td>0.7516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.5811</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.5428</td>\n",
       "      <td>0.7523</td>\n",
       "      <td>0.70334</td>\n",
       "      <td>0.6562</td>\n",
       "      <td>0.5637</td>\n",
       "      <td>0.7527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>0.5967</td>\n",
       "      <td>0.5956</td>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.8196</td>\n",
       "      <td>0.5444</td>\n",
       "      <td>0.7515</td>\n",
       "      <td>0.69972</td>\n",
       "      <td>0.6551</td>\n",
       "      <td>0.5597</td>\n",
       "      <td>0.7424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>0.5867</td>\n",
       "      <td>0.5644</td>\n",
       "      <td>0.6956</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.8251</td>\n",
       "      <td>0.5428</td>\n",
       "      <td>0.7522</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.6607</td>\n",
       "      <td>0.5602</td>\n",
       "      <td>0.7429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>0.5789</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.6955</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.7018</td>\n",
       "      <td>0.6512</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.7475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>0.5944</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.6977</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.5282</td>\n",
       "      <td>0.7513</td>\n",
       "      <td>0.70668</td>\n",
       "      <td>0.6532</td>\n",
       "      <td>0.5506</td>\n",
       "      <td>0.7572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.5711</td>\n",
       "      <td>0.6855</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.8245</td>\n",
       "      <td>0.5384</td>\n",
       "      <td>0.7552</td>\n",
       "      <td>0.70896</td>\n",
       "      <td>0.6471</td>\n",
       "      <td>0.5548</td>\n",
       "      <td>0.7601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>0.5678</td>\n",
       "      <td>0.5567</td>\n",
       "      <td>0.6884</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.8245</td>\n",
       "      <td>0.5425</td>\n",
       "      <td>0.7527</td>\n",
       "      <td>0.70494</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.7446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.6921</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.8193</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.70156</td>\n",
       "      <td>0.6523</td>\n",
       "      <td>0.5579</td>\n",
       "      <td>0.7601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>0.5744</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.5596</td>\n",
       "      <td>0.7529</td>\n",
       "      <td>0.69808</td>\n",
       "      <td>0.6531</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.7522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.5633</td>\n",
       "      <td>0.6921</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.8293</td>\n",
       "      <td>0.5442</td>\n",
       "      <td>0.7538</td>\n",
       "      <td>0.70784</td>\n",
       "      <td>0.6554</td>\n",
       "      <td>0.5634</td>\n",
       "      <td>0.7547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.5944</td>\n",
       "      <td>0.5911</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.8238</td>\n",
       "      <td>0.5506</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.70566</td>\n",
       "      <td>0.6531</td>\n",
       "      <td>0.5615</td>\n",
       "      <td>0.7592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5622</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.8255</td>\n",
       "      <td>0.5546</td>\n",
       "      <td>0.7542</td>\n",
       "      <td>0.70552</td>\n",
       "      <td>0.6555</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>0.7598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>0.5767</td>\n",
       "      <td>0.5622</td>\n",
       "      <td>0.6871</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.8255</td>\n",
       "      <td>0.5627</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.70292</td>\n",
       "      <td>0.6583</td>\n",
       "      <td>0.5655</td>\n",
       "      <td>0.7551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.5956</td>\n",
       "      <td>0.5833</td>\n",
       "      <td>0.6893</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.8264</td>\n",
       "      <td>0.5542</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.70896</td>\n",
       "      <td>0.6499</td>\n",
       "      <td>0.5539</td>\n",
       "      <td>0.7635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>0.5678</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.6924</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.8271</td>\n",
       "      <td>0.5637</td>\n",
       "      <td>0.7547</td>\n",
       "      <td>0.70632</td>\n",
       "      <td>0.6501</td>\n",
       "      <td>0.5558</td>\n",
       "      <td>0.7516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.5867</td>\n",
       "      <td>0.5756</td>\n",
       "      <td>0.6934</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.8219</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.7518</td>\n",
       "      <td>0.70416</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.5615</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>0.5756</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.8164</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.7539</td>\n",
       "      <td>0.70846</td>\n",
       "      <td>0.6475</td>\n",
       "      <td>0.5606</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5400</th>\n",
       "      <td>0.5833</td>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.6874</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.8212</td>\n",
       "      <td>0.5584</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.7062</td>\n",
       "      <td>0.6473</td>\n",
       "      <td>0.5522</td>\n",
       "      <td>0.7443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5600</th>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5711</td>\n",
       "      <td>0.6938</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.8271</td>\n",
       "      <td>0.5651</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.70892</td>\n",
       "      <td>0.6504</td>\n",
       "      <td>0.5621</td>\n",
       "      <td>0.7398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5800</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5856</td>\n",
       "      <td>0.6987</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.5679</td>\n",
       "      <td>0.7549</td>\n",
       "      <td>0.70938</td>\n",
       "      <td>0.6605</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.7516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6907</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.8303</td>\n",
       "      <td>0.5717</td>\n",
       "      <td>0.7532</td>\n",
       "      <td>0.70424</td>\n",
       "      <td>0.6563</td>\n",
       "      <td>0.5621</td>\n",
       "      <td>0.7478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1  docvqa mathvista    ai2d chartqa     vqa  textvqa  \\\n",
       "200   0.5689  0.5578  0.6852     0.411  0.8174  0.5189  0.7468  0.69688   \n",
       "400   0.5767  0.5544  0.6946      0.42   0.818  0.5345  0.7513   0.7067   \n",
       "600     0.58  0.5689  0.6835     0.432  0.8235  0.5301  0.7491  0.70012   \n",
       "800   0.5756  0.5622  0.6862     0.435  0.8258  0.5386  0.7527   0.7053   \n",
       "1000  0.5667  0.5511  0.6915     0.441  0.8229  0.5447  0.7517  0.70046   \n",
       "1200  0.5833  0.5778  0.6882     0.443  0.8251  0.5338   0.753  0.69956   \n",
       "1400  0.5967  0.5944   0.693     0.449   0.819   0.536  0.7514   0.7001   \n",
       "1600    0.59  0.5833  0.6895     0.462  0.8167  0.5229  0.7529  0.69826   \n",
       "1800  0.5822    0.57  0.6844     0.479  0.8174     NaN  0.7503  0.70316   \n",
       "2000  0.5811  0.5656    0.69     0.465   0.819  0.5428  0.7523  0.70334   \n",
       "2200  0.5967  0.5956  0.6923     0.479  0.8196  0.5444  0.7515  0.69972   \n",
       "2400  0.5867  0.5644  0.6956     0.467  0.8251  0.5428  0.7522     0.71   \n",
       "2600  0.5789    0.57  0.6955     0.473   0.831  0.5409  0.7553   0.7018   \n",
       "2800  0.5944  0.5844  0.6977     0.457  0.8229  0.5282  0.7513  0.70668   \n",
       "3000    0.58  0.5711  0.6855     0.482  0.8245  0.5384  0.7552  0.70896   \n",
       "3200  0.5678  0.5567  0.6884     0.466  0.8245  0.5425  0.7527  0.70494   \n",
       "3400  0.5689  0.5544  0.6921     0.485  0.8193   0.532   0.752  0.70156   \n",
       "3600  0.5744  0.5689   0.699     0.487   0.828  0.5596  0.7529  0.69808   \n",
       "3800  0.5667  0.5633  0.6921     0.489  0.8293  0.5442  0.7538  0.70784   \n",
       "4000  0.5944  0.5911  0.6861     0.488  0.8238  0.5506   0.752  0.70566   \n",
       "4200  0.5722  0.5622   0.691     0.472  0.8255  0.5546  0.7542  0.70552   \n",
       "4400  0.5767  0.5622  0.6871     0.478  0.8255  0.5627   0.751  0.70292   \n",
       "4600  0.5956  0.5833  0.6893     0.494  0.8264  0.5542  0.7543  0.70896   \n",
       "4800  0.5678    0.56  0.6924     0.478  0.8271  0.5637  0.7547  0.70632   \n",
       "5000  0.5867  0.5756  0.6934     0.489  0.8219   0.547  0.7518  0.70416   \n",
       "5200  0.5756  0.5689    0.69     0.487  0.8164    0.57  0.7539  0.70846   \n",
       "5400  0.5833  0.5722  0.6874     0.488  0.8212  0.5584  0.7543   0.7062   \n",
       "5600  0.5722  0.5711  0.6938     0.473  0.8271  0.5651   0.751  0.70892   \n",
       "5800     0.6  0.5856  0.6987     0.512  0.8229  0.5679  0.7549  0.70938   \n",
       "6000     NaN     NaN  0.6907     0.489  0.8303  0.5717  0.7532  0.70424   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "200              0.6447       0.5587   0.743  \n",
       "400              0.6489       0.5459  0.7529  \n",
       "600              0.6515       0.5574   0.743  \n",
       "800              0.6473       0.5585  0.7507  \n",
       "1000             0.6548       0.5557  0.7557  \n",
       "1200             0.6509       0.5539  0.7495  \n",
       "1400             0.6499       0.5689  0.7511  \n",
       "1600             0.6523       0.5537  0.7524  \n",
       "1800             0.6526       0.5586  0.7516  \n",
       "2000             0.6562       0.5637  0.7527  \n",
       "2200             0.6551       0.5597  0.7424  \n",
       "2400             0.6607       0.5602  0.7429  \n",
       "2600             0.6512        0.562  0.7475  \n",
       "2800             0.6532       0.5506  0.7572  \n",
       "3000             0.6471       0.5548  0.7601  \n",
       "3200             0.6529        0.558  0.7446  \n",
       "3400             0.6523       0.5579  0.7601  \n",
       "3600             0.6531        0.558  0.7522  \n",
       "3800             0.6554       0.5634  0.7547  \n",
       "4000             0.6531       0.5615  0.7592  \n",
       "4200             0.6555       0.5543  0.7598  \n",
       "4400             0.6583       0.5655  0.7551  \n",
       "4600             0.6499       0.5539  0.7635  \n",
       "4800             0.6501       0.5558  0.7516  \n",
       "5000              0.652       0.5615   0.752  \n",
       "5200             0.6475       0.5606   0.752  \n",
       "5400             0.6473       0.5522  0.7443  \n",
       "5600             0.6504       0.5621  0.7398  \n",
       "5800             0.6605       0.5581  0.7516  \n",
       "6000             0.6563       0.5621  0.7478  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores_all(\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp28\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 504px exp 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New checkpoints: [4200, 4400, 4600]\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_504px_64nodes_exp28/checkpoint-4200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_504px_64nodes_exp28/evals/eval_jobs_checkpoint-4200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_504px_64nodes_exp28/checkpoint-4400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_504px_64nodes_exp28/evals/eval_jobs_checkpoint-4400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_504px_64nodes_exp28/checkpoint-4600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_504px_64nodes_exp28/evals/eval_jobs_checkpoint-4600.json\n"
     ]
    }
   ],
   "source": [
    "eval_helper.run_eval_sweep(\n",
    "    output_dir=f\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_504px_64nodes_exp28\",\n",
    "    eval_sbatch=EVAL_SBATCH,\n",
    "    eval_config_dir=EVAL_CONFIG_DIR,\n",
    "    aligner_parent_dir=ALIGNER_CODE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3554613515.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[48], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    benchmarks=[\"mmmu\"]\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# output_dir = \"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_504px_64nodes_exp29\"\n",
    "# c = 5200\n",
    "# eval_helper.run_eval_plan(\n",
    "#     eval_base_sbatch=EVAL_SBATCH,\n",
    "#     aligner_parent_dir=ALIGNER_CODE_DIR,\n",
    "#     eval_config_dir=EVAL_CONFIG_DIR,\n",
    "#     checkpoint_dir=output_dir,\n",
    "#     checkpoints=[c],\n",
    "#     save_eval_jobs=eval_helper.get_eval_jobs_record(output_dir, c),\n",
    "#     benchmarks=[\"mmmu\"],\n",
    "#     rerun_if_exists=True,\n",
    "#     print_cmd=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3400, 3600, 3800, 4000, 4200, 4400, 4600]\n",
      "Got result for mmmu - 3400: [('mmmu/accounting', 0.4), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.9), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.8), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.5), ('mmmu/geography', 0.5), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.9), ('mmmu/manage', 0.43333333333333335), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.4), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.6), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.5333333333333333), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5488888888888889), ('mmmu/mllm_eval_accuracy', 0.5555555555555556)]\n",
      "Got result for docvqa - 3400: [('docvqa/anls_total_score', 0.6663898306386112), ('docvqa/mllm_evaluation_anls_score', 0.6685029837123192), ('docvqa/mmllm_fixed_anls_score', 0.7055768637478149)]\n",
      "Got result for mathvista - 3400: [('mathvista/accuracy', 0.397)]\n",
      "Got result for ai2d - 3400: [('ai2d/accuracy', 0.7982512953367875)]\n",
      "Got result for chartqa - 3400: [('chartqa/accuracy', 0.5204247515933151)]\n",
      "Got result for vqa - 3400: [('vqa/accuracy', 0.7225319999999766), ('vqa/recall', 0.7541319999999748), ('vqa/bleu', 0.03303714096546173), ('vqa/mllm_evaluation_accuracy', 0.7482519999999762)]\n",
      "Got result for textvqa - 3400: [('textvqa/accuracy', 68.05400000000031), ('textvqa/mllm_eval_accuracy', 72.79800000000039)]\n",
      "Got result for infographics_w_ocr - 3400: [('infographics_w_ocr/anls_total_score', 0.6387138981123882), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5870502132653176), ('infographics_w_ocr/answer_type_multi_span_score', 0.5116450513295461), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6393398644754884), ('infographics_w_ocr/answer_type_question_span_score', 0.724416626134913), ('infographics_w_ocr/answer_type_single_span_score', 0.646800960639649), ('infographics_w_ocr/evidence_type_figure_score', 0.6181039978602075), ('infographics_w_ocr/evidence_type_map_score', 0.5842440974866717), ('infographics_w_ocr/evidence_type_table_list_score', 0.6301799854129261), ('infographics_w_ocr/evidence_type_text_score', 0.6868633422041727), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5304972022693323), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6367178483616838), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5273289617722103), ('infographics_w_ocr/reasoning_type_counting_score', 0.6322843822843822)]\n",
      "Got result for infographics - 3400: [('infographics/anls_total_score', 0.5530060416110254), ('infographics/mllm_evaluation_anls_score', 0.4932823360232426), ('infographics/answer_type_multi_span_score', 0.36365659349346613), ('infographics/answer_type_non_extractive_score', 0.5505444826421318), ('infographics/answer_type_question_span_score', 0.7079302641802642), ('infographics/answer_type_single_span_score', 0.5656969801671264), ('infographics/evidence_type_figure_score', 0.5412849165630316), ('infographics/evidence_type_map_score', 0.5530774962050442), ('infographics/evidence_type_table_list_score', 0.5216561112674218), ('infographics/evidence_type_text_score', 0.6027273197360761), ('infographics/evidence_type_visual_layout_score', 0.5077337583267902), ('infographics/reasoning_type_arithmetic_score', 0.5132457724923479), ('infographics/reasoning_type_comparison_score', 0.4544629449655898), ('infographics/reasoning_type_counting_score', 0.5766899766899767)]\n",
      "Got result for mmbench - 3400: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.575), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9433962264150944), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7445177237017512)]\n",
      "3400\n",
      "Got result for mmmu - 3600: [('mmmu/accounting', 0.43333333333333335), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.43333333333333335), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.9), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.4666666666666667), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.7), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.6333333333333333), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.43333333333333335), ('mmmu/materials', 0.3), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.5), ('mmmu/public_health', 0.5666666666666667), ('mmmu/sociology', 0.5666666666666667), ('mmmu/accuracy', 0.5566666666666666), ('mmmu/mllm_eval_accuracy', 0.5766666666666667)]\n",
      "Got result for docvqa - 3600: [('docvqa/anls_total_score', 0.6617993207761174), ('docvqa/mllm_evaluation_anls_score', 0.6624266313433262), ('docvqa/mmllm_fixed_anls_score', 0.7023888902933899)]\n",
      "Got result for mathvista - 3600: [('mathvista/accuracy', 0.384)]\n",
      "Got result for ai2d - 3600: [('ai2d/accuracy', 0.7985751295336787)]\n",
      "Got result for chartqa - 3600: [('chartqa/accuracy', 0.5120294379412977)]\n",
      "Got result for vqa - 3600: [('vqa/accuracy', 0.7218159999999774), ('vqa/recall', 0.7548119999999751), ('vqa/bleu', 0.029405279085040092), ('vqa/mllm_evaluation_accuracy', 0.7487599999999766)]\n",
      "Got result for textvqa - 3600: [('textvqa/accuracy', 68.34000000000032), ('textvqa/mllm_eval_accuracy', 73.03400000000036)]\n",
      "Got result for infographics_w_ocr - 3600: [('infographics_w_ocr/anls_total_score', 0.6412590710911179), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5893459131482057), ('infographics_w_ocr/answer_type_multi_span_score', 0.5293632113949839), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6284059972577153), ('infographics_w_ocr/answer_type_question_span_score', 0.6921768071303881), ('infographics_w_ocr/answer_type_single_span_score', 0.6556831581587793), ('infographics_w_ocr/evidence_type_figure_score', 0.6168330299925207), ('infographics_w_ocr/evidence_type_map_score', 0.5868843615130744), ('infographics_w_ocr/evidence_type_table_list_score', 0.6310051277938756), ('infographics_w_ocr/evidence_type_text_score', 0.6963844702511978), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5835293945690686), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6154972025691202), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5419280630513976), ('infographics_w_ocr/reasoning_type_counting_score', 0.6287878787878788)]\n",
      "Got result for infographics - 3600: [('infographics/anls_total_score', 0.5410206821752275), ('infographics/mllm_evaluation_anls_score', 0.4777151137850926), ('infographics/answer_type_multi_span_score', 0.3426045454936419), ('infographics/answer_type_non_extractive_score', 0.5369393716590823), ('infographics/answer_type_question_span_score', 0.7029511620857775), ('infographics/answer_type_single_span_score', 0.5544982236918544), ('infographics/evidence_type_figure_score', 0.5266426850919882), ('infographics/evidence_type_map_score', 0.5098136284216657), ('infographics/evidence_type_table_list_score', 0.5145417521483742), ('infographics/evidence_type_text_score', 0.5753382404809994), ('infographics/evidence_type_visual_layout_score', 0.47816868802081347), ('infographics/reasoning_type_arithmetic_score', 0.47908951778814796), ('infographics/reasoning_type_comparison_score', 0.46633227043286424), ('infographics/reasoning_type_counting_score', 0.5903263403263403)]\n",
      "Got result for mmbench - 3600: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6103896103896104), ('mmbench/overall', 0.7361604391098971)]\n",
      "3600\n",
      "Got result for mmmu - 3800: [('mmmu/accounting', 0.5333333333333333), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.5), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.3), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.3333333333333333), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.6), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.43333333333333335), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.36666666666666664), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5511111111111111), ('mmmu/mllm_eval_accuracy', 0.5611111111111111)]\n",
      "Got result for docvqa - 3800: [('docvqa/anls_total_score', 0.6628371326370398), ('docvqa/mllm_evaluation_anls_score', 0.6623573257320231), ('docvqa/mmllm_fixed_anls_score', 0.7017333430736306)]\n",
      "Got result for mathvista - 3800: [('mathvista/accuracy', 0.382)]\n",
      "Got result for ai2d - 3800: [('ai2d/accuracy', 0.7911269430051814)]\n",
      "Got result for chartqa - 3800: [('chartqa/accuracy', 0.5220774838119165)]\n",
      "Got result for vqa - 3800: [('vqa/accuracy', 0.7220399999999773), ('vqa/recall', 0.753739999999975), ('vqa/bleu', 0.0), ('vqa/mllm_evaluation_accuracy', 0.7477119999999761)]\n",
      "Got result for textvqa - 3800: [('textvqa/accuracy', 68.1520000000003), ('textvqa/mllm_eval_accuracy', 72.72800000000035)]\n",
      "Got result for infographics_w_ocr - 3800: [('infographics_w_ocr/anls_total_score', 0.6433735695828031), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5920224197668886), ('infographics_w_ocr/answer_type_multi_span_score', 0.518897347508328), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6294540601050548), ('infographics_w_ocr/answer_type_question_span_score', 0.7095790929784143), ('infographics_w_ocr/answer_type_single_span_score', 0.6572243990915589), ('infographics_w_ocr/evidence_type_figure_score', 0.6289686252813619), ('infographics_w_ocr/evidence_type_map_score', 0.5858117542523483), ('infographics_w_ocr/evidence_type_table_list_score', 0.640281569217612), ('infographics_w_ocr/evidence_type_text_score', 0.6834220374402479), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5674737469579114), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6143427919112848), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5340731309475063), ('infographics_w_ocr/reasoning_type_counting_score', 0.6356643356643357)]\n",
      "Got result for infographics - 3800: [('infographics/anls_total_score', 0.5536856979673426), ('infographics/mllm_evaluation_anls_score', 0.49337783772777905), ('infographics/answer_type_multi_span_score', 0.381549705280001), ('infographics/answer_type_non_extractive_score', 0.5359659930184343), ('infographics/answer_type_question_span_score', 0.7183663025009179), ('infographics/answer_type_single_span_score', 0.5682280034866272), ('infographics/evidence_type_figure_score', 0.547486290927372), ('infographics/evidence_type_map_score', 0.4785963890506698), ('infographics/evidence_type_table_list_score', 0.5319149339940145), ('infographics/evidence_type_text_score', 0.5889187413377728), ('infographics/evidence_type_visual_layout_score', 0.5001225638357868), ('infographics/reasoning_type_arithmetic_score', 0.4836387927141352), ('infographics/reasoning_type_comparison_score', 0.4845223472327601), ('infographics/reasoning_type_counting_score', 0.578088578088578)]\n",
      "Got result for mmbench - 3800: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9433962264150944), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7413262350841021)]\n",
      "3800\n",
      "Got result for mmmu - 4000: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.23333333333333334), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.9), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.7333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.7), ('mmmu/electronics', 0.26666666666666666), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.4), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.3), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5555555555555556), ('mmmu/mllm_eval_accuracy', 0.5677777777777778)]\n",
      "Got result for docvqa - 4000: [('docvqa/anls_total_score', 0.6646241250422438), ('docvqa/mllm_evaluation_anls_score', 0.6645039715661726), ('docvqa/mmllm_fixed_anls_score', 0.7037051967885504)]\n",
      "Got result for mathvista - 4000: [('mathvista/accuracy', 0.385)]\n",
      "Got result for ai2d - 4000: [('ai2d/accuracy', 0.783678756476684)]\n",
      "Got result for chartqa - 4000: [('chartqa/accuracy', 0.519711236403825)]\n",
      "Got result for vqa - 4000: [('vqa/accuracy', 0.7209679999999777), ('vqa/recall', 0.7536119999999755), ('vqa/bleu', 0.02487114444375038), ('vqa/mllm_evaluation_accuracy', 0.7471799999999765)]\n",
      "Got result for textvqa - 4000: [('textvqa/accuracy', 68.37800000000031), ('textvqa/mllm_eval_accuracy', 72.89600000000036)]\n",
      "Got result for infographics_w_ocr - 4000: [('infographics_w_ocr/anls_total_score', 0.637736839315623), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5862281844084967), ('infographics_w_ocr/answer_type_multi_span_score', 0.5137177170072248), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6179887924010891), ('infographics_w_ocr/answer_type_question_span_score', 0.6919508878502091), ('infographics_w_ocr/answer_type_single_span_score', 0.6542998586002592), ('infographics_w_ocr/evidence_type_figure_score', 0.6163127077548908), ('infographics_w_ocr/evidence_type_map_score', 0.5790121036990576), ('infographics_w_ocr/evidence_type_table_list_score', 0.6279686417908114), ('infographics_w_ocr/evidence_type_text_score', 0.6945708793672808), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.558869489836234), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6091819709970395), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5295613614332652), ('infographics_w_ocr/reasoning_type_counting_score', 0.6159673659673659)]\n",
      "Got result for infographics - 4000: [('infographics/anls_total_score', 0.5377626703787666), ('infographics/mllm_evaluation_anls_score', 0.47507000041295083), ('infographics/answer_type_multi_span_score', 0.38101432698141524), ('infographics/answer_type_non_extractive_score', 0.542098488210604), ('infographics/answer_type_question_span_score', 0.6795537261883415), ('infographics/answer_type_single_span_score', 0.5472729885670305), ('infographics/evidence_type_figure_score', 0.5279296694938453), ('infographics/evidence_type_map_score', 0.5309692125399152), ('infographics/evidence_type_table_list_score', 0.5034992990312798), ('infographics/evidence_type_text_score', 0.5719465793521353), ('infographics/evidence_type_visual_layout_score', 0.5189228626496973), ('infographics/reasoning_type_arithmetic_score', 0.4975358355495341), ('infographics/reasoning_type_comparison_score', 0.4625250959472744), ('infographics/reasoning_type_counting_score', 0.5755244755244755)]\n",
      "Got result for mmbench - 4000: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.4567901234567901), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.742263199766321)]\n",
      "4000\n",
      "Got result for docvqa - 4200: [('docvqa/anls_total_score', 0.6566293147440582), ('docvqa/mllm_evaluation_anls_score', 0.6564623074441205), ('docvqa/mmllm_fixed_anls_score', 0.6973264796455246)]\n",
      "Got result for mathvista - 4200: [('mathvista/accuracy', 0.385)]\n",
      "Got result for ai2d - 4200: [('ai2d/accuracy', 0.8001943005181347)]\n",
      "Got result for chartqa - 4200: [('chartqa/accuracy', 0.5191566553934844)]\n",
      "Got result for vqa - 4200: [('vqa/accuracy', 0.7235679999999765), ('vqa/recall', 0.7552439999999752), ('vqa/bleu', 0.02472078427672386), ('vqa/mllm_evaluation_accuracy', 0.7490039999999764)]\n",
      "Got result for textvqa - 4200: [('textvqa/accuracy', 68.38000000000031), ('textvqa/mllm_eval_accuracy', 72.76200000000036)]\n",
      "Got result for infographics_w_ocr - 4200: [('infographics_w_ocr/anls_total_score', 0.6502165064018606), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5964723702172133), ('infographics_w_ocr/answer_type_multi_span_score', 0.5404901457693948), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6409458233147204), ('infographics_w_ocr/answer_type_question_span_score', 0.7137228658721871), ('infographics_w_ocr/answer_type_single_span_score', 0.6607486892949596), ('infographics_w_ocr/evidence_type_figure_score', 0.6346908864641302), ('infographics_w_ocr/evidence_type_map_score', 0.6085015232292459), ('infographics_w_ocr/evidence_type_table_list_score', 0.646317573135674), ('infographics_w_ocr/evidence_type_text_score', 0.6821141793189567), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5482934614833084), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6311976265743389), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5334746139826667), ('infographics_w_ocr/reasoning_type_counting_score', 0.6350815850815852)]\n",
      "Got result for infographics - 4200: [('infographics/anls_total_score', 0.5478833012040266), ('infographics/mllm_evaluation_anls_score', 0.48495076907510665), ('infographics/answer_type_multi_span_score', 0.36574924752368754), ('infographics/answer_type_non_extractive_score', 0.5162269516699897), ('infographics/answer_type_question_span_score', 0.7021381396381396), ('infographics/answer_type_single_span_score', 0.5675016077802678), ('infographics/evidence_type_figure_score', 0.5311713651446696), ('infographics/evidence_type_map_score', 0.5070499486923482), ('infographics/evidence_type_table_list_score', 0.5250116394977251), ('infographics/evidence_type_text_score', 0.5875092643339324), ('infographics/evidence_type_visual_layout_score', 0.5047329748398229), ('infographics/reasoning_type_arithmetic_score', 0.47102798267181833), ('infographics/reasoning_type_comparison_score', 0.45979530520309736), ('infographics/reasoning_type_counting_score', 0.5568764568764568)]\n",
      "Got result for mmbench - 4200: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9433962264150944), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.8461538461538461), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7457640413308515)]\n",
      "4200\n",
      "Got result for docvqa - 4400: [('docvqa/anls_total_score', 0.6557553612258423), ('docvqa/mllm_evaluation_anls_score', 0.6560448667313834), ('docvqa/mmllm_fixed_anls_score', 0.6961318643248542)]\n",
      "Got result for mathvista - 4400: [('mathvista/accuracy', 0.389)]\n",
      "Got result for ai2d - 4400: [('ai2d/accuracy', 0.7956606217616581)]\n",
      "Got result for chartqa - 4400: [('chartqa/accuracy', 0.5024928124383479)]\n",
      "Got result for vqa - 4400: [('vqa/accuracy', 0.7237239999999775), ('vqa/recall', 0.7559999999999754), ('vqa/bleu', 0.03010716289281845), ('vqa/mllm_evaluation_accuracy', 0.7496199999999773)]\n",
      "Got result for textvqa - 4400: [('textvqa/accuracy', 68.66000000000031), ('textvqa/mllm_eval_accuracy', 73.28200000000035)]\n",
      "Got result for infographics_w_ocr - 4400: [('infographics_w_ocr/anls_total_score', 0.637071147741705), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5831627979475972), ('infographics_w_ocr/answer_type_multi_span_score', 0.516621153277926), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6307371049685699), ('infographics_w_ocr/answer_type_question_span_score', 0.7077879919226073), ('infographics_w_ocr/answer_type_single_span_score', 0.6474840900103593), ('infographics_w_ocr/evidence_type_figure_score', 0.6211113625022244), ('infographics_w_ocr/evidence_type_map_score', 0.5899961012584775), ('infographics_w_ocr/evidence_type_table_list_score', 0.6304996064285988), ('infographics_w_ocr/evidence_type_text_score', 0.6793895584055943), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.562685848915751), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6179142204827134), ('infographics_w_ocr/reasoning_type_comparison_score', 0.549357142493028), ('infographics_w_ocr/reasoning_type_counting_score', 0.6282051282051282)]\n",
      "Got result for infographics - 4400: [('infographics/anls_total_score', 0.5430703363954547), ('infographics/mllm_evaluation_anls_score', 0.4809208776826286), ('infographics/answer_type_multi_span_score', 0.3832199918528053), ('infographics/answer_type_non_extractive_score', 0.5386695281812823), ('infographics/answer_type_question_span_score', 0.6943278409624564), ('infographics/answer_type_single_span_score', 0.5548729857710976), ('infographics/evidence_type_figure_score', 0.5365896815750654), ('infographics/evidence_type_map_score', 0.508851529949375), ('infographics/evidence_type_table_list_score', 0.5120808078178647), ('infographics/evidence_type_text_score', 0.577537037270774), ('infographics/evidence_type_visual_layout_score', 0.49448265368615063), ('infographics/reasoning_type_arithmetic_score', 0.49121318179537354), ('infographics/reasoning_type_comparison_score', 0.4855574649672251), ('infographics/reasoning_type_counting_score', 0.5755244755244755)]\n",
      "Got result for mmbench - 4400: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8354430379746836), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.8461538461538461), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7443013700756678)]\n",
      "4400\n",
      "Got result for docvqa - 4600: [('docvqa/anls_total_score', 0.6638772323986837), ('docvqa/mllm_evaluation_anls_score', 0.6644685596065137), ('docvqa/mmllm_fixed_anls_score', 0.7023909266251664)]\n",
      "Got result for mathvista - 4600: [('mathvista/accuracy', 0.386)]\n",
      "Got result for ai2d - 4600: [('ai2d/accuracy', 0.8031088082901554)]\n",
      "Got result for chartqa - 4600: [('chartqa/accuracy', 0.5193256095531362)]\n",
      "Got result for vqa - 4600: [('vqa/accuracy', 0.7209759999999771), ('vqa/recall', 0.7531359999999752), ('vqa/bleu', 0.036795321851968765), ('vqa/mllm_evaluation_accuracy', 0.746587999999976)]\n",
      "Got result for textvqa - 4600: [('textvqa/accuracy', 68.55800000000029), ('textvqa/mllm_eval_accuracy', 72.96800000000036)]\n",
      "Got result for infographics_w_ocr - 4600: [('infographics_w_ocr/anls_total_score', 0.6365169327868592), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5815481401673815), ('infographics_w_ocr/answer_type_multi_span_score', 0.5390357461630688), ('infographics_w_ocr/answer_type_non_extractive_score', 0.628557408032996), ('infographics_w_ocr/answer_type_question_span_score', 0.678743549211836), ('infographics_w_ocr/answer_type_single_span_score', 0.6476007025482831), ('infographics_w_ocr/evidence_type_figure_score', 0.6193318361365969), ('infographics_w_ocr/evidence_type_map_score', 0.583302532741962), ('infographics_w_ocr/evidence_type_table_list_score', 0.6307249564424862), ('infographics_w_ocr/evidence_type_text_score', 0.6798821631693948), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5622693312207895), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6100191095054106), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5303669324465189), ('infographics_w_ocr/reasoning_type_counting_score', 0.632051282051282)]\n",
      "Got result for infographics - 4600: [('infographics/anls_total_score', 0.5439318066002102), ('infographics/mllm_evaluation_anls_score', 0.479511028201319), ('infographics/answer_type_multi_span_score', 0.37114427214990847), ('infographics/answer_type_non_extractive_score', 0.5443273122296632), ('infographics/answer_type_question_span_score', 0.6901878836974991), ('infographics/answer_type_single_span_score', 0.5558271716369639), ('infographics/evidence_type_figure_score', 0.532755354923567), ('infographics/evidence_type_map_score', 0.5221607454863133), ('infographics/evidence_type_table_list_score', 0.5164789945791248), ('infographics/evidence_type_text_score', 0.5772385813443988), ('infographics/evidence_type_visual_layout_score', 0.516413133427972), ('infographics/reasoning_type_arithmetic_score', 0.5000445330924781), ('infographics/reasoning_type_comparison_score', 0.46719372259048464), ('infographics/reasoning_type_counting_score', 0.5807692307692307)]\n",
      "Got result for mmbench - 4600: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8354430379746836), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9433962264150944), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4444444444444444), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.743989747194951)]\n",
      "4600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.5489</td>\n",
       "      <td>0.6664</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.7983</td>\n",
       "      <td>0.5204</td>\n",
       "      <td>0.7225</td>\n",
       "      <td>0.68054</td>\n",
       "      <td>0.6387</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.7445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>0.5767</td>\n",
       "      <td>0.5567</td>\n",
       "      <td>0.6618</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.7986</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.7218</td>\n",
       "      <td>0.6834</td>\n",
       "      <td>0.6413</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.7362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>0.5611</td>\n",
       "      <td>0.5511</td>\n",
       "      <td>0.6628</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.7911</td>\n",
       "      <td>0.5221</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.68152</td>\n",
       "      <td>0.6434</td>\n",
       "      <td>0.5537</td>\n",
       "      <td>0.7413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.5678</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.6646</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.5197</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.68378</td>\n",
       "      <td>0.6377</td>\n",
       "      <td>0.5378</td>\n",
       "      <td>0.7423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6566</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.8002</td>\n",
       "      <td>0.5192</td>\n",
       "      <td>0.7236</td>\n",
       "      <td>0.6838</td>\n",
       "      <td>0.6502</td>\n",
       "      <td>0.5479</td>\n",
       "      <td>0.7458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6558</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.7957</td>\n",
       "      <td>0.5025</td>\n",
       "      <td>0.7237</td>\n",
       "      <td>0.6866</td>\n",
       "      <td>0.6371</td>\n",
       "      <td>0.5431</td>\n",
       "      <td>0.7443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6639</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.8031</td>\n",
       "      <td>0.5193</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.68558</td>\n",
       "      <td>0.6365</td>\n",
       "      <td>0.5439</td>\n",
       "      <td>0.744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1  docvqa mathvista    ai2d chartqa     vqa  textvqa  \\\n",
       "3400  0.5556  0.5489  0.6664     0.397  0.7983  0.5204  0.7225  0.68054   \n",
       "3600  0.5767  0.5567  0.6618     0.384  0.7986   0.512  0.7218   0.6834   \n",
       "3800  0.5611  0.5511  0.6628     0.382  0.7911  0.5221   0.722  0.68152   \n",
       "4000  0.5678  0.5556  0.6646     0.385  0.7837  0.5197   0.721  0.68378   \n",
       "4200     NaN     NaN  0.6566     0.385  0.8002  0.5192  0.7236   0.6838   \n",
       "4400     NaN     NaN  0.6558     0.389  0.7957  0.5025  0.7237   0.6866   \n",
       "4600     NaN     NaN  0.6639     0.386  0.8031  0.5193   0.721  0.68558   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "3400             0.6387        0.553  0.7445  \n",
       "3600             0.6413        0.541  0.7362  \n",
       "3800             0.6434       0.5537  0.7413  \n",
       "4000             0.6377       0.5378  0.7423  \n",
       "4200             0.6502       0.5479  0.7458  \n",
       "4400             0.6371       0.5431  0.7443  \n",
       "4600             0.6365       0.5439   0.744  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores_all(\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_504px_64nodes_exp28\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 336px i18n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New checkpoints: []\n"
     ]
    }
   ],
   "source": [
    "eval_helper.run_eval_sweep(\n",
    "    output_dir=f\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_64nodes_i18n\",\n",
    "    eval_sbatch=EVAL_SBATCH,\n",
    "    eval_config_dir=EVAL_CONFIG_DIR,\n",
    "    aligner_parent_dir=ALIGNER_CODE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000, 5200, 5400, 5600, 5800, 6000, 6200, 6400]\n",
      "Got result for mmmu - 5000: [('mmmu/accounting', 0.6), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.6333333333333333), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.4666666666666667), ('mmmu/design', 0.6), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.7), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.6333333333333333), ('mmmu/finance', 0.4), ('mmmu/geography', 0.6), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.43333333333333335), ('mmmu/materials', 0.3), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.3333333333333333), ('mmmu/psychology', 0.5333333333333333), ('mmmu/public_health', 0.7), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.5433333333333333), ('mmmu/mllm_eval_accuracy', 0.5444444444444444)]\n",
      "Got result for docvqa - 5000: [('docvqa/anls_total_score', 0.670637585261586), ('docvqa/mllm_evaluation_anls_score', 0.6717115681299881), ('docvqa/mmllm_fixed_anls_score', 0.7098482827714544)]\n",
      "Got result for mathvista - 5000: [('mathvista/accuracy', 0.476)]\n",
      "Got result for ai2d - 5000: [('ai2d/accuracy', 0.8487694300518135)]\n",
      "Got result for chartqa - 5000: [('chartqa/accuracy', 0.7013713040155702)]\n",
      "Got result for vqa - 5000: [('vqa/accuracy', 0.7255639999999781), ('vqa/recall', 0.7703199999999752), ('vqa/bleu', 0.02905668318271637), ('vqa/mllm_evaluation_accuracy', 0.7620959999999765)]\n",
      "Got result for textvqa - 5000: [('textvqa/accuracy', 66.91600000000031), ('textvqa/mllm_eval_accuracy', 72.05800000000038)]\n",
      "Got result for infographics_w_ocr - 5000: [('infographics_w_ocr/anls_total_score', 0.6587297113673974), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6052329270539901), ('infographics_w_ocr/answer_type_multi_span_score', 0.49944804635198403), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6579324232127127), ('infographics_w_ocr/answer_type_question_span_score', 0.7174033765379919), ('infographics_w_ocr/answer_type_single_span_score', 0.6722565143940358), ('infographics_w_ocr/evidence_type_figure_score', 0.6386578491151), ('infographics_w_ocr/evidence_type_map_score', 0.6146071337903021), ('infographics_w_ocr/evidence_type_table_list_score', 0.652155825003489), ('infographics_w_ocr/evidence_type_text_score', 0.7148331041823205), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5843910088653583), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.662796678207637), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5495278857502663), ('infographics_w_ocr/reasoning_type_counting_score', 0.6432400932400932)]\n",
      "Got result for infographics - 5000: [('infographics/anls_total_score', 0.5542418821514459), ('infographics/mllm_evaluation_anls_score', 0.49538160071682635), ('infographics/answer_type_multi_span_score', 0.3964590614371482), ('infographics/answer_type_non_extractive_score', 0.5370127310904886), ('infographics/answer_type_question_span_score', 0.7127330468676623), ('infographics/answer_type_single_span_score', 0.5690404304472785), ('infographics/evidence_type_figure_score', 0.5369959821881908), ('infographics/evidence_type_map_score', 0.5463945974429375), ('infographics/evidence_type_table_list_score', 0.5292795570991102), ('infographics/evidence_type_text_score', 0.6047008255767977), ('infographics/evidence_type_visual_layout_score', 0.497281120300204), ('infographics/reasoning_type_arithmetic_score', 0.4779156631040195), ('infographics/reasoning_type_comparison_score', 0.4874533088981116), ('infographics/reasoning_type_counting_score', 0.5795454545454546)]\n",
      "Got result for mmbench - 5000: [('mmbench/attribute_comparison', 0.7272727272727273), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.55), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.8490566037735849), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.5802469135802469), ('mmbench/ocr', 0.8717948717948718), ('mmbench/physical_property_reasoning', 0.6933333333333334), ('mmbench/physical_relation', 0.75), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.7142857142857143), ('mmbench/overall', 0.7887324153485087)]\n",
      "5000\n",
      "Got result for mmmu - 5200: [('mmmu/accounting', 0.5666666666666667), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.5666666666666667), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.5333333333333333), ('mmmu/computer_science', 0.5), ('mmmu/design', 0.6333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.5333333333333333), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.3333333333333333), ('mmmu/geography', 0.5), ('mmmu/history', 0.8), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.43333333333333335), ('mmmu/materials', 0.26666666666666666), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.26666666666666666), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.5333333333333333), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5277777777777778), ('mmmu/mllm_eval_accuracy', 0.5422222222222223)]\n",
      "Got result for docvqa - 5200: [('docvqa/anls_total_score', 0.6631147285194107), ('docvqa/mllm_evaluation_anls_score', 0.6651859112559485), ('docvqa/mmllm_fixed_anls_score', 0.705311167713988)]\n",
      "Got result for mathvista - 5200: [('mathvista/accuracy', 0.524)]\n",
      "Got result for ai2d - 5200: [('ai2d/accuracy', 0.8724093264248705)]\n",
      "Got result for chartqa - 5200: [('chartqa/accuracy', 0.77878383530482)]\n",
      "Got result for vqa - 5200: [('vqa/accuracy', 0.7279519999999774), ('vqa/recall', 0.7583879999999757), ('vqa/bleu', 0.021016282960772514), ('vqa/mllm_evaluation_accuracy', 0.7521079999999764)]\n",
      "Got result for textvqa - 5200: [('textvqa/accuracy', 66.48400000000035), ('textvqa/mllm_eval_accuracy', 71.19600000000038)]\n",
      "Got result for infographics_w_ocr - 5200: [('infographics_w_ocr/anls_total_score', 0.6527497136634058), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6003825017246154), ('infographics_w_ocr/answer_type_multi_span_score', 0.49536616919547577), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6454729668563306), ('infographics_w_ocr/answer_type_question_span_score', 0.6954899267399267), ('infographics_w_ocr/answer_type_single_span_score', 0.6678365477310179), ('infographics_w_ocr/evidence_type_figure_score', 0.6343397723280322), ('infographics_w_ocr/evidence_type_map_score', 0.6203827113480578), ('infographics_w_ocr/evidence_type_table_list_score', 0.6489245524968859), ('infographics_w_ocr/evidence_type_text_score', 0.6988071501948709), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.574185477431139), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6383443516148992), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5522644390014025), ('infographics_w_ocr/reasoning_type_counting_score', 0.6384032634032634)]\n",
      "Got result for infographics - 5200: [('infographics/anls_total_score', 0.5202706772978193), ('infographics/mllm_evaluation_anls_score', 0.47142314915796146), ('infographics/answer_type_multi_span_score', 0.35172581764910577), ('infographics/answer_type_non_extractive_score', 0.4846594333936105), ('infographics/answer_type_question_span_score', 0.707409951159951), ('infographics/answer_type_single_span_score', 0.5384356957578428), ('infographics/evidence_type_figure_score', 0.5015040520669284), ('infographics/evidence_type_map_score', 0.5147984441301272), ('infographics/evidence_type_table_list_score', 0.49225625090675046), ('infographics/evidence_type_text_score', 0.5589801698423115), ('infographics/evidence_type_visual_layout_score', 0.46309517071891576), ('infographics/reasoning_type_arithmetic_score', 0.37500000000000006), ('infographics/reasoning_type_comparison_score', 0.46012078685604085), ('infographics/reasoning_type_counting_score', 0.5795454545454545)]\n",
      "Got result for mmbench - 5200: [('mmbench/attribute_comparison', 0.7272727272727273), ('mmbench/attribute_recognition', 0.9436619718309859), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.55), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7708333333333334), ('mmbench/object_localization', 0.5679012345679012), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.7466666666666667), ('mmbench/physical_relation', 0.7916666666666666), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.7272727272727273), ('mmbench/overall', 0.7946849384057302)]\n",
      "5200\n",
      "Got result for mmmu - 5400: [('mmmu/accounting', 0.43333333333333335), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.5333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5), ('mmmu/design', 0.7), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.3333333333333333), ('mmmu/energy_and_power', 0.7), ('mmmu/finance', 0.4), ('mmmu/geography', 0.6666666666666666), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.26666666666666666), ('mmmu/math', 0.36666666666666664), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.5666666666666667), ('mmmu/psychology', 0.5666666666666667), ('mmmu/public_health', 0.5333333333333333), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5355555555555556), ('mmmu/mllm_eval_accuracy', 0.5444444444444444)]\n",
      "Got result for docvqa - 5400: [('docvqa/anls_total_score', 0.641838160825989), ('docvqa/mllm_evaluation_anls_score', 0.6449672436995196), ('docvqa/mmllm_fixed_anls_score', 0.6834503558936926)]\n",
      "Got result for mathvista - 5400: [('mathvista/accuracy', 0.521)]\n",
      "Got result for ai2d - 5400: [('ai2d/accuracy', 0.881800518134715)]\n",
      "Got result for chartqa - 5400: [('chartqa/accuracy', 0.7923155418680999)]\n",
      "Got result for vqa - 5400: [('vqa/accuracy', 0.7171559999999774), ('vqa/recall', 0.7514839999999755), ('vqa/bleu', 0.017669549211859703), ('vqa/mllm_evaluation_accuracy', 0.7444439999999758)]\n",
      "Got result for textvqa - 5400: [('textvqa/accuracy', 65.89800000000032), ('textvqa/mllm_eval_accuracy', 71.17800000000035)]\n",
      "Got result for infographics_w_ocr - 5400: [('infographics_w_ocr/anls_total_score', 0.6534752684865679), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6021788516045447), ('infographics_w_ocr/answer_type_multi_span_score', 0.46453368142480655), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6523244838344298), ('infographics_w_ocr/answer_type_question_span_score', 0.7082458673804828), ('infographics_w_ocr/answer_type_single_span_score', 0.6700545234450036), ('infographics_w_ocr/evidence_type_figure_score', 0.6353385068897828), ('infographics_w_ocr/evidence_type_map_score', 0.6222150272302772), ('infographics_w_ocr/evidence_type_table_list_score', 0.6473518115950101), ('infographics_w_ocr/evidence_type_text_score', 0.7060117909118188), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5728771112381088), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.670669313563149), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5553838676095528), ('infographics_w_ocr/reasoning_type_counting_score', 0.6194638694638694)]\n",
      "Got result for infographics - 5400: [('infographics/anls_total_score', 0.5298240142229065), ('infographics/mllm_evaluation_anls_score', 0.4791340211128352), ('infographics/answer_type_multi_span_score', 0.3592175234621677), ('infographics/answer_type_non_extractive_score', 0.5309911306294671), ('infographics/answer_type_question_span_score', 0.7102029914529914), ('infographics/answer_type_single_span_score', 0.5380994474916069), ('infographics/evidence_type_figure_score', 0.5153769918507295), ('infographics/evidence_type_map_score', 0.4659031460421583), ('infographics/evidence_type_table_list_score', 0.5035818660867833), ('infographics/evidence_type_text_score', 0.5754177098005614), ('infographics/evidence_type_visual_layout_score', 0.4902879714863689), ('infographics/reasoning_type_arithmetic_score', 0.4564660795825179), ('infographics/reasoning_type_comparison_score', 0.46085607299315806), ('infographics/reasoning_type_counting_score', 0.587995337995338)]\n",
      "Got result for mmbench - 5400: [('mmbench/attribute_comparison', 0.7272727272727273), ('mmbench/attribute_recognition', 0.9436619718309859), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.55), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.75), ('mmbench/object_localization', 0.5185185185185185), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.7733333333333333), ('mmbench/physical_relation', 0.7916666666666666), ('mmbench/spatial_relationship', 0.5), ('mmbench/structuralized_imagetext_understanding', 0.6753246753246753), ('mmbench/overall', 0.7902476313269925)]\n",
      "5400\n",
      "Got result for mmmu - 5600: [('mmmu/accounting', 0.4), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.6), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5333333333333333), ('mmmu/design', 0.6333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.6666666666666666), ('mmmu/finance', 0.3333333333333333), ('mmmu/geography', 0.6), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.16666666666666666), ('mmmu/math', 0.4), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.3333333333333333), ('mmmu/psychology', 0.5666666666666667), ('mmmu/public_health', 0.5333333333333333), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5211111111111111), ('mmmu/mllm_eval_accuracy', 0.5277777777777778)]\n",
      "Got result for docvqa - 5600: [('docvqa/anls_total_score', 0.648769674145729), ('docvqa/mllm_evaluation_anls_score', 0.6516133037863439), ('docvqa/mmllm_fixed_anls_score', 0.6893111146627985)]\n",
      "Got result for ai2d - 5600: [('ai2d/accuracy', 0.8950777202072538)]\n",
      "Got result for chartqa - 5600: [('chartqa/accuracy', 0.7982149435649808)]\n",
      "Got result for vqa - 5600: [('vqa/accuracy', 0.712295999999978), ('vqa/recall', 0.7495919999999758), ('vqa/bleu', 0.016216762363910675), ('vqa/mllm_evaluation_accuracy', 0.7411279999999764)]\n",
      "Got result for textvqa - 5600: [('textvqa/accuracy', 65.26600000000028), ('textvqa/mllm_eval_accuracy', 70.79200000000029)]\n",
      "Got result for infographics - 5600: [('infographics/anls_total_score', 0.5400153120712504), ('infographics/mllm_evaluation_anls_score', 0.4823149412166404), ('infographics/answer_type_multi_span_score', 0.3665127042660745), ('infographics/answer_type_non_extractive_score', 0.5309609919917335), ('infographics/answer_type_question_span_score', 0.7219516060862213), ('infographics/answer_type_single_span_score', 0.5520740334422093), ('infographics/evidence_type_figure_score', 0.5254525829818045), ('infographics/evidence_type_map_score', 0.541431198847439), ('infographics/evidence_type_table_list_score', 0.4973839635416816), ('infographics/evidence_type_text_score', 0.5912562903858297), ('infographics/evidence_type_visual_layout_score', 0.5122317063256603), ('infographics/reasoning_type_arithmetic_score', 0.4698793215916503), ('infographics/reasoning_type_comparison_score', 0.4466229466937155), ('infographics/reasoning_type_counting_score', 0.5772144522144522)]\n",
      "Got result for mmbench - 5600: [('mmbench/attribute_comparison', 0.6590909090909091), ('mmbench/attribute_recognition', 0.9295774647887324), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.525), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.5555555555555556), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.7466666666666667), ('mmbench/physical_relation', 0.7083333333333334), ('mmbench/spatial_relationship', 0.5), ('mmbench/structuralized_imagetext_understanding', 0.7012987012987013), ('mmbench/overall', 0.7878702280782688)]\n",
      "5600\n",
      "Got result for mmmu - 5800: [('mmmu/accounting', 0.6), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.7), ('mmmu/basic_medical_science', 0.6333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.3), ('mmmu/clinical_medicine', 0.7), ('mmmu/computer_science', 0.5333333333333333), ('mmmu/design', 0.7), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.3), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.43333333333333335), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.6333333333333333), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5388888888888889), ('mmmu/mllm_eval_accuracy', 0.55)]\n",
      "Got result for docvqa - 5800: [('docvqa/anls_total_score', 0.6481590973298108), ('docvqa/mllm_evaluation_anls_score', 0.6517799632443199), ('docvqa/mmllm_fixed_anls_score', 0.6950030568201728)]\n",
      "Got result for mathvista - 5800: [('mathvista/accuracy', 0.565)]\n",
      "Got result for ai2d - 5800: [('ai2d/accuracy', 0.8908678756476683)]\n",
      "Got result for chartqa - 5800: [('chartqa/accuracy', 0.800995224418768)]\n",
      "Got result for vqa - 5800: [('vqa/accuracy', 0.7168999999999779), ('vqa/recall', 0.7557119999999755), ('vqa/bleu', 0.0201967004686594), ('vqa/mllm_evaluation_accuracy', 0.7471439999999756)]\n",
      "Got result for textvqa - 5800: [('textvqa/accuracy', 65.32000000000028), ('textvqa/mllm_eval_accuracy', 71.02000000000031)]\n",
      "Got result for infographics_w_ocr - 5800: [('infographics_w_ocr/anls_total_score', 0.6400898567522709), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5890014464636287), ('infographics_w_ocr/answer_type_multi_span_score', 0.47299291825094913), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6396301558598124), ('infographics_w_ocr/answer_type_question_span_score', 0.699496336996337), ('infographics_w_ocr/answer_type_single_span_score', 0.6544817443493565), ('infographics_w_ocr/evidence_type_figure_score', 0.6234140801306912), ('infographics_w_ocr/evidence_type_map_score', 0.5936985723459224), ('infographics_w_ocr/evidence_type_table_list_score', 0.6338705344895262), ('infographics_w_ocr/evidence_type_text_score', 0.6928315164375424), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5748734071875359), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.644059849967384), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5279096383180244), ('infographics_w_ocr/reasoning_type_counting_score', 0.6241796664873588)]\n",
      "Got result for infographics - 5800: [('infographics/anls_total_score', 0.522499501173765), ('infographics/mllm_evaluation_anls_score', 0.4721290742230477), ('infographics/answer_type_multi_span_score', 0.3720889846584698), ('infographics/answer_type_non_extractive_score', 0.4864955719386098), ('infographics/answer_type_question_span_score', 0.7229894571240725), ('infographics/answer_type_single_span_score', 0.539040904972798), ('infographics/evidence_type_figure_score', 0.5052817739858165), ('infographics/evidence_type_map_score', 0.4599203673131576), ('infographics/evidence_type_table_list_score', 0.49032609281058714), ('infographics/evidence_type_text_score', 0.5713060250104918), ('infographics/evidence_type_visual_layout_score', 0.4803694002151992), ('infographics/reasoning_type_arithmetic_score', 0.38252985598876027), ('infographics/reasoning_type_comparison_score', 0.44829671664233195), ('infographics/reasoning_type_counting_score', 0.5746503496503497)]\n",
      "Got result for mmbench - 5800: [('mmbench/attribute_comparison', 0.7045454545454546), ('mmbench/attribute_recognition', 0.9436619718309859), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.575), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.5679012345679012), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.8), ('mmbench/physical_relation', 0.625), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6883116883116883), ('mmbench/overall', 0.7843912396018498)]\n",
      "5800\n",
      "Got result for mmmu - 6000: [('mmmu/accounting', 0.43333333333333335), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.4666666666666667), ('mmmu/art', 0.6), ('mmmu/art_theory', 0.6333333333333333), ('mmmu/basic_medical_science', 0.6333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.7), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.6333333333333333), ('mmmu/finance', 0.5), ('mmmu/geography', 0.5), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.5666666666666667), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.7), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5522222222222222), ('mmmu/mllm_eval_accuracy', 0.5544444444444444)]\n",
      "Got result for docvqa - 6000: [('docvqa/anls_total_score', 0.6225563382726894), ('docvqa/mllm_evaluation_anls_score', 0.624743853312618), ('docvqa/mmllm_fixed_anls_score', 0.6647037973554452)]\n",
      "Got result for mathvista - 6000: [('mathvista/accuracy', 0.55)]\n",
      "Got result for ai2d - 6000: [('ai2d/accuracy', 0.9028497409326425)]\n",
      "Got result for chartqa - 6000: [('chartqa/accuracy', 0.8121336186938924)]\n",
      "Got result for vqa - 6000: [('vqa/accuracy', 0.709799999999979), ('vqa/recall', 0.7481919999999755), ('vqa/bleu', 0.01966903917491436), ('vqa/mllm_evaluation_accuracy', 0.7396079999999767)]\n",
      "Got result for textvqa - 6000: [('textvqa/accuracy', 65.2180000000003), ('textvqa/mllm_eval_accuracy', 70.89000000000036)]\n",
      "Got result for infographics_w_ocr - 6000: [('infographics_w_ocr/anls_total_score', 0.6484606018739255), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5936063423998835), ('infographics_w_ocr/answer_type_multi_span_score', 0.4962421882450204), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6318134848876263), ('infographics_w_ocr/answer_type_question_span_score', 0.7085892739738894), ('infographics_w_ocr/answer_type_single_span_score', 0.6662293108259586), ('infographics_w_ocr/evidence_type_figure_score', 0.6304430156216781), ('infographics_w_ocr/evidence_type_map_score', 0.5967402278628253), ('infographics_w_ocr/evidence_type_table_list_score', 0.6467023727015322), ('infographics_w_ocr/evidence_type_text_score', 0.7070964945132682), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5511326870043679), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6450097847358117), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5230213169716028), ('infographics_w_ocr/reasoning_type_counting_score', 0.6058275058275057)]\n",
      "Got result for infographics - 6000: [('infographics/anls_total_score', 0.5235667422686492), ('infographics/mllm_evaluation_anls_score', 0.4750291600083593), ('infographics/answer_type_multi_span_score', 0.3903511578328981), ('infographics/answer_type_non_extractive_score', 0.4902410428631043), ('infographics/answer_type_question_span_score', 0.7260760073260073), ('infographics/answer_type_single_span_score', 0.5375230592087112), ('infographics/evidence_type_figure_score', 0.5082912169155553), ('infographics/evidence_type_map_score', 0.5104128711914946), ('infographics/evidence_type_table_list_score', 0.5032189065294689), ('infographics/evidence_type_text_score', 0.5659207124343892), ('infographics/evidence_type_visual_layout_score', 0.4882887255570725), ('infographics/reasoning_type_arithmetic_score', 0.4050341211300117), ('infographics/reasoning_type_comparison_score', 0.4472756110076091), ('infographics/reasoning_type_counting_score', 0.5608974358974359)]\n",
      "Got result for mmbench - 6000: [('mmbench/attribute_comparison', 0.7045454545454546), ('mmbench/attribute_recognition', 0.9436619718309859), ('mmbench/celebrity_recognition', 0.9292929292929293), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.475), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.5308641975308642), ('mmbench/ocr', 0.8461538461538461), ('mmbench/physical_property_reasoning', 0.7733333333333333), ('mmbench/physical_relation', 0.7083333333333334), ('mmbench/spatial_relationship', 0.5), ('mmbench/structuralized_imagetext_understanding', 0.6883116883116883), ('mmbench/overall', 0.786427401306212)]\n",
      "6000\n",
      "Got result for mmmu - 6200: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.7333333333333333), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.3333333333333333), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.5), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.4666666666666667), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.23333333333333334), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.5666666666666667), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.5433333333333333), ('mmmu/mllm_eval_accuracy', 0.5477777777777778)]\n",
      "Got result for docvqa - 6200: [('docvqa/anls_total_score', 0.6320537685629557), ('docvqa/mllm_evaluation_anls_score', 0.6348844596118374), ('docvqa/mmllm_fixed_anls_score', 0.6794462520691361)]\n",
      "Got result for mathvista - 6200: [('mathvista/accuracy', 0.564)]\n",
      "Got result for ai2d - 6200: [('ai2d/accuracy', 0.8957253886010362)]\n",
      "Got result for chartqa - 6200: [('chartqa/accuracy', 0.8031163518326795)]\n",
      "Got result for vqa - 6200: [('vqa/accuracy', 0.7085679999999783), ('vqa/recall', 0.7410039999999772), ('vqa/bleu', 0.0), ('vqa/mllm_evaluation_accuracy', 0.7325559999999772)]\n",
      "Got result for textvqa - 6200: [('textvqa/accuracy', 65.12400000000031), ('textvqa/mllm_eval_accuracy', 70.84400000000032)]\n",
      "Got result for infographics_w_ocr - 6200: [('infographics_w_ocr/anls_total_score', 0.643966629360914), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5957240252661248), ('infographics_w_ocr/answer_type_multi_span_score', 0.46594457020149255), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6504413157668132), ('infographics_w_ocr/answer_type_question_span_score', 0.6997751714097868), ('infographics_w_ocr/answer_type_single_span_score', 0.6568290867399764), ('infographics_w_ocr/evidence_type_figure_score', 0.6295221460858975), ('infographics_w_ocr/evidence_type_map_score', 0.5977059699897115), ('infographics_w_ocr/evidence_type_table_list_score', 0.6337661265614828), ('infographics_w_ocr/evidence_type_text_score', 0.6937912821607473), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5479495531133022), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6414179712981082), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5392213666987133), ('infographics_w_ocr/reasoning_type_counting_score', 0.6407925407925408)]\n",
      "Got result for infographics - 6200: [('infographics/anls_total_score', 0.5277842205193978), ('infographics/mllm_evaluation_anls_score', 0.4753568248826356), ('infographics/answer_type_multi_span_score', 0.36128719944509285), ('infographics/answer_type_non_extractive_score', 0.510042459047884), ('infographics/answer_type_question_span_score', 0.6908959096459096), ('infographics/answer_type_single_span_score', 0.5424217935048695), ('infographics/evidence_type_figure_score', 0.5158807171960773), ('infographics/evidence_type_map_score', 0.5100828381881943), ('infographics/evidence_type_table_list_score', 0.5015272827284045), ('infographics/evidence_type_text_score', 0.5716506873281096), ('infographics/evidence_type_visual_layout_score', 0.49125836494781006), ('infographics/reasoning_type_arithmetic_score', 0.42712378945255675), ('infographics/reasoning_type_comparison_score', 0.4549009088925855), ('infographics/reasoning_type_counting_score', 0.5769230769230769)]\n",
      "Got result for mmbench - 6200: [('mmbench/attribute_comparison', 0.6818181818181818), ('mmbench/attribute_recognition', 0.9436619718309859), ('mmbench/celebrity_recognition', 0.9292929292929293), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8611111111111112), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.5679012345679012), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.7466666666666667), ('mmbench/physical_relation', 0.7083333333333334), ('mmbench/spatial_relationship', 0.55), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7885844387132492)]\n",
      "6200\n",
      "Got result for mmmu - 6400: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.43333333333333335), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.7333333333333333), ('mmmu/basic_medical_science', 0.6), ('mmmu/biology', 0.26666666666666666), ('mmmu/chemistry', 0.3), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.5666666666666667), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5444444444444444), ('mmmu/mllm_eval_accuracy', 0.5488888888888889)]\n",
      "Got result for docvqa - 6400: [('docvqa/anls_total_score', 0.6456780104551609), ('docvqa/mllm_evaluation_anls_score', 0.6472735744733693), ('docvqa/mmllm_fixed_anls_score', 0.6880354760223297)]\n",
      "Got result for mathvista - 6400: [('mathvista/accuracy', 0.581)]\n",
      "Got result for ai2d - 6400: [('ai2d/accuracy', 0.9034974093264249)]\n",
      "Got result for chartqa - 6400: [('chartqa/accuracy', 0.8083823765033065)]\n",
      "Got result for vqa - 6400: [('vqa/accuracy', 0.7016279999999785), ('vqa/recall', 0.736695999999977), ('vqa/bleu', 0.013117595575749874), ('vqa/mllm_evaluation_accuracy', 0.7285679999999773)]\n",
      "Got result for textvqa - 6400: [('textvqa/accuracy', 64.49800000000033), ('textvqa/mllm_eval_accuracy', 70.66200000000035)]\n",
      "Got result for infographics_w_ocr - 6400: [('infographics_w_ocr/anls_total_score', 0.6480874088053854), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5970853112819781), ('infographics_w_ocr/answer_type_multi_span_score', 0.47117033310368694), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6514423490915355), ('infographics_w_ocr/answer_type_question_span_score', 0.731547619047619), ('infographics_w_ocr/answer_type_single_span_score', 0.6596650324288442), ('infographics_w_ocr/evidence_type_figure_score', 0.6357192647034131), ('infographics_w_ocr/evidence_type_map_score', 0.6041416135540679), ('infographics_w_ocr/evidence_type_table_list_score', 0.6414376624419228), ('infographics_w_ocr/evidence_type_text_score', 0.6957928450353565), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5537735320747905), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6584964122635353), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5362850732163409), ('infographics_w_ocr/reasoning_type_counting_score', 0.6287878787878787)]\n",
      "Got result for infographics - 6400: [('infographics/anls_total_score', 0.5346366444805102), ('infographics/mllm_evaluation_anls_score', 0.48217456418720644), ('infographics/answer_type_multi_span_score', 0.4160775137549833), ('infographics/answer_type_non_extractive_score', 0.49788151872238673), ('infographics/answer_type_question_span_score', 0.7063227904574059), ('infographics/answer_type_single_span_score', 0.5485397858341805), ('infographics/evidence_type_figure_score', 0.5215268135623273), ('infographics/evidence_type_map_score', 0.5211389437987555), ('infographics/evidence_type_table_list_score', 0.5077253737290466), ('infographics/evidence_type_text_score', 0.5712222300962516), ('infographics/evidence_type_visual_layout_score', 0.5074194691454754), ('infographics/reasoning_type_arithmetic_score', 0.4040358899091776), ('infographics/reasoning_type_comparison_score', 0.4635989494635539), ('infographics/reasoning_type_counting_score', 0.581002331002331)]\n",
      "Got result for mmbench - 6400: [('mmbench/attribute_comparison', 0.7045454545454546), ('mmbench/attribute_recognition', 0.9436619718309859), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.9166666666666666), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.5432098765432098), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.76), ('mmbench/physical_relation', 0.75), ('mmbench/spatial_relationship', 0.5), ('mmbench/structuralized_imagetext_understanding', 0.7142857142857143), ('mmbench/overall', 0.7977350502215844)]\n",
      "6400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.5444</td>\n",
       "      <td>0.5433</td>\n",
       "      <td>0.6706</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.8488</td>\n",
       "      <td>0.7014</td>\n",
       "      <td>0.7256</td>\n",
       "      <td>0.66916</td>\n",
       "      <td>0.6587</td>\n",
       "      <td>0.5542</td>\n",
       "      <td>0.7887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>0.5422</td>\n",
       "      <td>0.5278</td>\n",
       "      <td>0.6631</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.8724</td>\n",
       "      <td>0.7788</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.66484</td>\n",
       "      <td>0.6527</td>\n",
       "      <td>0.5203</td>\n",
       "      <td>0.7947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5400</th>\n",
       "      <td>0.5444</td>\n",
       "      <td>0.5356</td>\n",
       "      <td>0.6418</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.7923</td>\n",
       "      <td>0.7172</td>\n",
       "      <td>0.65898</td>\n",
       "      <td>0.6535</td>\n",
       "      <td>0.5298</td>\n",
       "      <td>0.7902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5600</th>\n",
       "      <td>0.5278</td>\n",
       "      <td>0.5211</td>\n",
       "      <td>0.6488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8951</td>\n",
       "      <td>0.7982</td>\n",
       "      <td>0.7123</td>\n",
       "      <td>0.65266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.7879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5800</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5389</td>\n",
       "      <td>0.6482</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.7169</td>\n",
       "      <td>0.6532</td>\n",
       "      <td>0.6401</td>\n",
       "      <td>0.5225</td>\n",
       "      <td>0.7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.5522</td>\n",
       "      <td>0.6226</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.9028</td>\n",
       "      <td>0.8121</td>\n",
       "      <td>0.7098</td>\n",
       "      <td>0.65218</td>\n",
       "      <td>0.6485</td>\n",
       "      <td>0.5236</td>\n",
       "      <td>0.7864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6200</th>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.5433</td>\n",
       "      <td>0.6321</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.8957</td>\n",
       "      <td>0.8031</td>\n",
       "      <td>0.7086</td>\n",
       "      <td>0.65124</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.5278</td>\n",
       "      <td>0.7886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6400</th>\n",
       "      <td>0.5489</td>\n",
       "      <td>0.5444</td>\n",
       "      <td>0.6457</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.9035</td>\n",
       "      <td>0.8084</td>\n",
       "      <td>0.7016</td>\n",
       "      <td>0.64498</td>\n",
       "      <td>0.6481</td>\n",
       "      <td>0.5346</td>\n",
       "      <td>0.7977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1  docvqa mathvista    ai2d chartqa     vqa  textvqa  \\\n",
       "5000  0.5444  0.5433  0.6706     0.476  0.8488  0.7014  0.7256  0.66916   \n",
       "5200  0.5422  0.5278  0.6631     0.524  0.8724  0.7788   0.728  0.66484   \n",
       "5400  0.5444  0.5356  0.6418     0.521  0.8818  0.7923  0.7172  0.65898   \n",
       "5600  0.5278  0.5211  0.6488       NaN  0.8951  0.7982  0.7123  0.65266   \n",
       "5800    0.55  0.5389  0.6482     0.565  0.8909   0.801  0.7169   0.6532   \n",
       "6000  0.5544  0.5522  0.6226      0.55  0.9028  0.8121  0.7098  0.65218   \n",
       "6200  0.5478  0.5433  0.6321     0.564  0.8957  0.8031  0.7086  0.65124   \n",
       "6400  0.5489  0.5444  0.6457     0.581  0.9035  0.8084  0.7016  0.64498   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "5000             0.6587       0.5542  0.7887  \n",
       "5200             0.6527       0.5203  0.7947  \n",
       "5400             0.6535       0.5298  0.7902  \n",
       "5600                NaN         0.54  0.7879  \n",
       "5800             0.6401       0.5225  0.7844  \n",
       "6000             0.6485       0.5236  0.7864  \n",
       "6200              0.644       0.5278  0.7886  \n",
       "6400             0.6481       0.5346  0.7977  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores_all(\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_64nodes_i18n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New checkpoints: [6000, 6200, 6400, 6600, 6800, 7000, 7200, 7400, 7600, 7800, 8000, 8200, 8400, 8600, 8800, 9000, 9200, 9400, 9600, 9800, 10000, 10200]\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30/checkpoint-9200\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30/evals/eval_jobs_checkpoint-9200.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meval_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_eval_sweep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_sbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEVAL_SBATCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_config_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEVAL_CONFIG_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43maligner_parent_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALIGNER_CODE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9200\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-06bc3d6b93146dddd/user/tranx/experiments/lib/eval_helper.py:256\u001b[0m, in \u001b[0;36mrun_eval_sweep\u001b[0;34m(output_dir, eval_sbatch, eval_config_dir, aligner_parent_dir, print_cmd, min_checkpoint)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m checkpoints_eval:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart eval for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoint-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 256\u001b[0m     \u001b[43mrun_eval_plan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_base_sbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43maligner_parent_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maligner_parent_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_config_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_config_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# save_eval_jobs=f\"{output_dir}/evals/eval_jobs_checkpoint-{c}.json\"\u001b[39;49;00m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_eval_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_eval_jobs_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_cmd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_cmd\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-06bc3d6b93146dddd/user/tranx/experiments/lib/eval_helper.py:93\u001b[0m, in \u001b[0;36mrun_eval_plan\u001b[0;34m(eval_base_sbatch, aligner_parent_dir, eval_config_dir, checkpoint_dir, checkpoints, save_eval_jobs, benchmarks, rerun_if_exists, print_cmd)\u001b[0m\n\u001b[1;32m     82\u001b[0m         job_id \u001b[38;5;241m=\u001b[39m run_sbatch_job(\n\u001b[1;32m     83\u001b[0m             sbatch_base_script\u001b[38;5;241m=\u001b[39meval_base_sbatch,\n\u001b[1;32m     84\u001b[0m             sbatch_overwrite\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m             print_cmd\u001b[38;5;241m=\u001b[39mprint_cmd\n\u001b[1;32m     89\u001b[0m         )\n\u001b[1;32m     91\u001b[0m         job_dict[benchmark][chk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(job_id)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_eval_jobs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     94\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(job_dict, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval jobs saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_eval_jobs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30/evals/eval_jobs_checkpoint-9200.json'"
     ]
    }
   ],
   "source": [
    "eval_helper.run_eval_sweep(\n",
    "    output_dir=f\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30\",\n",
    "    eval_sbatch=EVAL_SBATCH,\n",
    "    eval_config_dir=EVAL_CONFIG_DIR,\n",
    "    aligner_parent_dir=ALIGNER_CODE_DIR,\n",
    "    min_checkpoint=9200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVAL_CONFIG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 mmmu 9200\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 docvqa 9200\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 mathvista 9200\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 ai2d 9200\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 chartqa 9200\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 vqa 9200\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 textvqa 9200\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 infographics_w_ocr 9200\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 infographics 9200\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 mmbench 9200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30/evals/eval_jobs_checkpoint-9200.json\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30\"\n",
    "c = 9200\n",
    "eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=EVAL_SBATCH,\n",
    "    aligner_parent_dir=ALIGNER_CODE_DIR,\n",
    "    eval_config_dir=EVAL_CONFIG_DIR,\n",
    "    checkpoint_dir=output_dir,\n",
    "    checkpoints=[c],\n",
    "    save_eval_jobs=eval_helper.get_eval_jobs_record(output_dir, c),\n",
    "    # benchmarks=[\"mmmu\"],\n",
    "    rerun_if_exists=True,\n",
    "    print_cmd=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mmmu': {'9200': 20362}, 'docvqa': {'9200': 20363}, 'mathvista': {'9200': 20364}, 'ai2d': {'9200': 20365}, 'chartqa': {'9200': 20366}, 'vqa': {'9200': 20367}, 'textvqa': {'9200': 20368}, 'infographics_w_ocr': {'9200': 20369}, 'infographics': {'9200': 20370}, 'mmbench': {'9200': 20371}}\n",
      "Got result for mathvista - 9200: [('mathvista/accuracy', 0.532)]\n",
      "Got result for chartqa - 9200: [('chartqa/accuracy', 0.5888241969056048)]\n",
      "Got result for mmbench - 9200: [('mmbench/attribute_comparison', 0.7272727272727273), ('mmbench/attribute_recognition', 0.9295774647887324), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.5432098765432098), ('mmbench/ocr', 0.8461538461538461), ('mmbench/physical_property_reasoning', 0.8), ('mmbench/physical_relation', 0.5833333333333334), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6883116883116883), ('mmbench/overall', 0.7868918887645543)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1 docvqa mathvista ai2d chartqa  vqa textvqa  \\\n",
       "9200     NaN     NaN    NaN     0.532  NaN  0.5888  NaN     NaN   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "9200                NaN          NaN  0.7869  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores(output_dir, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30/evals/eval_jobs_checkpoint-9200.json'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_jobs_record(\n",
    "    \"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30\", \n",
    "    9200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 mmmu 9400\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 docvqa 9400\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 mathvista 9400\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 ai2d 9400\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 chartqa 9400\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 vqa 9400\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 textvqa 9400\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 infographics_w_ocr 9400\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 infographics 9400\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30 mmbench 9400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30/evals/eval_jobs_checkpoint-9400.json\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30\"\n",
    "c = 9400\n",
    "eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=EVAL_SBATCH,\n",
    "    aligner_parent_dir=ALIGNER_CODE_DIR,\n",
    "    eval_config_dir=\"/fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval\",\n",
    "    checkpoint_dir=output_dir,\n",
    "    checkpoints=[c],\n",
    "    save_eval_jobs=eval_helper.get_eval_jobs_record(output_dir, c),\n",
    "    # benchmarks=[\"mmmu\"],\n",
    "    rerun_if_exists=True,\n",
    "    print_cmd=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_SBATCH = \"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\"\n",
    "ALIGNER_CODE_DIR=\"/fsx_0/user/tranx/eval_adel\"\n",
    "EVAL_CONFIG_DIR=f\"/fsx_0/user/tranx/eval_adel/llm_mm_aligner/experiments/aws_adel/eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-700.json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_jobs_record(output_dir, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128 ai2d 700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-700.json\n"
     ]
    }
   ],
   "source": [
    "ALIGNER_ADEL_PROD = '/fsx_0/user/tranx/adel_prod'\n",
    "\n",
    "output_dir = \"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128\"\n",
    "c = 700\n",
    "eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    aligner_parent_dir=ALIGNER_ADEL_PROD,\n",
    "    eval_config_dir=f\"{ALIGNER_ADEL_PROD}/llm_mm_aligner/experiments/aws_adel/eval_sft\",\n",
    "    checkpoint_dir=output_dir,\n",
    "    checkpoints=[c],\n",
    "    save_eval_jobs=eval_helper.get_eval_jobs_record(output_dir, c),\n",
    "    # benchmarks=[\"mmmu\"],\n",
    "    # benchmarks=[\"mathvista\"],\n",
    "    benchmarks=[\"ai2d\"],\n",
    "    rerun_if_exists=True,\n",
    "    print_cmd=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate overwrite\n",
    "train_config = utils.read_json(\"/fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_mh19_stage2/stage2_MM9_70B_MH19_336px_128nodes_exp30.json\")\n",
    "\n",
    "for f in glob.glob(\"/fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_mh19_stage2/eval_*.json\"):\n",
    "    eval_config = utils.read_json(f)\n",
    "    \n",
    "    benchmark_file = f.split(\"/\")[-1]\n",
    "    \n",
    "    overwrite = eval_helper.get_eval_config_overwrite(train_config, eval_config)\n",
    "    \n",
    "    utils.save_json(overwrite, f\"/fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_mh19_stage2/eval_overwrite/overwrite_{benchmark_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['scheduler_type', 'hpc_job_oncall', 'num_gpus', 'num_nodes', 'trainer_args', 'fsdp_config', 'datarecipe'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmbench\n",
      "infographics_w_ocr\n",
      "docvqa\n"
     ]
    }
   ],
   "source": [
    "# Generate eval config \n",
    "config_dir = \"/fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65\"\n",
    "train_config = utils.read_json(f\"{config_dir}/sft_mm9_babyLora_16nodes.json\")\n",
    "\n",
    "# for b in eval_helper.ALL_BENCHMARKS:\n",
    "for b in [\"mmbench\", \"infographics_w_ocr\", \"docvqa\"]:\n",
    "    print(b)\n",
    "    overwrite = utils.read_json(f\"/fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_mh19_stage2/eval_overwrite/overwrite_eval_{b}.json\")\n",
    "    \n",
    "    eval_config = eval_helper.gen_eval_config(train_config, overwrite)\n",
    "    \n",
    "    utils.save_json(eval_config, f\"/fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_{b}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fb overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'infographics_w_ocr': {'train_file': '/fsx_0/dataset01/infographicsvqa/processed_val.jsonl',\n",
       "  'validation_file': '/fsx_0/dataset01/infographicsvqa/processed_val.jsonl'},\n",
       " 'infographics': {'train_file': '/fsx_0/dataset01/infographicsvqa/processed_val.jsonl',\n",
       "  'validation_file': '/fsx_0/dataset01/infographicsvqa/processed_val.jsonl'},\n",
       " 'mmbench': {'train_file': '/fsx_0/dataset01/mmbench/processed_dev_20231212.json',\n",
       "  'validation_file': '/fsx_0/dataset01/mmbench/processed_dev_20231212.json'},\n",
       " 'mathvista': {'train_file': '/fsx_0/dataset01/mathvista/test.jsonl',\n",
       "  'validation_file': '/fsx_0/dataset01/mathvista/test.jsonl'},\n",
       " 'ai2d': {'train_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl',\n",
       "  'validation_file': '/fsx_0/dataset01/ai2d/ai2d_test.jsonl'},\n",
       " 'docvqa': {'train_file': '/fsx_0/dataset01/docvqa/val.jsonl',\n",
       "  'validation_file': '/fsx_0/dataset01/docvqa/val.jsonl'},\n",
       " 'textvqa': {'train_file': '/fsx_0/dataset01/textvqa/text_vqa_val_set_updated.jsonl',\n",
       "  'validation_file': '/fsx_0/dataset01/textvqa/text_vqa_val_set_updated.jsonl'},\n",
       " 'chartqa': {'train_file': '/fsx_0/dataset01/ChartQA/chartqa_test.jsonl',\n",
       "  'validation_file': '/fsx_0/dataset01/ChartQA/chartqa_test.jsonl'},\n",
       " 'mmmu': {'train_file': '/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl',\n",
       "  'validation_file': '/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl'},\n",
       " 'vqa': {'train_file': '/fsx_0/dataset01/vqa/VQAv2_train_processed.jsonl',\n",
       "  'validation_file': '/fsx_0/dataset01/vqa/VQAv2_val_multians_20231118.json'}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {}\n",
    "for f in glob.glob(\"/fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_*.json\"):\n",
    "    conf = utils.read_json(f)\n",
    "    bm = f.split(\"/\")[-1].replace(\"eval_\", \"\").replace(\".json\", \"\")\n",
    "    # print(bm)\n",
    "    data_files[bm] = {\n",
    "        \"train_file\": conf[\"eval_args\"][\"train_file\"],\n",
    "        \"validation_file\": conf[\"eval_args\"][\"validation_file\"]\n",
    "    }\n",
    "\n",
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmmu\n",
      "docvqa\n",
      "mathvista\n",
      "ai2d\n",
      "chartqa\n",
      "vqa\n",
      "textvqa\n",
      "infographics_w_ocr\n",
      "infographics\n",
      "mmbench\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "from pprint import pprint\n",
    "\n",
    "config_dir = \"/fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65_fb\"\n",
    "train_config = utils.read_json(f\"{config_dir}/sft_mm9_babyLora_16nodes_r26a.json\")\n",
    "\n",
    "for b in eval_helper.ALL_BENCHMARKS:\n",
    "    print(b)\n",
    "    overwrite_file = f\"/fsx_0/user/tranx/experiments/eval/sft_eval_overwrite_fb/{b}.json\"\n",
    "    overwrite = utils.read_json(overwrite_file)\n",
    "    \n",
    "    eval_config = {\n",
    "        \"eval_only\": True,\n",
    "        \"eval_args\": copy.deepcopy(train_config[\"trainer_args\"]),\n",
    "        \"fsdp_config\": copy.deepcopy(train_config[\"fsdp_config\"])\n",
    "    }\n",
    "    \n",
    "\n",
    "    eval_config[\"eval_args\"].update(overwrite[\"eval_args\"])\n",
    "    \n",
    "\n",
    "    eval_config[\"eval_args\"].update({\n",
    "        \"resume_from_checkpoint\": \"CHECKPOINT_PATH\",\n",
    "        \"logging_dir\": \"LOGGING_DIR\",\n",
    "        \"output_dir\": \"OUTPUT_DIR\",\n",
    "        \"tb_logdir\": \"TB_LOGDIR\",\n",
    "        \"eval_ckpt\": \"EVAL_CHECKPOINT\",\n",
    "        \"train_file\": data_files[b][\"train_file\"],\n",
    "        \"validation_file\": data_files[b][\"validation_file\"]\n",
    "    })\n",
    "    \n",
    "    # for k in [\"per_device_eval_batch_size\", \n",
    "    #           \"batch_size_generation\", \n",
    "    #           \"max_seq_len\",\n",
    "    #           \"max_length\",\n",
    "    #           \"max_new_tokens\"\n",
    "    #           ]:\n",
    "    #     if k in eval_config[\"eval_args\"]:\n",
    "    #         print(k, \":\", eval_config[\"eval_args\"][k])\n",
    "    # print(\"--------------\")\n",
    "    # to fix OOM\n",
    "    eval_config[\"eval_args\"].update({\n",
    "        \"per_device_eval_batch_size\": 1,\n",
    "        \"batch_size_generation\": 1\n",
    "    })\n",
    "    \n",
    "    utils.save_json(eval_config, f\"{config_dir}/eval_{b}.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65_fb/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a mmmu 100\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65_fb/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a docvqa 100\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65_fb/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a mathvista 100\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65_fb/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a ai2d 100\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65_fb/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a chartqa 100\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65_fb/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a vqa 100\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65_fb/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a textvqa 100\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65_fb/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a infographics_w_ocr 100\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65_fb/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a infographics 100\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65_fb/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a mmbench 100\n",
      "OrderedDict([('mathvista', {100: 21810}), ('mmmu', {100: 21808}), ('docvqa', {100: 21809}), ('ai2d', {100: 21811}), ('chartqa', {100: 21812}), ('vqa', {100: 21813}), ('textvqa', {100: 21814}), ('infographics_w_ocr', {100: 21815}), ('infographics', {100: 21816}), ('mmbench', {100: 21817})])\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/evals/eval_jobs_checkpoint-100.json\n"
     ]
    }
   ],
   "source": [
    "# try a run\n",
    "ALIGNER_ADEL_PROD_FSDP = '/fsx_0/user/tranx/adel_prod_fsdp'\n",
    "ALIGNER_ADEL_PROD = '/fsx_0/user/tranx/adel_prod'\n",
    "output_dir = \"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a\"\n",
    "\n",
    "c = 100\n",
    "eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    aligner_parent_dir=ALIGNER_ADEL_PROD_FSDP,\n",
    "    eval_config_dir=f\"{ALIGNER_ADEL_PROD}/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65\",\n",
    "    checkpoint_dir=output_dir,\n",
    "    checkpoints=[c],\n",
    "    save_eval_jobs=eval_helper.get_eval_jobs_record(output_dir, c),\n",
    "    \n",
    "    # # benchmarks=[\"mmmu\"],\n",
    "    # benchmarks=[\"mathvista\"],\n",
    "    # # benchmarks=[\"ai2d\"],\n",
    "    # benchmarks=[b for b in eval_helper.ALL_BENCHMARKS if b not in [\"mmmu\"]],\n",
    "    \n",
    "    rerun_if_exists=True,\n",
    "    update_if_exists=True,\n",
    "    print_cmd=True,\n",
    "    print_job_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [mmmu_v2, mmmu_v1, docvqa, mathvista, ai2d, chartqa, vqa, textvqa, infographics_w_ocr, infographics, mmbench]\n",
       "Index: []"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores_all(\"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65_fb/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a mmmu 100\n",
      "OrderedDict([('mathvista', OrderedDict([('100', 21750)])), ('mmmu', {100: 21752})])\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/evals/eval_jobs_checkpoint-100.json\n"
     ]
    }
   ],
   "source": [
    "# try a run\n",
    "ALIGNER_ADEL_PROD = '/fsx_0/user/tranx/adel_prod'\n",
    "output_dir = \"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a\"\n",
    "\n",
    "c = 100\n",
    "eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    aligner_parent_dir=ALIGNER_ADEL_PROD,\n",
    "    eval_config_dir=f\"{ALIGNER_ADEL_PROD}/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65_fb\",\n",
    "    checkpoint_dir=output_dir,\n",
    "    checkpoints=[c],\n",
    "    save_eval_jobs=eval_helper.get_eval_jobs_record(output_dir, c),\n",
    "    \n",
    "    benchmarks=[\"mmmu\"],\n",
    "    # benchmarks=[\"mathvista\"],\n",
    "    # benchmarks=[\"ai2d\"],\n",
    "    \n",
    "    rerun_if_exists=True,\n",
    "    update_if_exists=True,\n",
    "    print_cmd=True,\n",
    "    print_job_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes mathvista 3000\n",
      "{'mathvista': {3000: 20495}}\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-3000.json\n"
     ]
    }
   ],
   "source": [
    "# try a run\n",
    "ALIGNER_ADEL_PROD = '/fsx_0/user/tranx/adel_prod'\n",
    "\n",
    "output_dir = \"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes\"\n",
    "\n",
    "c = 3000\n",
    "eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    aligner_parent_dir=ALIGNER_ADEL_PROD,\n",
    "    eval_config_dir=f\"{ALIGNER_ADEL_PROD}/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65\",\n",
    "    checkpoint_dir=output_dir,\n",
    "    checkpoints=[c],\n",
    "    save_eval_jobs=eval_helper.get_eval_jobs_record(output_dir, c),\n",
    "    \n",
    "    # benchmarks=[\"mmmu\"],\n",
    "    benchmarks=[\"mathvista\"],\n",
    "    # benchmarks=[\"ai2d\"],\n",
    "    \n",
    "    rerun_if_exists=True,\n",
    "    print_cmd=True,\n",
    "    print_job_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes mmmu 900\n",
      "{'mmmu': {900: 20530}}\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-900.json\n"
     ]
    }
   ],
   "source": [
    "c = 900\n",
    "eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    aligner_parent_dir=ALIGNER_ADEL_PROD,\n",
    "    eval_config_dir=f\"{ALIGNER_ADEL_PROD}/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65\",\n",
    "    checkpoint_dir=output_dir,\n",
    "    checkpoints=[c],\n",
    "    save_eval_jobs=eval_helper.get_eval_jobs_record(output_dir, c),\n",
    "    \n",
    "    benchmarks=[\"mmmu\"],\n",
    "    # benchmarks=[\"mathvista\"],\n",
    "    # benchmarks=[\"ai2d\"],\n",
    "    \n",
    "    rerun_if_exists=True,\n",
    "    print_cmd=True,\n",
    "    print_job_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes mmmu 1000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes docvqa 1000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes mathvista 1000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes ai2d 1000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes chartqa 1000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes vqa 1000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes textvqa 1000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes infographics_w_ocr 1000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes infographics 1000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes mmbench 1000\n",
      "{'mmmu': {1000: 20512}, 'docvqa': {1000: 20513}, 'mathvista': {1000: 20514}, 'ai2d': {1000: 20515}, 'chartqa': {1000: 20516}, 'vqa': {1000: 20517}, 'textvqa': {1000: 20518}, 'infographics_w_ocr': {1000: 20519}, 'infographics': {1000: 20520}, 'mmbench': {1000: 20521}}\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-1000.json\n"
     ]
    }
   ],
   "source": [
    "c = 1000\n",
    "eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    aligner_parent_dir=ALIGNER_ADEL_PROD,\n",
    "    eval_config_dir=f\"{ALIGNER_ADEL_PROD}/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65\",\n",
    "    checkpoint_dir=output_dir,\n",
    "    checkpoints=[c],\n",
    "    save_eval_jobs=eval_helper.get_eval_jobs_record(output_dir, c),\n",
    "    \n",
    "    # benchmarks=[\"mmmu\"],\n",
    "    # benchmarks=[\"mathvista\"],\n",
    "    # benchmarks=[\"ai2d\"],\n",
    "    \n",
    "    rerun_if_exists=True,\n",
    "    print_cmd=True,\n",
    "    print_job_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200]\n",
      "{'mmmu': {'1000': 20512}, 'docvqa': {'1000': 20513}, 'mathvista': {'1000': 20514}, 'ai2d': {'1000': 20515}, 'chartqa': {'1000': 20516}, 'vqa': {'1000': 20517}, 'textvqa': {'1000': 20518}, 'infographics_w_ocr': {'1000': 20519}, 'infographics': {'1000': 20520}, 'mmbench': {'1000': 20521}}\n",
      "Got result for mmmu - 1000: [('mmmu/accounting', 0.5333333333333333), ('mmmu/agriculture', 0.6666666666666666), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.43333333333333335), ('mmmu/design', 0.7), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.3333333333333333), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.3333333333333333), ('mmmu/geography', 0.6), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.23333333333333334), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.6), ('mmmu/physics', 0.3333333333333333), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5255555555555556), ('mmmu/mllm_eval_accuracy', 0.54)]\n",
      "Got result for chartqa - 1000: [('chartqa/accuracy', 0.5610157242657646)]\n",
      "1000\n",
      "{'mmmu': {'1000': 20502}, 'docvqa': {'1000': 20503}, 'mathvista': {'1000': 20504}, 'ai2d': {'1000': 20505}, 'chartqa': {'1000': 20506}, 'vqa': {'1000': 20507}, 'textvqa': {'1000': 20508}, 'infographics_w_ocr': {'1000': 20509}, 'infographics': {'1000': 20510}, 'mmbench': {'1000': 20511}}\n",
      "Got result for mmmu - 1000: [('mmmu/accounting', 0.5333333333333333), ('mmmu/agriculture', 0.6666666666666666), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.43333333333333335), ('mmmu/design', 0.7), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.3333333333333333), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.3333333333333333), ('mmmu/geography', 0.6), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.23333333333333334), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.6), ('mmmu/physics', 0.3333333333333333), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5255555555555556), ('mmmu/mllm_eval_accuracy', 0.54)]\n",
      "Got result for chartqa - 1000: [('chartqa/accuracy', 0.5610157242657646)]\n",
      "3000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1 docvqa mathvista ai2d chartqa  vqa textvqa  \\\n",
       "1000    0.54  0.5256    NaN       NaN  NaN   0.561  NaN     NaN   \n",
       "1000    0.54  0.5256    NaN       NaN  NaN   0.561  NaN     NaN   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "1000                NaN          NaN     NaN  \n",
       "1000                NaN          NaN     NaN  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores_all(\"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c mmmu 1000\n",
      "{'mmmu': {1000: 20550}}\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-1000.json\n"
     ]
    }
   ],
   "source": [
    "c = 1000\n",
    "eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    aligner_parent_dir=ALIGNER_ADEL_PROD,\n",
    "    eval_config_dir=f\"{ALIGNER_ADEL_PROD}/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65\",\n",
    "    checkpoint_dir=\"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c\",\n",
    "    checkpoints=[c],\n",
    "    save_eval_jobs=eval_helper.get_eval_jobs_record(output_dir, c),\n",
    "    \n",
    "    benchmarks=[\"mmmu\"],\n",
    "    # benchmarks=[\"mathvista\"],\n",
    "    # benchmarks=[\"ai2d\"],\n",
    "    \n",
    "    rerun_if_exists=True,\n",
    "    print_cmd=True,\n",
    "    print_job_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes docvqa 1000\n",
      "{'docvqa': {1000: 20622}}\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-1000.json\n"
     ]
    }
   ],
   "source": [
    "c = 1000\n",
    "eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    aligner_parent_dir=ALIGNER_ADEL_PROD,\n",
    "    eval_config_dir=f\"{ALIGNER_ADEL_PROD}/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65\",\n",
    "    checkpoint_dir=output_dir,\n",
    "    checkpoints=[c],\n",
    "    save_eval_jobs=eval_helper.get_eval_jobs_record(output_dir, c),\n",
    "    \n",
    "    # benchmarks=[\"mmmu\"],\n",
    "    # benchmarks=[\"mathvista\"],\n",
    "    # benchmarks=[\"ai2d\"],\n",
    "    # benchmarks=[\"mmbench\", \"infographics_w_ocr\", \"docvqa\"],\n",
    "    # benchmarks=[\"mmbench\", \"infographics_w_ocr\"],\n",
    "    # benchmarks=[\"mmbench\", \"infographics_w_ocr\"],\n",
    "    benchmarks=[\"docvqa\"],\n",
    "    \n",
    "    rerun_if_exists=True,\n",
    "    print_cmd=True,\n",
    "    print_job_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mmmu',\n",
       " 'docvqa',\n",
       " 'mathvista',\n",
       " 'ai2d',\n",
       " 'chartqa',\n",
       " 'vqa',\n",
       " 'textvqa',\n",
       " 'infographics_w_ocr',\n",
       " 'infographics',\n",
       " 'mmbench']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.ALL_BENCHMARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIGNER_ADEL_PROD = '/fsx_0/user/tranx/adel_prod'\n",
    "EVAL_SBATCH = \"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\"\n",
    "EVAL_CONFIG_DIR_EMB65 = f\"{ALIGNER_ADEL_PROD}/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65\"\n",
    "EVAL_CONFIG_DIR_EMB65_FB = f\"{ALIGNER_ADEL_PROD}/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65_fb\"\n",
    "\n",
    "EVAL_CONFIG_DIR_EMB129 = f\"{ALIGNER_ADEL_PROD}/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb129\"\n",
    "\n",
    "\n",
    "OUTPUT_SFT1 = \"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes\"\n",
    "OUTPUT_SFT3 = \"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e\"\n",
    "OUTPUT_SFT4 = \"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c\"\n",
    "\n",
    "OUTPUT_SFT2 = \"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128\"\n",
    "OUTPUT_SFT5 = \"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128\"\n",
    "\n",
    "OUTPUT_SFT6 = \"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a\"\n",
    "OUTPUT_SFT7 = \"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26b\"\n",
    "\n",
    "# BENCHMARKS = [\"mathvista\", \"vqa\"]\n",
    "BENCHMARKS = [\"mmmu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New checkpoints: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200]\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-1000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-1000.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-1100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-1100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-1200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-1200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-1300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-1300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-1400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-1400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-1500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-1500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-1600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-1600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-1700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-1700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-1800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-1800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-1900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-1900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-2000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-2000.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-2100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-2100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-2200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-2200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-2300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-2300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-2400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-2400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-2500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-2500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-2600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-2600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-2700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-2700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-2800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-2800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-2900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-2900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-3000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-3000.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-3100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-3100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/checkpoint-3200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-3200.json\n"
     ]
    }
   ],
   "source": [
    "eval_helper.run_eval_sweep(\n",
    "    output_dir=OUTPUT_SFT1,\n",
    "    eval_sbatch=EVAL_SBATCH,\n",
    "    eval_config_dir=EVAL_CONFIG_DIR_EMB65,\n",
    "    aligner_parent_dir=ALIGNER_ADEL_PROD,\n",
    "    # benchmarks=[\"mmmu\", \"chartqa\", \"mmbench\"],\n",
    "    benchmarks=BENCHMARKS,\n",
    "    update_if_exists=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.5456</td>\n",
       "      <td>0.5278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.5422</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.5433</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.5311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.5456</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>0.5411</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>0.5378</td>\n",
       "      <td>0.5211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5685</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>0.5344</td>\n",
       "      <td>0.5189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>0.5389</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>0.5467</td>\n",
       "      <td>0.5311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>0.5322</td>\n",
       "      <td>0.5156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.513</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.5178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>0.5311</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>0.5378</td>\n",
       "      <td>0.5244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.508</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.5389</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>0.5367</td>\n",
       "      <td>0.5189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>0.5322</td>\n",
       "      <td>0.5156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>0.5344</td>\n",
       "      <td>0.5189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5517</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>0.5289</td>\n",
       "      <td>0.5122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>0.5267</td>\n",
       "      <td>0.5122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.5144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>0.5322</td>\n",
       "      <td>0.5178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>0.5289</td>\n",
       "      <td>0.5111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1 docvqa mathvista ai2d chartqa  vqa textvqa  \\\n",
       "100     0.55  0.5322    NaN     0.501  NaN  0.6372  NaN     NaN   \n",
       "200   0.5456  0.5278    NaN     0.492  NaN  0.6097  NaN     NaN   \n",
       "300   0.5422  0.5267    NaN       NaN  NaN  0.5957  NaN     NaN   \n",
       "400   0.5433  0.5267    NaN     0.495  NaN  0.5745  NaN     NaN   \n",
       "500   0.5478  0.5311    NaN     0.503  NaN  0.5808  NaN     NaN   \n",
       "600   0.5478    0.53    NaN       NaN  NaN  0.5804  NaN     NaN   \n",
       "700   0.5456    0.53    NaN       NaN  NaN  0.5823  NaN     NaN   \n",
       "800     0.55  0.5333    NaN     0.515  NaN  0.5719  NaN     NaN   \n",
       "900   0.5411  0.5256    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "1000     NaN     NaN    NaN     0.507  NaN     NaN  NaN     NaN   \n",
       "1100  0.5378  0.5211    NaN       NaN  NaN  0.5685  NaN     NaN   \n",
       "1200  0.5344  0.5189    NaN     0.496  NaN  0.5586  NaN     NaN   \n",
       "1300  0.5389  0.5233    NaN     0.504  NaN  0.5557  NaN     NaN   \n",
       "1400  0.5467  0.5311    NaN     0.506  NaN  0.5498  NaN     NaN   \n",
       "1500    0.54  0.5244    NaN       NaN  NaN  0.5495  NaN     NaN   \n",
       "1600  0.5322  0.5156    NaN     0.513  NaN  0.5641  NaN     NaN   \n",
       "1700    0.53  0.5178    NaN       NaN  NaN  0.5555  NaN     NaN   \n",
       "1800  0.5311  0.5167    NaN       NaN  NaN  0.5632  NaN     NaN   \n",
       "1900  0.5378  0.5244    NaN     0.508  NaN   0.562  NaN     NaN   \n",
       "2000    0.54  0.5233    NaN       NaN  NaN  0.5555  NaN     NaN   \n",
       "2100    0.54  0.5233    NaN     0.497  NaN  0.5553  NaN     NaN   \n",
       "2200  0.5556  0.5389    NaN       NaN  NaN  0.5485  NaN     NaN   \n",
       "2300  0.5367  0.5189    NaN       NaN  NaN  0.5627  NaN     NaN   \n",
       "2400  0.5322  0.5156    NaN       NaN  NaN  0.5583  NaN     NaN   \n",
       "2500  0.5344  0.5189    NaN       NaN  NaN  0.5567  NaN     NaN   \n",
       "2600  0.5333  0.5167    NaN      0.51  NaN  0.5517  NaN     NaN   \n",
       "2700  0.5289  0.5122    NaN       NaN  NaN  0.5629  NaN     NaN   \n",
       "2800  0.5267  0.5122    NaN       NaN  NaN  0.5649  NaN     NaN   \n",
       "2900    0.53  0.5144    NaN      0.52  NaN  0.5549  NaN     NaN   \n",
       "1000    0.54  0.5256    NaN       NaN  NaN   0.561  NaN     NaN   \n",
       "3100  0.5322  0.5178    NaN     0.507  NaN  0.5568  NaN     NaN   \n",
       "3200  0.5289  0.5111    NaN       NaN  NaN  0.5583  NaN     NaN   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "100                 NaN          NaN  0.7722  \n",
       "200                 NaN          NaN  0.7733  \n",
       "300                 NaN          NaN  0.7652  \n",
       "400                 NaN          NaN  0.7624  \n",
       "500                 NaN          NaN  0.7699  \n",
       "600                 NaN          NaN  0.7656  \n",
       "700                 NaN          NaN  0.7724  \n",
       "800                 NaN          NaN   0.771  \n",
       "900                 NaN          NaN     NaN  \n",
       "1000                NaN          NaN     NaN  \n",
       "1100                NaN          NaN  0.7696  \n",
       "1200                NaN          NaN  0.7684  \n",
       "1300                NaN          NaN  0.7667  \n",
       "1400                NaN          NaN  0.7678  \n",
       "1500                NaN          NaN  0.7652  \n",
       "1600                NaN          NaN  0.7689  \n",
       "1700                NaN          NaN  0.7658  \n",
       "1800                NaN          NaN  0.7679  \n",
       "1900                NaN          NaN  0.7592  \n",
       "2000                NaN          NaN   0.763  \n",
       "2100                NaN          NaN  0.7607  \n",
       "2200                NaN          NaN  0.7636  \n",
       "2300                NaN          NaN  0.7594  \n",
       "2400                NaN          NaN  0.7559  \n",
       "2500                NaN          NaN  0.7549  \n",
       "2600                NaN          NaN  0.7568  \n",
       "2700                NaN          NaN  0.7537  \n",
       "2800                NaN          NaN  0.7614  \n",
       "2900                NaN          NaN   0.764  \n",
       "1000                NaN          NaN     NaN  \n",
       "3100                NaN          NaN  0.7651  \n",
       "3200                NaN          NaN  0.7622  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores_all(OUTPUT_SFT1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.5389</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.5311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1 docvqa mathvista ai2d chartqa  vqa textvqa  \\\n",
       "2200  0.5556  0.5389    NaN       NaN  NaN  0.5485  NaN     NaN   \n",
       "100     0.55  0.5322    NaN     0.503  NaN  0.6372  NaN     NaN   \n",
       "800     0.55  0.5333    NaN       NaN  NaN  0.5719  NaN     NaN   \n",
       "500   0.5478  0.5311    NaN       NaN  NaN  0.5808  NaN     NaN   \n",
       "600   0.5478    0.53    NaN       NaN  NaN  0.5804  NaN     NaN   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "2200                NaN          NaN  0.7636  \n",
       "100                 NaN          NaN  0.7722  \n",
       "800                 NaN          NaN   0.771  \n",
       "500                 NaN          NaN  0.7699  \n",
       "600                 NaN          NaN  0.7656  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = eval_helper.get_eval_scores_all(OUTPUT_SFT1)\n",
    "df1 = df1.sort_values(by=['mmmu_v2'], ascending=False)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New checkpoints: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000]\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-1000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-1000.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-1100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-1100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-1200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-1200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-1300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-1300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-1400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-1400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-1500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-1500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-1600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-1600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-1700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-1700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-1800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-1800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-1900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-1900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-2000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-2000.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-2100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-2100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-2200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-2200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-2300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-2300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-2400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-2400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-2500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-2500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-2600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-2600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-2700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-2700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-2800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-2800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-2900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-2900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/checkpoint-3000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r17e/evals/eval_jobs_checkpoint-3000.json\n"
     ]
    }
   ],
   "source": [
    "eval_helper.run_eval_sweep(\n",
    "    output_dir=OUTPUT_SFT3,\n",
    "    eval_sbatch=EVAL_SBATCH,\n",
    "    eval_config_dir=EVAL_CONFIG_DIR_EMB65,\n",
    "    aligner_parent_dir=ALIGNER_ADEL_PROD,\n",
    "    # benchmarks=[\"mmmu\", \"chartqa\", \"mmbench\"],\n",
    "    benchmarks=BENCHMARKS,\n",
    "    update_if_exists=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.5522</td>\n",
       "      <td>0.5322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.5522</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.5467</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.5444</td>\n",
       "      <td>0.5244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.5344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.5422</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.5467</td>\n",
       "      <td>0.5278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.5411</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>0.5522</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.5467</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.5422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5715</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>0.5422</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>0.5489</td>\n",
       "      <td>0.5356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5716</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>0.5367</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>0.5378</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>0.5367</td>\n",
       "      <td>0.5244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>0.5367</td>\n",
       "      <td>0.5244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>0.5411</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>0.5344</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>0.5278</td>\n",
       "      <td>0.5156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>0.5344</td>\n",
       "      <td>0.5211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5891</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>0.5178</td>\n",
       "      <td>0.5078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>0.5278</td>\n",
       "      <td>0.5178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>0.5278</td>\n",
       "      <td>0.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5793</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>0.5322</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>0.5322</td>\n",
       "      <td>0.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1 docvqa mathvista ai2d chartqa  vqa textvqa  \\\n",
       "100   0.5522  0.5322    NaN       NaN  NaN  0.6306  NaN     NaN   \n",
       "200   0.5522  0.5333    NaN       NaN  NaN  0.6071  NaN     NaN   \n",
       "300   0.5467  0.5267    NaN     0.524  NaN  0.5938  NaN     NaN   \n",
       "400   0.5444  0.5244    NaN       NaN  NaN  0.5776  NaN     NaN   \n",
       "500   0.5544  0.5344    NaN       NaN  NaN  0.5695  NaN     NaN   \n",
       "600   0.5422  0.5233    NaN     0.535  NaN  0.5775  NaN     NaN   \n",
       "700   0.5467  0.5278    NaN       NaN  NaN  0.5772  NaN     NaN   \n",
       "800   0.5411  0.5267    NaN       NaN  NaN  0.5749  NaN     NaN   \n",
       "900   0.5522  0.5333    NaN     0.532  NaN  0.5712  NaN     NaN   \n",
       "1000  0.5467    0.53    NaN       NaN  NaN  0.5735  NaN     NaN   \n",
       "1100  0.5556  0.5422    NaN       NaN  NaN  0.5715  NaN     NaN   \n",
       "1200  0.5422    0.53    NaN       NaN  NaN   0.574  NaN     NaN   \n",
       "1300  0.5478  0.5333    NaN       NaN  NaN  0.5709  NaN     NaN   \n",
       "1400  0.5489  0.5356    NaN       NaN  NaN  0.5783  NaN     NaN   \n",
       "1500    0.54  0.5267    NaN      0.54  NaN  0.5716  NaN     NaN   \n",
       "1600  0.5367  0.5233    NaN       NaN  NaN  0.5723  NaN     NaN   \n",
       "1700    0.55  0.5378    NaN     0.545  NaN  0.5712  NaN     NaN   \n",
       "1800  0.5378    0.53    NaN      0.54  NaN  0.5799  NaN     NaN   \n",
       "1900  0.5367  0.5244    NaN       NaN  NaN  0.5724  NaN     NaN   \n",
       "2000    0.54  0.5311    NaN       NaN  NaN  0.5801  NaN     NaN   \n",
       "2100  0.5367  0.5244    NaN       NaN  NaN  0.5797  NaN     NaN   \n",
       "2200  0.5411  0.5256    NaN       NaN  NaN  0.5786  NaN     NaN   \n",
       "2300  0.5344  0.5233    NaN       NaN  NaN   0.578  NaN     NaN   \n",
       "2400  0.5278  0.5156    NaN       NaN  NaN  0.5807  NaN     NaN   \n",
       "2500  0.5344  0.5211    NaN       NaN  NaN  0.5891  NaN     NaN   \n",
       "2600  0.5178  0.5078    NaN       NaN  NaN  0.5816  NaN     NaN   \n",
       "2700  0.5278  0.5178    NaN       NaN  NaN  0.5767  NaN     NaN   \n",
       "2800  0.5278    0.52    NaN       NaN  NaN  0.5793  NaN     NaN   \n",
       "2900  0.5322  0.5256    NaN       NaN  NaN  0.5749  NaN     NaN   \n",
       "3000  0.5322    0.52    NaN       NaN  NaN  0.5816  NaN     NaN   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "100                 NaN          NaN  0.7514  \n",
       "200                 NaN          NaN  0.7547  \n",
       "300                 NaN          NaN  0.7629  \n",
       "400                 NaN          NaN  0.7648  \n",
       "500                 NaN          NaN   0.764  \n",
       "600                 NaN          NaN  0.7678  \n",
       "700                 NaN          NaN   0.764  \n",
       "800                 NaN          NaN   0.765  \n",
       "900                 NaN          NaN  0.7668  \n",
       "1000                NaN          NaN  0.7667  \n",
       "1100                NaN          NaN  0.7631  \n",
       "1200                NaN          NaN  0.7631  \n",
       "1300                NaN          NaN  0.7609  \n",
       "1400                NaN          NaN  0.7632  \n",
       "1500                NaN          NaN  0.7667  \n",
       "1600                NaN          NaN  0.7651  \n",
       "1700                NaN          NaN  0.7617  \n",
       "1800                NaN          NaN  0.7615  \n",
       "1900                NaN          NaN  0.7623  \n",
       "2000                NaN          NaN  0.7606  \n",
       "2100                NaN          NaN   0.759  \n",
       "2200                NaN          NaN  0.7628  \n",
       "2300                NaN          NaN   0.759  \n",
       "2400                NaN          NaN   0.762  \n",
       "2500                NaN          NaN    0.76  \n",
       "2600                NaN          NaN  0.7607  \n",
       "2700                NaN          NaN  0.7564  \n",
       "2800                NaN          NaN  0.7581  \n",
       "2900                NaN          NaN  0.7555  \n",
       "3000                NaN          NaN  0.7647  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores_all(OUTPUT_SFT3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.5422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5715</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.5344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.5522</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>0.5522</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.5522</td>\n",
       "      <td>0.5322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1 docvqa mathvista ai2d chartqa  vqa textvqa  \\\n",
       "1100  0.5556  0.5422    NaN       NaN  NaN  0.5715  NaN     NaN   \n",
       "500   0.5544  0.5344    NaN       NaN  NaN  0.5695  NaN     NaN   \n",
       "200   0.5522  0.5333    NaN       NaN  NaN  0.6071  NaN     NaN   \n",
       "900   0.5522  0.5333    NaN       NaN  NaN  0.5712  NaN     NaN   \n",
       "100   0.5522  0.5322    NaN       NaN  NaN  0.6306  NaN     NaN   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "1100                NaN          NaN  0.7631  \n",
       "500                 NaN          NaN   0.764  \n",
       "200                 NaN          NaN  0.7547  \n",
       "900                 NaN          NaN  0.7668  \n",
       "100                 NaN          NaN  0.7514  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = eval_helper.get_eval_scores_all(OUTPUT_SFT3)\n",
    "df3 = df3.sort_values(by=['mmmu_v2'], ascending=False)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New checkpoints: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800]\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-1000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-1000.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-1100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-1100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-1200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-1200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-1300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-1300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-1400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-1400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-1500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-1500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-1600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-1600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-1700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-1700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-1800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-1800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-1900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-1900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-2000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-2000.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-2100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-2100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-2200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-2200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-2300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-2300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-2400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-2400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-2500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-2500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-2600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-2600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-2700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-2700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-2800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-2800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-2900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-2900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-3000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-3000.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-3100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-3100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-3200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-3200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-3300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-3300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-3400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-3400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-3500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-3500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-3600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-3600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-3700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-3700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-3800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-3800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-3900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-3900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-4000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-4000.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-4100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-4100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-4200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-4200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-4300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-4300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-4400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-4400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-4500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-4500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-4600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-4600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-4700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-4700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/checkpoint-4800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r30c/evals/eval_jobs_checkpoint-4800.json\n"
     ]
    }
   ],
   "source": [
    "eval_helper.run_eval_sweep(\n",
    "    output_dir=OUTPUT_SFT4,\n",
    "    eval_sbatch=EVAL_SBATCH,\n",
    "    eval_config_dir=EVAL_CONFIG_DIR_EMB65,\n",
    "    aligner_parent_dir=ALIGNER_ADEL_PROD,\n",
    "    # benchmarks=[\"mmmu\", \"chartqa\", \"mmbench\"],\n",
    "    benchmarks=BENCHMARKS,\n",
    "    update_if_exists=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6274</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.5311</td>\n",
       "      <td>0.5122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.5344</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.5367</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5769</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.5378</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.5411</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.5367</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.5433</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>0.5533</td>\n",
       "      <td>0.5422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.5378</td>\n",
       "      <td>0.5311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>0.5367</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.5156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>0.5344</td>\n",
       "      <td>0.5211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>0.5278</td>\n",
       "      <td>0.5211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5795</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>0.5411</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>0.5244</td>\n",
       "      <td>0.5133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>0.5256</td>\n",
       "      <td>0.5144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>0.5344</td>\n",
       "      <td>0.5211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.5367</td>\n",
       "      <td>0.5278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>0.5244</td>\n",
       "      <td>0.5133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5747</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>0.5322</td>\n",
       "      <td>0.5222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>0.5311</td>\n",
       "      <td>0.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>0.5378</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>0.5289</td>\n",
       "      <td>0.5189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5788</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>0.5211</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>0.5322</td>\n",
       "      <td>0.5222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>0.5356</td>\n",
       "      <td>0.5244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>0.5322</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>0.5378</td>\n",
       "      <td>0.5289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.5222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.533</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>0.5378</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3300</th>\n",
       "      <td>0.5211</td>\n",
       "      <td>0.5144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.5156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.533</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5768</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3500</th>\n",
       "      <td>0.5267</td>\n",
       "      <td>0.5156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>0.5278</td>\n",
       "      <td>0.5156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3700</th>\n",
       "      <td>0.5422</td>\n",
       "      <td>0.5289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3900</th>\n",
       "      <td>0.5422</td>\n",
       "      <td>0.5311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5795</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.5367</td>\n",
       "      <td>0.5289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>0.5344</td>\n",
       "      <td>0.5278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5811</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4300</th>\n",
       "      <td>0.5489</td>\n",
       "      <td>0.5378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>0.5444</td>\n",
       "      <td>0.5378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5802</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500</th>\n",
       "      <td>0.5433</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5788</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>0.5378</td>\n",
       "      <td>0.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>0.5344</td>\n",
       "      <td>0.5222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1 docvqa mathvista ai2d chartqa  vqa textvqa  \\\n",
       "100     0.54  0.5233    NaN       NaN  NaN  0.6274  NaN     NaN   \n",
       "200   0.5311  0.5122    NaN       NaN  NaN  0.5996  NaN     NaN   \n",
       "300   0.5344  0.5167    NaN       NaN  NaN  0.5888  NaN     NaN   \n",
       "400   0.5367  0.5167    NaN       NaN  NaN  0.5769  NaN     NaN   \n",
       "500   0.5378  0.5233    NaN     0.537  NaN  0.5724  NaN     NaN   \n",
       "600   0.5411  0.5233    NaN       NaN  NaN   0.579  NaN     NaN   \n",
       "700   0.5367  0.5256    NaN       NaN  NaN  0.5777  NaN     NaN   \n",
       "800   0.5433  0.5333    NaN     0.538  NaN  0.5859  NaN     NaN   \n",
       "900   0.5533  0.5422    NaN     0.536  NaN  0.5707  NaN     NaN   \n",
       "1000  0.5378  0.5311    NaN       NaN  NaN  0.5742  NaN     NaN   \n",
       "1100    0.55  0.5378    NaN       NaN  NaN  0.5763  NaN     NaN   \n",
       "1200  0.5367  0.5256    NaN     0.536  NaN  0.5726  NaN     NaN   \n",
       "1300    0.53  0.5156    NaN       NaN  NaN  0.5841  NaN     NaN   \n",
       "1400  0.5344  0.5211    NaN       NaN  NaN  0.5766  NaN     NaN   \n",
       "1500  0.5278  0.5211    NaN       NaN  NaN  0.5795  NaN     NaN   \n",
       "1600  0.5411  0.5267    NaN       NaN  NaN  0.5757  NaN     NaN   \n",
       "1700  0.5244  0.5133    NaN     0.535  NaN  0.5801  NaN     NaN   \n",
       "1800  0.5256  0.5144    NaN       NaN  NaN  0.5761  NaN     NaN   \n",
       "1900  0.5344  0.5211    NaN       NaN  NaN  0.5815  NaN     NaN   \n",
       "2000  0.5367  0.5278    NaN       NaN  NaN  0.5784  NaN     NaN   \n",
       "2100  0.5244  0.5133    NaN     0.546  NaN  0.5747  NaN     NaN   \n",
       "2200  0.5322  0.5222    NaN     0.538  NaN   0.579  NaN     NaN   \n",
       "2300  0.5311    0.52    NaN       NaN  NaN  0.5784  NaN     NaN   \n",
       "2400  0.5378  0.5267    NaN       NaN  NaN  0.5763  NaN     NaN   \n",
       "2500  0.5289  0.5189    NaN     0.526  NaN  0.5788  NaN     NaN   \n",
       "2600  0.5211  0.5233    NaN     0.537  NaN  0.5829  NaN     NaN   \n",
       "2700  0.5322  0.5222    NaN       NaN  NaN  0.5781  NaN     NaN   \n",
       "2800  0.5356  0.5244    NaN      0.53  NaN  0.5821  NaN     NaN   \n",
       "2900  0.5322  0.5256    NaN     0.535  NaN  0.5827  NaN     NaN   \n",
       "3000  0.5378  0.5289    NaN       NaN  NaN  0.5832  NaN     NaN   \n",
       "3100  0.5333  0.5222    NaN     0.533  NaN  0.5805  NaN     NaN   \n",
       "3200  0.5378  0.5256    NaN       NaN  NaN  0.5772  NaN     NaN   \n",
       "3300  0.5211  0.5144    NaN       NaN  NaN  0.5772  NaN     NaN   \n",
       "3400    0.53  0.5156    NaN     0.533  NaN  0.5768  NaN     NaN   \n",
       "3500  0.5267  0.5156    NaN       NaN  NaN  0.5725  NaN     NaN   \n",
       "3600  0.5278  0.5156    NaN     0.531  NaN  0.5742  NaN     NaN   \n",
       "3700  0.5422  0.5289    NaN       NaN  NaN   0.576  NaN     NaN   \n",
       "3800    0.53  0.5167    NaN       NaN  NaN  0.5836  NaN     NaN   \n",
       "3900  0.5422  0.5311    NaN       NaN  NaN  0.5795  NaN     NaN   \n",
       "4000  0.5367  0.5289    NaN       NaN  NaN  0.5806  NaN     NaN   \n",
       "4100    0.55  0.5311    NaN       NaN  NaN  0.5863  NaN     NaN   \n",
       "4200  0.5344  0.5278    NaN     0.544  NaN  0.5811  NaN     NaN   \n",
       "4300  0.5489  0.5378    NaN       NaN  NaN  0.5831  NaN     NaN   \n",
       "4400  0.5444  0.5378    NaN       NaN  NaN  0.5802  NaN     NaN   \n",
       "4500  0.5433    0.53    NaN     0.544  NaN  0.5868  NaN     NaN   \n",
       "4600    0.54  0.5322    NaN       NaN  NaN  0.5788  NaN     NaN   \n",
       "4700  0.5378    0.52    NaN     0.543  NaN  0.5808  NaN     NaN   \n",
       "4800  0.5344  0.5222    NaN       NaN  NaN  0.5854  NaN     NaN   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "100                 NaN          NaN  0.7579  \n",
       "200                 NaN          NaN  0.7507  \n",
       "300                 NaN          NaN  0.7463  \n",
       "400                 NaN          NaN  0.7504  \n",
       "500                 NaN          NaN  0.7599  \n",
       "600                 NaN          NaN  0.7636  \n",
       "700                 NaN          NaN  0.7653  \n",
       "800                 NaN          NaN  0.7627  \n",
       "900                 NaN          NaN  0.7648  \n",
       "1000                NaN          NaN  0.7615  \n",
       "1100                NaN          NaN   0.756  \n",
       "1200                NaN          NaN  0.7594  \n",
       "1300                NaN          NaN  0.7571  \n",
       "1400                NaN          NaN  0.7618  \n",
       "1500                NaN          NaN  0.7559  \n",
       "1600                NaN          NaN  0.7615  \n",
       "1700                NaN          NaN  0.7616  \n",
       "1800                NaN          NaN  0.7552  \n",
       "1900                NaN          NaN  0.7569  \n",
       "2000                NaN          NaN  0.7527  \n",
       "2100                NaN          NaN   0.759  \n",
       "2200                NaN          NaN   0.759  \n",
       "2300                NaN          NaN  0.7584  \n",
       "2400                NaN          NaN  0.7615  \n",
       "2500                NaN          NaN  0.7592  \n",
       "2600                NaN          NaN  0.7604  \n",
       "2700                NaN          NaN  0.7545  \n",
       "2800                NaN          NaN  0.7531  \n",
       "2900                NaN          NaN  0.7564  \n",
       "3000                NaN          NaN   0.752  \n",
       "3100                NaN          NaN  0.7509  \n",
       "3200                NaN          NaN  0.7525  \n",
       "3300                NaN          NaN  0.7527  \n",
       "3400                NaN          NaN  0.7568  \n",
       "3500                NaN          NaN  0.7523  \n",
       "3600                NaN          NaN  0.7514  \n",
       "3700                NaN          NaN  0.7586  \n",
       "3800                NaN          NaN  0.7583  \n",
       "3900                NaN          NaN  0.7562  \n",
       "4000                NaN          NaN  0.7644  \n",
       "4100                NaN          NaN  0.7651  \n",
       "4200                NaN          NaN  0.7632  \n",
       "4300                NaN          NaN  0.7615  \n",
       "4400                NaN          NaN  0.7615  \n",
       "4500                NaN          NaN  0.7607  \n",
       "4600                NaN          NaN  0.7622  \n",
       "4700                NaN          NaN  0.7626  \n",
       "4800                NaN          NaN    0.76  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores_all(OUTPUT_SFT4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>0.5533</td>\n",
       "      <td>0.5422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4300</th>\n",
       "      <td>0.5489</td>\n",
       "      <td>0.5378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>0.5444</td>\n",
       "      <td>0.5378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5802</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1 docvqa mathvista ai2d chartqa  vqa textvqa  \\\n",
       "900   0.5533  0.5422    NaN       NaN  NaN  0.5707  NaN     NaN   \n",
       "4100    0.55  0.5311    NaN       NaN  NaN  0.5863  NaN     NaN   \n",
       "1100    0.55  0.5378    NaN       NaN  NaN  0.5763  NaN     NaN   \n",
       "4300  0.5489  0.5378    NaN       NaN  NaN  0.5831  NaN     NaN   \n",
       "4400  0.5444  0.5378    NaN       NaN  NaN  0.5802  NaN     NaN   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "900                 NaN          NaN  0.7648  \n",
       "4100                NaN          NaN  0.7651  \n",
       "1100                NaN          NaN   0.756  \n",
       "4300                NaN          NaN  0.7615  \n",
       "4400                NaN          NaN  0.7615  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = eval_helper.get_eval_scores_all(OUTPUT_SFT4)\n",
    "df4 = df4.sort_values(by=['mmmu_v2'], ascending=False)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New checkpoints: [100, 200, 300, 400, 500, 600, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200]\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-1000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1000.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-1100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-1200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-1300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-1400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-1500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-1600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-1700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-1800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-1900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-2000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2000.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-2100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-2200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-2300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-2400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-2500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-2600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-2700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-2800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-2900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-3000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-3000.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-3100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-3100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/checkpoint-3200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-3200.json\n"
     ]
    }
   ],
   "source": [
    "# eval_helper.run_eval_sweep(\n",
    "#     output_dir=OUTPUT_SFT2,\n",
    "#     eval_sbatch=EVAL_SBATCH,\n",
    "#     eval_config_dir=EVAL_CONFIG_DIR_EMB129,\n",
    "#     aligner_parent_dir=ALIGNER_ADEL_PROD,\n",
    "#     benchmarks=[\"mmmu\", \"chartqa\", \"mmbench\"],\n",
    "#     rerun_if_exists=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5621</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5541</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5626</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1 docvqa mathvista ai2d chartqa  vqa textvqa  \\\n",
       "100      NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "200      NaN     NaN    NaN       NaN  NaN  0.5924  NaN     NaN   \n",
       "300      NaN     NaN    NaN       NaN  NaN  0.5785  NaN     NaN   \n",
       "400      NaN     NaN    NaN       NaN  NaN  0.5866  NaN     NaN   \n",
       "500      NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "600      NaN     NaN    NaN       NaN  NaN   0.575  NaN     NaN   \n",
       "800      NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "900      NaN     NaN    NaN       NaN  NaN  0.5825  NaN     NaN   \n",
       "1000     NaN     NaN    NaN       NaN  NaN  0.5705  NaN     NaN   \n",
       "1100     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "1200     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "1300     NaN     NaN    NaN       NaN  NaN  0.5755  NaN     NaN   \n",
       "1400     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "1500     NaN     NaN    NaN       NaN  NaN  0.5722  NaN     NaN   \n",
       "1600     NaN     NaN    NaN       NaN  NaN   0.574  NaN     NaN   \n",
       "1700     NaN     NaN    NaN       NaN  NaN   0.557  NaN     NaN   \n",
       "1800     NaN     NaN    NaN       NaN  NaN  0.5621  NaN     NaN   \n",
       "1900     NaN     NaN    NaN       NaN  NaN  0.5587  NaN     NaN   \n",
       "2000     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "2100     NaN     NaN    NaN       NaN  NaN  0.5541  NaN     NaN   \n",
       "2200     NaN     NaN    NaN       NaN  NaN  0.5599  NaN     NaN   \n",
       "2300     NaN     NaN    NaN       NaN  NaN  0.5626  NaN     NaN   \n",
       "2400     NaN     NaN    NaN       NaN  NaN  0.5679  NaN     NaN   \n",
       "2500     NaN     NaN    NaN       NaN  NaN  0.5645  NaN     NaN   \n",
       "2600     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "2700     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "2800     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "2900     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "3000     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "3100     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "3200     NaN     NaN    NaN       NaN  NaN  0.5391  NaN     NaN   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "100                 NaN          NaN  0.7028  \n",
       "200                 NaN          NaN   0.718  \n",
       "300                 NaN          NaN    0.73  \n",
       "400                 NaN          NaN   0.741  \n",
       "500                 NaN          NaN  0.7516  \n",
       "600                 NaN          NaN  0.7553  \n",
       "800                 NaN          NaN  0.7528  \n",
       "900                 NaN          NaN  0.7557  \n",
       "1000                NaN          NaN  0.7538  \n",
       "1100                NaN          NaN  0.7661  \n",
       "1200                NaN          NaN  0.7621  \n",
       "1300                NaN          NaN   0.767  \n",
       "1400                NaN          NaN  0.7641  \n",
       "1500                NaN          NaN  0.7618  \n",
       "1600                NaN          NaN   0.765  \n",
       "1700                NaN          NaN  0.7669  \n",
       "1800                NaN          NaN  0.7644  \n",
       "1900                NaN          NaN  0.7644  \n",
       "2000                NaN          NaN  0.7591  \n",
       "2100                NaN          NaN  0.7628  \n",
       "2200                NaN          NaN  0.7602  \n",
       "2300                NaN          NaN  0.7599  \n",
       "2400                NaN          NaN  0.7565  \n",
       "2500                NaN          NaN   0.756  \n",
       "2600                NaN          NaN  0.7584  \n",
       "2700                NaN          NaN  0.7596  \n",
       "2800                NaN          NaN  0.7571  \n",
       "2900                NaN          NaN  0.7574  \n",
       "3000                NaN          NaN  0.7534  \n",
       "3100                NaN          NaN  0.7405  \n",
       "3200                NaN          NaN  0.7472  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores_all(OUTPUT_SFT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New checkpoints: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000]\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-1000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1000.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-1100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-1200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-1300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-1400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-1500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-1600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-1700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-1800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-1900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-1900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-2000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2000.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-2100\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2100.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-2200\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2200.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-2300\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2300.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-2400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2400.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-2500\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2500.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-2600\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2600.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-2700\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2700.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-2800\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2800.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-2900\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-2900.json\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/checkpoint-3000\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_16nodes_latents128/evals/eval_jobs_checkpoint-3000.json\n"
     ]
    }
   ],
   "source": [
    "# eval_helper.run_eval_sweep(\n",
    "#     output_dir=OUTPUT_SFT5,\n",
    "#     eval_sbatch=EVAL_SBATCH,\n",
    "#     eval_config_dir=EVAL_CONFIG_DIR_EMB129,\n",
    "#     aligner_parent_dir=ALIGNER_ADEL_PROD,\n",
    "#     benchmarks=[\"mmmu\", \"chartqa\", \"mmbench\"],\n",
    "#     rerun_if_exists=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5677</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1 docvqa mathvista ai2d chartqa  vqa textvqa  \\\n",
       "100      NaN     NaN    NaN       NaN  NaN  0.5885  NaN     NaN   \n",
       "200      NaN     NaN    NaN       NaN  NaN  0.5889  NaN     NaN   \n",
       "300      NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "400      NaN     NaN    NaN       NaN  NaN  0.5874  NaN     NaN   \n",
       "500      NaN     NaN    NaN       NaN  NaN  0.5835  NaN     NaN   \n",
       "600      NaN     NaN    NaN       NaN  NaN  0.5844  NaN     NaN   \n",
       "700      NaN     NaN    NaN       NaN  NaN  0.5968  NaN     NaN   \n",
       "800      NaN     NaN    NaN       NaN  NaN  0.5924  NaN     NaN   \n",
       "900      NaN     NaN    NaN       NaN  NaN  0.5844  NaN     NaN   \n",
       "1000     NaN     NaN    NaN       NaN  NaN  0.5786  NaN     NaN   \n",
       "1100     NaN     NaN    NaN       NaN  NaN  0.5762  NaN     NaN   \n",
       "1200     NaN     NaN    NaN       NaN  NaN  0.5739  NaN     NaN   \n",
       "1300     NaN     NaN    NaN       NaN  NaN  0.5727  NaN     NaN   \n",
       "1400     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "1500     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "1600     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "1700     NaN     NaN    NaN       NaN  NaN  0.5677  NaN     NaN   \n",
       "1800     NaN     NaN    NaN       NaN  NaN   0.582  NaN     NaN   \n",
       "1900     NaN     NaN    NaN       NaN  NaN  0.5721  NaN     NaN   \n",
       "2000     NaN     NaN    NaN       NaN  NaN  0.5708  NaN     NaN   \n",
       "2100     NaN     NaN    NaN       NaN  NaN  0.5619  NaN     NaN   \n",
       "2200     NaN     NaN    NaN       NaN  NaN  0.5629  NaN     NaN   \n",
       "2300     NaN     NaN    NaN       NaN  NaN  0.5537  NaN     NaN   \n",
       "2400     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "2500     NaN     NaN    NaN       NaN  NaN  0.5647  NaN     NaN   \n",
       "2600     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "2700     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "2800     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "2900     NaN     NaN    NaN       NaN  NaN     NaN  NaN     NaN   \n",
       "3000     NaN     NaN    NaN       NaN  NaN  0.5506  NaN     NaN   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "100                 NaN          NaN  0.7031  \n",
       "200                 NaN          NaN   0.729  \n",
       "300                 NaN          NaN  0.7386  \n",
       "400                 NaN          NaN  0.7352  \n",
       "500                 NaN          NaN  0.7371  \n",
       "600                 NaN          NaN  0.7552  \n",
       "700                 NaN          NaN  0.7486  \n",
       "800                 NaN          NaN  0.7522  \n",
       "900                 NaN          NaN  0.7587  \n",
       "1000                NaN          NaN  0.7501  \n",
       "1100                NaN          NaN  0.7594  \n",
       "1200                NaN          NaN  0.7597  \n",
       "1300                NaN          NaN  0.7635  \n",
       "1400                NaN          NaN  0.7648  \n",
       "1500                NaN          NaN  0.7644  \n",
       "1600                NaN          NaN  0.7657  \n",
       "1700                NaN          NaN  0.7603  \n",
       "1800                NaN          NaN  0.7635  \n",
       "1900                NaN          NaN  0.7617  \n",
       "2000                NaN          NaN    0.76  \n",
       "2100                NaN          NaN  0.7615  \n",
       "2200                NaN          NaN  0.7652  \n",
       "2300                NaN          NaN  0.7566  \n",
       "2400                NaN          NaN  0.7586  \n",
       "2500                NaN          NaN  0.7601  \n",
       "2600                NaN          NaN  0.7621  \n",
       "2700                NaN          NaN  0.7638  \n",
       "2800                NaN          NaN  0.7688  \n",
       "2900                NaN          NaN  0.7741  \n",
       "3000                NaN          NaN  0.7684  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores_all(OUTPUT_SFT5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL FSDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIGNER_ADEL_PROD_FSDP = '/fsx_0/user/tranx/adel_prod_fsdp'\n",
    "EVAL_SBATCH = \"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\"\n",
    "EVAL_CONFIG_DIR_EMB65 = f\"{ALIGNER_ADEL_PROD}/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65\"\n",
    "EVAL_CONFIG_DIR_EMB129 = f\"{ALIGNER_ADEL_PROD}/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb129\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes mmbench 100\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes infographics_w_ocr 100\n",
      "OrderedDict([('mmmu', OrderedDict([('100', 20625)])), ('chartqa', OrderedDict([('100', 20626)])), ('mmbench', {100: 21185}), ('docvqa', OrderedDict([('100', 21184)])), ('infographics_w_ocr', {100: 21186})])\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-100.json\n"
     ]
    }
   ],
   "source": [
    "c = 100\n",
    "eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=EVAL_SBATCH,\n",
    "    aligner_parent_dir=ALIGNER_ADEL_PROD_FSDP,\n",
    "    eval_config_dir=EVAL_CONFIG_DIR_EMB65,\n",
    "    checkpoint_dir=OUTPUT_SFT1,\n",
    "    checkpoints=[c],\n",
    "    save_eval_jobs=eval_helper.get_eval_jobs_record(OUTPUT_SFT1, c),\n",
    "    \n",
    "    # benchmarks=[\"mmmu\"],\n",
    "    # benchmarks=[\"mathvista\"],\n",
    "    # benchmarks=[\"ai2d\"],\n",
    "    # benchmarks=[\"mmbench\", \"infographics_w_ocr\", \"docvqa\"],\n",
    "    benchmarks=[\"mmbench\", \"infographics_w_ocr\"],\n",
    "    # benchmarks=[\"mmbench\", \"infographics_w_ocr\"],\n",
    "    # benchmarks=[\"docvqa\"],\n",
    "    \n",
    "    update_if_exists=True,\n",
    "    print_cmd=True,\n",
    "    print_job_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes docvqa 100\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes ai2d 100\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes textvqa 100\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes infographics_w_ocr 100\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh /fsx_0/user/tranx/adel_prod_fsdp /fsx_0/user/tranx/adel_prod/llm_mm_aligner/experiments/aws_adel/eval_sft_babyLora_emb65/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes infographics 100\n",
      "OrderedDict([('mmmu', OrderedDict([('100', 20625)])), ('chartqa', OrderedDict([('100', 20626)])), ('mmbench', OrderedDict([('100', 21197)])), ('docvqa', {100: 21201}), ('infographics_w_ocr', {100: 21204}), ('mathvista', OrderedDict([('100', 21191)])), ('ai2d', {100: 21202}), ('vqa', OrderedDict([('100', 21193)])), ('textvqa', {100: 21203}), ('infographics', {100: 21205})])\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_5800_BabyLora_16nodes/evals/eval_jobs_checkpoint-100.json\n"
     ]
    }
   ],
   "source": [
    "# redo_benchmarks = [\n",
    "#  'docvqa',\n",
    "#  'mathvista',\n",
    "#  'ai2d',\n",
    "#  'vqa',\n",
    "#  'textvqa',\n",
    "#  'infographics_w_ocr',\n",
    "#  'infographics',\n",
    "#  'mmbench'\n",
    "#  ]\n",
    "\n",
    "redo_benchmarks = [\n",
    " 'docvqa',\n",
    " 'ai2d',\n",
    "#  'vqa',\n",
    " 'textvqa',\n",
    " 'infographics_w_ocr',\n",
    " 'infographics',\n",
    "#  'mmbench'\n",
    " ]\n",
    "\n",
    "c = 100\n",
    "eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=EVAL_SBATCH,\n",
    "    aligner_parent_dir=ALIGNER_ADEL_PROD_FSDP,\n",
    "    eval_config_dir=EVAL_CONFIG_DIR_EMB65,\n",
    "    checkpoint_dir=OUTPUT_SFT1,\n",
    "    checkpoints=[c],\n",
    "    save_eval_jobs=eval_helper.get_eval_jobs_record(OUTPUT_SFT1, c),\n",
    "    benchmarks=redo_benchmarks,\n",
    "    update_if_exists=True,\n",
    "    print_cmd=True,\n",
    "    print_job_dict=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
