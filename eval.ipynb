{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on host:  submit-1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# BOILER PLATE, MUST BE RUN ON SUBMIT NODE\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import socket\n",
    "print(\"Running on host: \", socket.gethostname())\n",
    "\n",
    "import sys \n",
    "lib_path = '/fsx_0/user/tranx/experiments'\n",
    "if lib_path not in sys.path:\n",
    "    sys.path.append(lib_path)\n",
    "#=================================================\n",
    "    \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from lib import eval_helper\n",
    "from lib import utils\n",
    "from lib import slurm\n",
    "from lib.slurm import run_sbatch_job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9675"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"eval_plan\": \"Llama31_336px\",\n",
    "    \"json_config\": \"/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json\",\n",
    "    \"checkpoint_dir\": \"/fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch\",\n",
    "    \"benchmark_name\": \"vqa\",\n",
    "    \"checkpoint_id\": \"2000\"\n",
    "}\n",
    "\n",
    "job = run_job(\n",
    "    sbatch_base_script=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    sbatch_overwrite={\n",
    "        \"job-name\": \"eval_\" + params[\"benchmark_name\"]\n",
    "    },\n",
    "    positional_env_vars=list(params.values())\n",
    ")\n",
    "\n",
    "job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 2000\n",
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 3000\n",
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 4000\n",
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 5000\n",
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 6000\n",
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 7000\n",
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 8000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 2000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 3000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 4000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 5000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 6000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 7000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 8000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 2000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 3000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 4000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 5000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 6000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 7000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 8000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 2000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 3000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 4000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 5000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 6000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 7000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 8000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 2000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 3000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 4000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 5000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 6000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 7000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 8000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 2000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 3000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 4000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 5000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 6000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 7000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 8000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 2000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 3000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 4000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 5000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 6000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 7000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 8000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 2000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 3000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 4000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 5000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 6000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 7000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 8000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 2000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 3000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 4000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 5000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 6000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 7000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 8000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 2000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 3000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 4000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 5000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 6000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 7000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_336px /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 8000\n"
     ]
    }
   ],
   "source": [
    "# Llama31\n",
    "EVAL_PLAN = \"Llama31_336px\"\n",
    "BASE_SCRIPT = \"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\"\n",
    "CONFIG_DIR_LLAMA31 = \"/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31\"\n",
    "CHECKPOINT_DIR_LLAMA31 = \"/fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch\"\n",
    "\n",
    "all_benchmarks = \"mmmu docvqa mathvista ai2d chartqa vqa textvqa infographics_w_ocr infographics mmbench\"\n",
    "checkpoint_start = 2000\n",
    "checkpoint_end = 8000\n",
    "checkpoint_int = 1000\n",
    "\n",
    "job_dict = {}\n",
    "for benchmark in all_benchmarks.split():\n",
    "    job_dict[benchmark] = {}\n",
    "\n",
    "    for chk in range(checkpoint_start, checkpoint_end + checkpoint_int, checkpoint_int):\n",
    "        params = {\n",
    "            \"eval_plan\": EVAL_PLAN,\n",
    "            \"json_config\": f\"{CONFIG_DIR_LLAMA31}/eval_{benchmark}.json\",\n",
    "            \"checkpoint_dir\": CHECKPOINT_DIR_LLAMA31,\n",
    "            \"benchmark_name\": benchmark,\n",
    "            \"checkpoint_id\": str(chk)\n",
    "        }\n",
    "\n",
    "        assert os.path.exists(params[\"json_config\"])\n",
    "        assert os.path.exists(f\"{params['checkpoint_dir']}/checkpoint-{chk}\")\n",
    "\n",
    "        job_id = run_sbatch_job(\n",
    "            sbatch_base_script=BASE_SCRIPT,\n",
    "            sbatch_overwrite={\n",
    "                \"job-name\": f\"eval_{benchmark}\"\n",
    "            },\n",
    "            positional_env_vars=list(params.values())\n",
    "        )\n",
    "\n",
    "        job_dict[benchmark][chk] = int(job_id)\n",
    "\n",
    "with open(f'job_dict_{EVAL_PLAN}.json', 'w') as f:\n",
    "    json.dump(job_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m checkpoints \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m8500\u001b[39m, \u001b[38;5;241m9000\u001b[39m, \u001b[38;5;241m9500\u001b[39m]\n\u001b[1;32m      5\u001b[0m job_dict_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_dict_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEVAL_PLAN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(job_dict_json)\n\u001b[1;32m      8\u001b[0m BASE_SCRIPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m CONFIG_DIR_LLAMA31 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Llama31\n",
    "EVAL_PLAN = \"Llama31_336px_8500_9500\"\n",
    "checkpoints = [8500, 9000, 9500]\n",
    "\n",
    "job_dict_json = f'job_dict_{EVAL_PLAN}.json'\n",
    "assert not os.path.exists(job_dict_json)\n",
    "\n",
    "BASE_SCRIPT = \"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\"\n",
    "CONFIG_DIR_LLAMA31 = \"/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31\"\n",
    "CHECKPOINT_DIR_LLAMA31 = \"/fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch\"\n",
    "\n",
    "all_benchmarks = \"mmmu docvqa mathvista ai2d chartqa vqa textvqa infographics_w_ocr infographics mmbench\"\n",
    "\n",
    "job_dict = {}\n",
    "for benchmark in all_benchmarks.split():\n",
    "    job_dict[benchmark] = {}\n",
    "\n",
    "    for chk in checkpoints:\n",
    "        params = {\n",
    "            \"eval_plan\": EVAL_PLAN,\n",
    "            \"json_config\": f\"{CONFIG_DIR_LLAMA31}/eval_{benchmark}.json\",\n",
    "            \"checkpoint_dir\": CHECKPOINT_DIR_LLAMA31,\n",
    "            \"benchmark_name\": benchmark,\n",
    "            \"checkpoint_id\": str(chk)\n",
    "        }\n",
    "\n",
    "        assert os.path.exists(params[\"json_config\"])\n",
    "        assert os.path.exists(f\"{params['checkpoint_dir']}/checkpoint-{chk}\")\n",
    "\n",
    "        job_id = run_job(\n",
    "            sbatch_base_script=BASE_SCRIPT,\n",
    "            sbatch_overwrite={\n",
    "                \"job-name\": f\"eval_{benchmark}\"\n",
    "            },\n",
    "            positional_env_vars=list(params.values())\n",
    "        )\n",
    "\n",
    "        job_dict[benchmark][chk] = int(job_id)\n",
    "\n",
    "with open(job_dict_json, 'w') as f:\n",
    "    json.dump(job_dict, f, indent=4)\n",
    "\n",
    "print(f\"Saved job_dict to {job_dict_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mmmu': {2000: 9759,\n",
       "  3000: 9760,\n",
       "  4000: 9761,\n",
       "  5000: 9762,\n",
       "  6000: 9763,\n",
       "  7000: 9764,\n",
       "  8000: 9765},\n",
       " 'docvqa': {2000: 9766,\n",
       "  3000: 9767,\n",
       "  4000: 9768,\n",
       "  5000: 9769,\n",
       "  6000: 9770,\n",
       "  7000: 9771,\n",
       "  8000: 9772},\n",
       " 'mathvista': {2000: 9773,\n",
       "  3000: 9774,\n",
       "  4000: 9775,\n",
       "  5000: 9776,\n",
       "  6000: 9777,\n",
       "  7000: 9778,\n",
       "  8000: 9779},\n",
       " 'ai2d': {2000: 9780,\n",
       "  3000: 9781,\n",
       "  4000: 9782,\n",
       "  5000: 9783,\n",
       "  6000: 9784,\n",
       "  7000: 9785,\n",
       "  8000: 9786},\n",
       " 'chartqa': {2000: 9787,\n",
       "  3000: 9788,\n",
       "  4000: 9789,\n",
       "  5000: 9790,\n",
       "  6000: 9791,\n",
       "  7000: 9792,\n",
       "  8000: 9793},\n",
       " 'vqa': {2000: 9794,\n",
       "  3000: 9795,\n",
       "  4000: 9796,\n",
       "  5000: 9797,\n",
       "  6000: 9798,\n",
       "  7000: 9799,\n",
       "  8000: 9800},\n",
       " 'textvqa': {2000: 9801,\n",
       "  3000: 9802,\n",
       "  4000: 9803,\n",
       "  5000: 9804,\n",
       "  6000: 9805,\n",
       "  7000: 9806,\n",
       "  8000: 9807},\n",
       " 'infographics_w_ocr': {2000: 9808,\n",
       "  3000: 9809,\n",
       "  4000: 9810,\n",
       "  5000: 9811,\n",
       "  6000: 9812,\n",
       "  7000: 9813,\n",
       "  8000: 9814},\n",
       " 'infographics': {2000: 9815,\n",
       "  3000: 9816,\n",
       "  4000: 9817,\n",
       "  5000: 9818,\n",
       "  6000: 9819,\n",
       "  7000: 9820,\n",
       "  8000: 9821},\n",
       " 'mmbench': {2000: 9822,\n",
       "  3000: 9823,\n",
       "  4000: 9824,\n",
       "  5000: 9825,\n",
       "  6000: 9826,\n",
       "  7000: 9827,\n",
       "  8000: 9828}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama31 - 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmmu 10000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch docvqa 10000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mathvista 10000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch ai2d 10000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch chartqa 10000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch vqa 10000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch textvqa 10000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics_w_ocr 10000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch infographics 10000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh Llama31_10k /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch mmbench 10000\n",
      "job_dict saved to job_dict_Llama31_10k.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mmmu': {10000: 10008},\n",
       " 'docvqa': {10000: 10009},\n",
       " 'mathvista': {10000: 10010},\n",
       " 'ai2d': {10000: 10011},\n",
       " 'chartqa': {10000: 10012},\n",
       " 'vqa': {10000: 10013},\n",
       " 'textvqa': {10000: 10014},\n",
       " 'infographics_w_ocr': {10000: 10015},\n",
       " 'infographics': {10000: 10016},\n",
       " 'mmbench': {10000: 10017}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_dict_lm31_10k = eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    eval_plan=\"Llama31_10k\",\n",
    "    eval_config_dir=\"/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31\",\n",
    "    checkpoint_dir=\"/fsx_0/checkpoints/tranx/MM9-Pretrain-70B/Llama31_336px_128nodes_bz32_scratch\",\n",
    "    checkpoints=[10000],\n",
    "    # benchmarks=[\"mathvista\"]\n",
    ")\n",
    "\n",
    "job_dict_lm31_10k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama31 - fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/fb_llama3.1 mmmu 8000\n",
      "sbatch --parsable --job-name=eval_mmmu /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmmu.json /fsx_0/checkpoints/tranx/fb_llama3.1 mmmu 10000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 docvqa 8000\n",
      "sbatch --parsable --job-name=eval_docvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_docvqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 docvqa 10000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/fb_llama3.1 mathvista 8000\n",
      "sbatch --parsable --job-name=eval_mathvista /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mathvista.json /fsx_0/checkpoints/tranx/fb_llama3.1 mathvista 10000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/fb_llama3.1 ai2d 8000\n",
      "sbatch --parsable --job-name=eval_ai2d /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_ai2d.json /fsx_0/checkpoints/tranx/fb_llama3.1 ai2d 10000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 chartqa 8000\n",
      "sbatch --parsable --job-name=eval_chartqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_chartqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 chartqa 10000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 vqa 8000\n",
      "sbatch --parsable --job-name=eval_vqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_vqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 vqa 10000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 textvqa 8000\n",
      "sbatch --parsable --job-name=eval_textvqa /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_textvqa.json /fsx_0/checkpoints/tranx/fb_llama3.1 textvqa 10000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/fb_llama3.1 infographics_w_ocr 8000\n",
      "sbatch --parsable --job-name=eval_infographics_w_ocr /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics_w_ocr.json /fsx_0/checkpoints/tranx/fb_llama3.1 infographics_w_ocr 10000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/fb_llama3.1 infographics 8000\n",
      "sbatch --parsable --job-name=eval_infographics /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_infographics.json /fsx_0/checkpoints/tranx/fb_llama3.1 infographics 10000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/fb_llama3.1 mmbench 8000\n",
      "sbatch --parsable --job-name=eval_mmbench /fsx_0/user/tranx/experiments/eval/sbash_eval.sh FBL_Llama31 /fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31/eval_mmbench.json /fsx_0/checkpoints/tranx/fb_llama3.1 mmbench 10000\n",
      "job_dict saved to job_dict_FBL_Llama31.json\n"
     ]
    }
   ],
   "source": [
    "job_dict_fb_lm31 = eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    eval_plan=\"FBL_Llama31\",\n",
    "    eval_config_dir=\"/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval_31\",\n",
    "    checkpoint_dir=\"/fsx_0/checkpoints/tranx/fb_llama3.1\",\n",
    "    checkpoints=[8000, 10000],\n",
    "    # benchmarks=[\"mathvista\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MH19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10500, 11500, 12500, 13500, 14500, 15500, 16500, 17500, 18500]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoints = [10500 + i*1000 for i in range(9)]\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found existing job_dict: eval/logs/job_dict_MH19_10500_18500.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m job_dict_mh19 \u001b[38;5;241m=\u001b[39m \u001b[43meval_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_eval_plan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_base_sbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_plan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMH19_10500_18500\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_config_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/fsx_0/checkpoints/tranx/MM9-Pretrain-70B/MH19_336px_128nodes_bz32_resume\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10500\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# benchmarks=[\"mathvista\"]\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-06bc3d6b93146dddd/user/tranx/experiments/eval_helper.py:41\u001b[0m, in \u001b[0;36mrun_eval_plan\u001b[0;34m(eval_base_sbatch, eval_plan, eval_config_dir, checkpoint_dir, checkpoints, benchmarks, rerun_if_exists)\u001b[0m\n\u001b[1;32m     39\u001b[0m job_dict_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval/logs/job_dict_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_plan\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(job_dict_json) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rerun_if_exists:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound existing job_dict: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_dict_json\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m benchmarks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     benchmarks \u001b[38;5;241m=\u001b[39m ALL_BENCHMARKS\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found existing job_dict: eval/logs/job_dict_MH19_10500_18500.json"
     ]
    }
   ],
   "source": [
    "job_dict_mh19 = eval_helper.run_eval_plan(\n",
    "    eval_base_sbatch=\"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\",\n",
    "    eval_plan=\"MH19_10500_18500\",\n",
    "    eval_config_dir=\"/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval\",\n",
    "    checkpoint_dir=\"/fsx_0/checkpoints/tranx/MM9-Pretrain-70B/MH19_336px_128nodes_bz32_resume\",\n",
    "    checkpoints=[10500 + i*1000 for i in range(9)],\n",
    "    # benchmarks=[\"mathvista\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got result for mmmu - 10500\n",
      "Got result for mmmu - 11500\n",
      "Got result for mmmu - 12500\n",
      "Got result for mmmu - 13500\n",
      "Got result for mmmu - 14500\n",
      "Got result for mmmu - 15500\n",
      "Got result for mmmu - 16500\n",
      "Got result for mmmu - 17500\n",
      "Got result for mmmu - 18500\n",
      "Got result for docvqa - 10500\n",
      "Got result for docvqa - 11500\n",
      "Got result for docvqa - 12500\n",
      "Got result for docvqa - 13500\n",
      "Got result for docvqa - 14500\n",
      "Got result for docvqa - 15500\n",
      "Got result for docvqa - 16500\n",
      "Got result for docvqa - 17500\n",
      "Got result for docvqa - 18500\n",
      "Got result for mathvista - 10500\n",
      "Got result for mathvista - 11500\n",
      "Got result for mathvista - 12500\n",
      "Got result for mathvista - 13500\n",
      "Got result for mathvista - 14500\n",
      "Got result for mathvista - 15500\n",
      "Got result for mathvista - 16500\n",
      "Got result for mathvista - 17500\n",
      "Got result for mathvista - 18500\n",
      "Got result for ai2d - 10500\n",
      "Got result for ai2d - 11500\n",
      "Got result for ai2d - 12500\n",
      "Got result for ai2d - 13500\n",
      "Got result for ai2d - 14500\n",
      "Got result for ai2d - 15500\n",
      "Got result for ai2d - 16500\n",
      "Got result for ai2d - 17500\n",
      "Got result for ai2d - 18500\n",
      "Got result for chartqa - 10500\n",
      "Got result for chartqa - 11500\n",
      "Got result for chartqa - 12500\n",
      "Got result for chartqa - 13500\n",
      "Got result for chartqa - 14500\n",
      "Got result for chartqa - 15500\n",
      "Got result for chartqa - 16500\n",
      "Got result for chartqa - 17500\n",
      "Got result for chartqa - 18500\n",
      "Got result for vqa - 10500\n",
      "Got result for vqa - 11500\n",
      "Got result for vqa - 12500\n",
      "Got result for vqa - 13500\n",
      "Got result for vqa - 14500\n",
      "Got result for vqa - 15500\n",
      "Got result for vqa - 16500\n",
      "Got result for vqa - 17500\n",
      "Got result for vqa - 18500\n",
      "Got result for textvqa - 10500\n",
      "Got result for textvqa - 11500\n",
      "Got result for textvqa - 12500\n",
      "Got result for textvqa - 13500\n",
      "Got result for textvqa - 14500\n",
      "Got result for textvqa - 15500\n",
      "Got result for textvqa - 16500\n",
      "Got result for textvqa - 17500\n",
      "Got result for textvqa - 18500\n",
      "Got result for infographics_w_ocr - 10500\n",
      "Got result for infographics_w_ocr - 11500\n",
      "Got result for infographics_w_ocr - 12500\n",
      "Got result for infographics_w_ocr - 13500\n",
      "Got result for infographics_w_ocr - 14500\n",
      "Got result for infographics_w_ocr - 15500\n",
      "Got result for infographics_w_ocr - 16500\n",
      "Got result for infographics_w_ocr - 17500\n",
      "Got result for infographics_w_ocr - 18500\n",
      "Got result for infographics - 10500\n",
      "Got result for infographics - 11500\n",
      "Got result for infographics - 12500\n",
      "Got result for infographics - 13500\n",
      "Got result for infographics - 14500\n",
      "Got result for infographics - 15500\n",
      "Got result for infographics - 16500\n",
      "Got result for infographics - 17500\n",
      "Got result for infographics - 18500\n",
      "Got result for mmbench - 10500\n",
      "Got result for mmbench - 11500\n",
      "Got result for mmbench - 12500\n",
      "Got result for mmbench - 13500\n",
      "Got result for mmbench - 14500\n",
      "Got result for mmbench - 15500\n",
      "Got result for mmbench - 16500\n",
      "Got result for mmbench - 17500\n",
      "Got result for mmbench - 18500\n"
     ]
    }
   ],
   "source": [
    "df = eval_helper.get_eval_scores(job_dict_mh19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mmmu/accuracy                                    0.5878\n",
       "mmmu/mllm_eval_accuracy                          0.5889\n",
       "docvqa/anls_total_score                          0.6201\n",
       "docvqa/mllm_evaluation_anls_score                0.6216\n",
       "mathvista/accuracy                                0.377\n",
       "ai2d/accuracy                                    0.7704\n",
       "chartqa/accuracy                                 0.4914\n",
       "vqa/accuracy                                     0.7035\n",
       "vqa/mllm_evaluation_accuracy                     0.7256\n",
       "textvqa/accuracy                                 66.898\n",
       "textvqa/mllm_eval_accuracy                       71.578\n",
       "infographics_w_ocr/anls_total_score              0.6256\n",
       "infographics_w_ocr/mllm_evaluation_anls_score    0.5712\n",
       "infographics/anls_total_score                    0.4798\n",
       "infographics/mllm_evaluation_anls_score          0.4177\n",
       "mmbench/overall                                  0.7422\n",
       "Name: 17500, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[17500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m job_dict_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_dict_Llama31_336px_8500_9500.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(job_dict_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m     job_dict \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m eval_helper\u001b[38;5;241m.\u001b[39mget_eval_scores(job_dict)\n\u001b[1;32m      8\u001b[0m df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# job_dict_file = 'job_dict_Llama31_336px.json'\n",
    "job_dict_file = 'job_dict_Llama31_336px_8500_9500.json'\n",
    "\n",
    "with open(job_dict_file, 'r') as f:\n",
    "    job_dict = json.load(f)\n",
    "\n",
    "df = eval_helper.get_eval_scores(job_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got result for mmmu - 8000\n",
      "Got result for mmmu - 10000\n",
      "Got result for docvqa - 8000\n",
      "Got result for docvqa - 10000\n",
      "Got result for mathvista - 8000\n",
      "Got result for mathvista - 10000\n",
      "Got result for ai2d - 8000\n",
      "Got result for ai2d - 10000\n",
      "Got result for chartqa - 8000\n",
      "Got result for chartqa - 10000\n",
      "Got result for vqa - 8000\n",
      "Got result for vqa - 10000\n",
      "Got result for textvqa - 8000\n",
      "Got result for textvqa - 10000\n",
      "Got result for infographics_w_ocr - 8000\n",
      "Got result for infographics_w_ocr - 10000\n",
      "Got result for infographics - 8000\n",
      "Got result for infographics - 10000\n",
      "Got result for mmbench - 8000\n",
      "Got result for mmbench - 10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu/accuracy</th>\n",
       "      <th>mmmu/mllm_eval_accuracy</th>\n",
       "      <th>docvqa/anls_total_score</th>\n",
       "      <th>docvqa/mllm_evaluation_anls_score</th>\n",
       "      <th>mathvista/accuracy</th>\n",
       "      <th>ai2d/accuracy</th>\n",
       "      <th>chartqa/accuracy</th>\n",
       "      <th>vqa/accuracy</th>\n",
       "      <th>vqa/mllm_evaluation_accuracy</th>\n",
       "      <th>textvqa/accuracy</th>\n",
       "      <th>textvqa/mllm_eval_accuracy</th>\n",
       "      <th>infographics_w_ocr/anls_total_score</th>\n",
       "      <th>infographics_w_ocr/mllm_evaluation_anls_score</th>\n",
       "      <th>infographics/anls_total_score</th>\n",
       "      <th>infographics/mllm_evaluation_anls_score</th>\n",
       "      <th>mmbench/overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>0.5067</td>\n",
       "      <td>0.5156</td>\n",
       "      <td>0.5386</td>\n",
       "      <td>0.5411</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.7121</td>\n",
       "      <td>0.4746</td>\n",
       "      <td>0.6831</td>\n",
       "      <td>0.7137</td>\n",
       "      <td>59.028</td>\n",
       "      <td>65.38</td>\n",
       "      <td>0.5902</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.4384</td>\n",
       "      <td>0.3746</td>\n",
       "      <td>0.6635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.4744</td>\n",
       "      <td>0.4822</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5432</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4578</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.7224</td>\n",
       "      <td>59.414</td>\n",
       "      <td>65.708</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.5354</td>\n",
       "      <td>0.4472</td>\n",
       "      <td>0.3785</td>\n",
       "      <td>0.6772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mmmu/accuracy mmmu/mllm_eval_accuracy docvqa/anls_total_score  \\\n",
       "8000         0.5067                  0.5156                  0.5386   \n",
       "10000        0.4744                  0.4822                    0.54   \n",
       "\n",
       "      docvqa/mllm_evaluation_anls_score mathvista/accuracy ai2d/accuracy  \\\n",
       "8000                             0.5411              0.344        0.7121   \n",
       "10000                            0.5432              0.348        0.6953   \n",
       "\n",
       "      chartqa/accuracy vqa/accuracy vqa/mllm_evaluation_accuracy  \\\n",
       "8000            0.4746       0.6831                       0.7137   \n",
       "10000           0.4578       0.6931                       0.7224   \n",
       "\n",
       "      textvqa/accuracy textvqa/mllm_eval_accuracy  \\\n",
       "8000            59.028                      65.38   \n",
       "10000           59.414                     65.708   \n",
       "\n",
       "      infographics_w_ocr/anls_total_score  \\\n",
       "8000                               0.5902   \n",
       "10000                               0.592   \n",
       "\n",
       "      infographics_w_ocr/mllm_evaluation_anls_score  \\\n",
       "8000                                          0.536   \n",
       "10000                                        0.5354   \n",
       "\n",
       "      infographics/anls_total_score infographics/mllm_evaluation_anls_score  \\\n",
       "8000                         0.4384                                  0.3746   \n",
       "10000                        0.4472                                  0.3785   \n",
       "\n",
       "      mmbench/overall  \n",
       "8000           0.6635  \n",
       "10000          0.6772  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_dict_file = 'job_dict_FBL_Llama31.json'\n",
    "with open(job_dict_file, 'r') as f:\n",
    "    job_dict = json.load(f)\n",
    "\n",
    "df2 = eval_helper.get_eval_scores(job_dict)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_SBATCH = \"/fsx_0/user/tranx/experiments/eval/sbash_eval.sh\"\n",
    "ALIGNER_CODE_DIR=\"/fsx_0/user/tranx/eval_adel\"\n",
    "# EVAL_CONFIG_DIR=f\"/fsx_0/user/tranx/eval/llm_mm_aligner/experiments/aws_adel/eval\"\n",
    "EVAL_CONFIG_DIR=f\"/fsx_0/user/tranx/eval_adel/llm_mm_aligner/experiments/aws_adel/eval\"\n",
    "# STAGE2_OUTPUT_DIR=\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New checkpoints: []\n"
     ]
    }
   ],
   "source": [
    "eval_helper.run_eval_sweep(\n",
    "    output_dir=f\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp\",\n",
    "    eval_sbatch=EVAL_SBATCH,\n",
    "    eval_config_dir=EVAL_CONFIG_DIR,\n",
    "    aligner_parent_dir=ALIGNER_CODE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2600, 2800, 3000, 3200, 3400, 3600, 3800, 4000, 4200, 4400, 4600, 4800]\n",
      "Got result for mmmu - 200: [('mmmu/accounting', 0.4), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.23333333333333334), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.9333333333333333), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.26666666666666666), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.43333333333333335), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.3333333333333333), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.5333333333333333), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5444444444444444), ('mmmu/mllm_eval_accuracy', 0.5555555555555556)]\n",
      "Got result for docvqa - 200: [('docvqa/anls_total_score', 0.648974586635455), ('docvqa/mllm_evaluation_anls_score', 0.6481197547746367), ('docvqa/mmllm_fixed_anls_score', 0.6838029270024617)]\n",
      "Got result for mathvista - 200: [('mathvista/accuracy', 0.391)]\n",
      "Got result for ai2d - 200: [('ai2d/accuracy', 0.7937176165803109)]\n",
      "Got result for chartqa - 200: [('chartqa/accuracy', 0.49371597896623964)]\n",
      "Got result for vqa - 200: [('vqa/accuracy', 0.7255479999999764), ('vqa/recall', 0.7492599999999743), ('vqa/bleu', 0.020937373861670494), ('vqa/mllm_evaluation_accuracy', 0.7450439999999751)]\n",
      "Got result for textvqa - 200: [('textvqa/accuracy', 68.12600000000033), ('textvqa/mllm_eval_accuracy', 73.26600000000035)]\n",
      "Got result for infographics_w_ocr - 200: [('infographics_w_ocr/anls_total_score', 0.6409916230210374), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5854567964169172), ('infographics_w_ocr/answer_type_multi_span_score', 0.5338974316374145), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6330233359166454), ('infographics_w_ocr/answer_type_question_span_score', 0.7330070539063752), ('infographics_w_ocr/answer_type_single_span_score', 0.6500951211141338), ('infographics_w_ocr/evidence_type_figure_score', 0.6175575898411356), ('infographics_w_ocr/evidence_type_map_score', 0.5759932724041634), ('infographics_w_ocr/evidence_type_table_list_score', 0.6430477870563691), ('infographics_w_ocr/evidence_type_text_score', 0.6880197894970036), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5443827499957463), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6244129158512719), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5369375807695697), ('infographics_w_ocr/reasoning_type_counting_score', 0.6284965034965034)]\n",
      "Got result for infographics - 200: [('infographics/anls_total_score', 0.5379493460605843), ('infographics/mllm_evaluation_anls_score', 0.47600775196257794), ('infographics/answer_type_multi_span_score', 0.36748425796360557), ('infographics/answer_type_non_extractive_score', 0.5428836383628429), ('infographics/answer_type_question_span_score', 0.7130050505050506), ('infographics/answer_type_single_span_score', 0.5466798770418363), ('infographics/evidence_type_figure_score', 0.5290105259537133), ('infographics/evidence_type_map_score', 0.4711856346711625), ('infographics/evidence_type_table_list_score', 0.4989027652359333), ('infographics/evidence_type_text_score', 0.5788382727884572), ('infographics/evidence_type_visual_layout_score', 0.4924818133769208), ('infographics/reasoning_type_arithmetic_score', 0.49103191329218726), ('infographics/reasoning_type_comparison_score', 0.446195245471771), ('infographics/reasoning_type_counting_score', 0.5816433566433565)]\n",
      "Got result for mmbench - 200: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.575), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.9166666666666666), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.4444444444444444), ('mmbench/ocr', 0.8461538461538461), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.25), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7380514196021993)]\n",
      "200\n",
      "Got result for mmmu - 400: [('mmmu/accounting', 0.6), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.43333333333333335), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.6), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5655555555555556), ('mmmu/mllm_eval_accuracy', 0.57)]\n",
      "Got result for docvqa - 400: [('docvqa/anls_total_score', 0.6534120102355975), ('docvqa/mllm_evaluation_anls_score', 0.6513522658103867), ('docvqa/mmllm_fixed_anls_score', 0.6844828999693582)]\n",
      "Got result for mathvista - 400: [('mathvista/accuracy', 0.399)]\n",
      "Got result for ai2d - 400: [('ai2d/accuracy', 0.7895077720207254)]\n",
      "Got result for chartqa - 400: [('chartqa/accuracy', 0.49791647438025527)]\n",
      "Got result for vqa - 400: [('vqa/accuracy', 0.7258479999999763), ('vqa/recall', 0.7517559999999741), ('vqa/bleu', 0.0), ('vqa/mllm_evaluation_accuracy', 0.7470839999999748)]\n",
      "Got result for textvqa - 400: [('textvqa/accuracy', 68.66400000000034), ('textvqa/mllm_eval_accuracy', 73.45800000000037)]\n",
      "Got result for infographics_w_ocr - 400: [('infographics_w_ocr/anls_total_score', 0.6333954609425488), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5798452893172775), ('infographics_w_ocr/answer_type_multi_span_score', 0.505847540654381), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6253293722552313), ('infographics_w_ocr/answer_type_question_span_score', 0.7112121821115034), ('infographics_w_ocr/answer_type_single_span_score', 0.6460722512370143), ('infographics_w_ocr/evidence_type_figure_score', 0.6117414086165958), ('infographics_w_ocr/evidence_type_map_score', 0.5532590759075908), ('infographics_w_ocr/evidence_type_table_list_score', 0.6334879754404233), ('infographics_w_ocr/evidence_type_text_score', 0.6731628556221999), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5600575002516679), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.609271037181996), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5240481182359088), ('infographics_w_ocr/reasoning_type_counting_score', 0.6273310023310023)]\n",
      "Got result for infographics - 400: [('infographics/anls_total_score', 0.5391218184507696), ('infographics/mllm_evaluation_anls_score', 0.47887276038929505), ('infographics/answer_type_multi_span_score', 0.38638430186891565), ('infographics/answer_type_non_extractive_score', 0.5447543866621627), ('infographics/answer_type_question_span_score', 0.6992306304806305), ('infographics/answer_type_single_span_score', 0.546695121686073), ('infographics/evidence_type_figure_score', 0.5284556051924668), ('infographics/evidence_type_map_score', 0.4789636273660133), ('infographics/evidence_type_table_list_score', 0.5067757725482563), ('infographics/evidence_type_text_score', 0.5869814975451305), ('infographics/evidence_type_visual_layout_score', 0.4864893326090637), ('infographics/reasoning_type_arithmetic_score', 0.5110702368407846), ('infographics/reasoning_type_comparison_score', 0.4765089285749754), ('infographics/reasoning_type_counting_score', 0.5652680652680652)]\n",
      "Got result for mmbench - 400: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7352971577832469)]\n",
      "400\n",
      "Got result for mmmu - 600: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.5333333333333333), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.6333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.5666666666666667), ('mmmu/geography', 0.5), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.43333333333333335), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.4), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.5333333333333333), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5433333333333333), ('mmmu/mllm_eval_accuracy', 0.5566666666666666)]\n",
      "Got result for docvqa - 600: [('docvqa/anls_total_score', 0.6575888089569821), ('docvqa/mllm_evaluation_anls_score', 0.657276969036617), ('docvqa/mmllm_fixed_anls_score', 0.6929927830043519)]\n",
      "Got result for mathvista - 600: [('mathvista/accuracy', 0.379)]\n",
      "Got result for ai2d - 600: [('ai2d/accuracy', 0.7946891191709845)]\n",
      "Got result for chartqa - 600: [('chartqa/accuracy', 0.49772606244006784)]\n",
      "Got result for vqa - 600: [('vqa/accuracy', 0.7261719999999774), ('vqa/recall', 0.7539559999999752), ('vqa/bleu', 0.03368200361728668), ('vqa/mllm_evaluation_accuracy', 0.7489759999999759)]\n",
      "Got result for textvqa - 600: [('textvqa/accuracy', 68.3380000000003), ('textvqa/mllm_eval_accuracy', 72.94600000000037)]\n",
      "Got result for infographics_w_ocr - 600: [('infographics_w_ocr/anls_total_score', 0.645120098458749), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5922845273478874), ('infographics_w_ocr/answer_type_multi_span_score', 0.5371055480168933), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6376241148845129), ('infographics_w_ocr/answer_type_question_span_score', 0.7228597257443411), ('infographics_w_ocr/answer_type_single_span_score', 0.655476301762224), ('infographics_w_ocr/evidence_type_figure_score', 0.6327841762111491), ('infographics_w_ocr/evidence_type_map_score', 0.5816038334602691), ('infographics_w_ocr/evidence_type_table_list_score', 0.6337104729359286), ('infographics_w_ocr/evidence_type_text_score', 0.6790192802343173), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5583750370514137), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6263908751066284), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5450205332919241), ('infographics_w_ocr/reasoning_type_counting_score', 0.631118881118881)]\n",
      "Got result for infographics - 600: [('infographics/anls_total_score', 0.5387890745367289), ('infographics/mllm_evaluation_anls_score', 0.47306837557220427), ('infographics/answer_type_multi_span_score', 0.40358057113388285), ('infographics/answer_type_non_extractive_score', 0.542125535706006), ('infographics/answer_type_question_span_score', 0.6984407953157952), ('infographics/answer_type_single_span_score', 0.5456866014396378), ('infographics/evidence_type_figure_score', 0.5318784259066155), ('infographics/evidence_type_map_score', 0.48573335274703944), ('infographics/evidence_type_table_list_score', 0.5046150246320811), ('infographics/evidence_type_text_score', 0.5690155534867277), ('infographics/evidence_type_visual_layout_score', 0.47527225655468797), ('infographics/reasoning_type_arithmetic_score', 0.4932491595162826), ('infographics/reasoning_type_comparison_score', 0.4520703552380929), ('infographics/reasoning_type_counting_score', 0.5745920745920745)]\n",
      "Got result for mmbench - 600: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5121951219512195), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7369986371617516)]\n",
      "600\n",
      "Got result for mmmu - 800: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.7333333333333333), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.4), ('mmmu/geography', 0.43333333333333335), ('mmmu/history', 0.7), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.5333333333333333), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5622222222222222), ('mmmu/mllm_eval_accuracy', 0.5755555555555556)]\n",
      "Got result for docvqa - 800: [('docvqa/anls_total_score', 0.6861651868027577), ('docvqa/mllm_evaluation_anls_score', 0.6849665255082817), ('docvqa/mmllm_fixed_anls_score', 0.720501167940428)]\n",
      "Got result for mathvista - 800: [('mathvista/accuracy', 0.435)]\n",
      "Got result for ai2d - 800: [('ai2d/accuracy', 0.8257772020725389)]\n",
      "Got result for chartqa - 800: [('chartqa/accuracy', 0.5386423595754996)]\n",
      "Got result for vqa - 800: [('vqa/accuracy', 0.7527439999999767), ('vqa/recall', 0.7740319999999759), ('vqa/bleu', 0.04545086994767189), ('vqa/mllm_evaluation_accuracy', 0.7709879999999767)]\n",
      "Got result for textvqa - 800: [('textvqa/accuracy', 70.53000000000036), ('textvqa/mllm_eval_accuracy', 74.64600000000038)]\n",
      "Got result for infographics_w_ocr - 800: [('infographics_w_ocr/anls_total_score', 0.6472951114150778), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5973177554748575), ('infographics_w_ocr/answer_type_multi_span_score', 0.503323993986201), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6466639398375387), ('infographics_w_ocr/answer_type_question_span_score', 0.7170577254570468), ('infographics_w_ocr/answer_type_single_span_score', 0.6575469899729094), ('infographics_w_ocr/evidence_type_figure_score', 0.6270001994179981), ('infographics_w_ocr/evidence_type_map_score', 0.608501523229246), ('infographics_w_ocr/evidence_type_table_list_score', 0.6534260886396096), ('infographics_w_ocr/evidence_type_text_score', 0.692856945110573), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5523783679965781), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.652300771906936), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5411386472815425), ('infographics_w_ocr/reasoning_type_counting_score', 0.6284965034965035)]\n",
      "Got result for infographics - 800: [('infographics/anls_total_score', 0.5584713034221711), ('infographics/mllm_evaluation_anls_score', 0.4991246766993003), ('infographics/answer_type_multi_span_score', 0.3821692950630614), ('infographics/answer_type_non_extractive_score', 0.5402885139774832), ('infographics/answer_type_question_span_score', 0.694036465671081), ('infographics/answer_type_single_span_score', 0.5761809310517206), ('infographics/evidence_type_figure_score', 0.5486501418344766), ('infographics/evidence_type_map_score', 0.5589467317772501), ('infographics/evidence_type_table_list_score', 0.524121350581692), ('infographics/evidence_type_text_score', 0.5997578810480703), ('infographics/evidence_type_visual_layout_score', 0.5009458084050376), ('infographics/reasoning_type_arithmetic_score', 0.4844048455349825), ('infographics/reasoning_type_comparison_score', 0.4813541596300644), ('infographics/reasoning_type_counting_score', 0.5786713286713286)]\n",
      "Got result for mmbench - 800: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.9166666666666666), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6753246753246753), ('mmbench/overall', 0.750688446377466)]\n",
      "800\n",
      "Got result for mmmu - 1000: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5333333333333333), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.6), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.4666666666666667), ('mmmu/energy_and_power', 0.36666666666666664), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.43333333333333335), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.6333333333333333), ('mmmu/materials', 0.4), ('mmmu/math', 0.6), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5511111111111111), ('mmmu/mllm_eval_accuracy', 0.5666666666666667)]\n",
      "Got result for docvqa - 1000: [('docvqa/anls_total_score', 0.6914809452898901), ('docvqa/mllm_evaluation_anls_score', 0.6910337685765978), ('docvqa/mmllm_fixed_anls_score', 0.7266098512822747)]\n",
      "Got result for mathvista - 1000: [('mathvista/accuracy', 0.441)]\n",
      "Got result for ai2d - 1000: [('ai2d/accuracy', 0.8228626943005182)]\n",
      "Got result for chartqa - 1000: [('chartqa/accuracy', 0.5446625354709164)]\n",
      "Got result for vqa - 1000: [('vqa/accuracy', 0.7517119999999763), ('vqa/recall', 0.7744439999999746), ('vqa/bleu', 0.03847002610564232), ('vqa/mllm_evaluation_accuracy', 0.7715879999999756)]\n",
      "Got result for textvqa - 1000: [('textvqa/accuracy', 70.04600000000036), ('textvqa/mllm_eval_accuracy', 74.15000000000038)]\n",
      "Got result for infographics_w_ocr - 1000: [('infographics_w_ocr/anls_total_score', 0.6547973502072989), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6048048545226773), ('infographics_w_ocr/answer_type_multi_span_score', 0.503602583214263), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6551580125721177), ('infographics_w_ocr/answer_type_question_span_score', 0.7178612519958674), ('infographics_w_ocr/answer_type_single_span_score', 0.6665106562611337), ('infographics_w_ocr/evidence_type_figure_score', 0.6354380330971015), ('infographics_w_ocr/evidence_type_map_score', 0.5891945925361767), ('infographics_w_ocr/evidence_type_table_list_score', 0.6538581681693623), ('infographics_w_ocr/evidence_type_text_score', 0.7021856039991057), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5823333066147026), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.65662915851272), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5496088289389683), ('infographics_w_ocr/reasoning_type_counting_score', 0.6491841491841491)]\n",
      "Got result for infographics - 1000: [('infographics/anls_total_score', 0.5557274631323967), ('infographics/mllm_evaluation_anls_score', 0.48987016150522056), ('infographics/answer_type_multi_span_score', 0.35456009292459895), ('infographics/answer_type_non_extractive_score', 0.5457338923885037), ('infographics/answer_type_question_span_score', 0.6946469662815817), ('infographics/answer_type_single_span_score', 0.5732867615153577), ('infographics/evidence_type_figure_score', 0.5433856843943352), ('infographics/evidence_type_map_score', 0.5117018932850822), ('infographics/evidence_type_table_list_score', 0.5286447006472307), ('infographics/evidence_type_text_score', 0.5924922026101045), ('infographics/evidence_type_visual_layout_score', 0.5158001118642461), ('infographics/reasoning_type_arithmetic_score', 0.49825630990014547), ('infographics/reasoning_type_comparison_score', 0.47883552954855163), ('infographics/reasoning_type_counting_score', 0.5812937062937062)]\n",
      "Got result for mmbench - 1000: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.5185185185185185), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7557467484518534)]\n",
      "1000\n",
      "Got result for mmmu - 1200: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.5), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.5666666666666667), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.6333333333333333), ('mmmu/marketing', 0.4), ('mmmu/materials', 0.26666666666666666), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5777777777777777), ('mmmu/mllm_eval_accuracy', 0.5833333333333334)]\n",
      "Got result for docvqa - 1200: [('docvqa/anls_total_score', 0.688159004463097), ('docvqa/mllm_evaluation_anls_score', 0.687887849937589), ('docvqa/mmllm_fixed_anls_score', 0.7248432274205141)]\n",
      "Got result for mathvista - 1200: [('mathvista/accuracy', 0.443)]\n",
      "Got result for ai2d - 1200: [('ai2d/accuracy', 0.8251295336787565)]\n",
      "Got result for chartqa - 1200: [('chartqa/accuracy', 0.5338041688877015)]\n",
      "Got result for vqa - 1200: [('vqa/accuracy', 0.7530399999999744), ('vqa/recall', 0.7722879999999732), ('vqa/bleu', 0.042081840336322784), ('vqa/mllm_evaluation_accuracy', 0.7701319999999744)]\n",
      "Got result for textvqa - 1200: [('textvqa/accuracy', 69.95600000000036), ('textvqa/mllm_eval_accuracy', 74.09000000000042)]\n",
      "Got result for infographics_w_ocr - 1200: [('infographics_w_ocr/anls_total_score', 0.6508986976368093), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5996206615245506), ('infographics_w_ocr/answer_type_multi_span_score', 0.5430357056045166), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6591065053090372), ('infographics_w_ocr/answer_type_question_span_score', 0.7079765288758502), ('infographics_w_ocr/answer_type_single_span_score', 0.6569890842073937), ('infographics_w_ocr/evidence_type_figure_score', 0.6324564858914928), ('infographics_w_ocr/evidence_type_map_score', 0.6080064737242955), ('infographics_w_ocr/evidence_type_table_list_score', 0.646020786828472), ('infographics_w_ocr/evidence_type_text_score', 0.6924020368555358), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5810923917569475), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6589699683877763), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5452584973158334), ('infographics_w_ocr/reasoning_type_counting_score', 0.6386946386946387)]\n",
      "Got result for infographics - 1200: [('infographics/anls_total_score', 0.5538923675620057), ('infographics/mllm_evaluation_anls_score', 0.49112106346003614), ('infographics/answer_type_multi_span_score', 0.39007474218426597), ('infographics/answer_type_non_extractive_score', 0.5509604620816195), ('infographics/answer_type_question_span_score', 0.7153276744622898), ('infographics/answer_type_single_span_score', 0.5647906983169795), ('infographics/evidence_type_figure_score', 0.54006393378161), ('infographics/evidence_type_map_score', 0.49796591220988223), ('infographics/evidence_type_table_list_score', 0.5145635853743483), ('infographics/evidence_type_text_score', 0.6015344858227378), ('infographics/evidence_type_visual_layout_score', 0.5387917275282756), ('infographics/reasoning_type_arithmetic_score', 0.5091819709970394), ('infographics/reasoning_type_comparison_score', 0.4477963166018653), ('infographics/reasoning_type_counting_score', 0.5775058275058275)]\n",
      "Got result for mmbench - 1200: [('mmbench/attribute_comparison', 0.6590909090909091), ('mmbench/attribute_recognition', 0.9014084507042254), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.7494843283432574)]\n",
      "1200\n",
      "Got result for mmmu - 1400: [('mmmu/accounting', 0.43333333333333335), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.7333333333333333), ('mmmu/basic_medical_science', 0.5666666666666667), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.3333333333333333), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.4666666666666667), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.4666666666666667), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.5666666666666667), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5577777777777778), ('mmmu/mllm_eval_accuracy', 0.5633333333333334)]\n",
      "Got result for docvqa - 1400: [('docvqa/anls_total_score', 0.6576122486567785), ('docvqa/mllm_evaluation_anls_score', 0.6566612892873913), ('docvqa/mmllm_fixed_anls_score', 0.6934407806760226)]\n",
      "Got result for mathvista - 1400: [('mathvista/accuracy', 0.39)]\n",
      "Got result for ai2d - 1400: [('ai2d/accuracy', 0.8014896373056994)]\n",
      "Got result for chartqa - 1400: [('chartqa/accuracy', 0.5053164478963773)]\n",
      "Got result for vqa - 1400: [('vqa/accuracy', 0.7249079999999761), ('vqa/recall', 0.7543519999999746), ('vqa/bleu', 0.030893689021468163), ('vqa/mllm_evaluation_accuracy', 0.7496639999999758)]\n",
      "Got result for textvqa - 1400: [('textvqa/accuracy', 68.46800000000027), ('textvqa/mllm_eval_accuracy', 73.00600000000034)]\n",
      "Got result for infographics_w_ocr - 1400: [('infographics_w_ocr/anls_total_score', 0.6369640423630668), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5829732966226111), ('infographics_w_ocr/answer_type_multi_span_score', 0.532606442211179), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6343472832170846), ('infographics_w_ocr/answer_type_question_span_score', 0.7184717526063681), ('infographics_w_ocr/answer_type_single_span_score', 0.6437379416417038), ('infographics_w_ocr/evidence_type_figure_score', 0.6145073401890329), ('infographics_w_ocr/evidence_type_map_score', 0.5446401370906321), ('infographics_w_ocr/evidence_type_table_list_score', 0.6425165028947857), ('infographics_w_ocr/evidence_type_text_score', 0.6775024510267813), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5382672591430826), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6234955968688843), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5446248559956286), ('infographics_w_ocr/reasoning_type_counting_score', 0.6243006993006992)]\n",
      "Got result for infographics - 1400: [('infographics/anls_total_score', 0.544698776916589), ('infographics/mllm_evaluation_anls_score', 0.4768804440485371), ('infographics/answer_type_multi_span_score', 0.3876663178392642), ('infographics/answer_type_non_extractive_score', 0.537036411448708), ('infographics/answer_type_question_span_score', 0.6925609113109114), ('infographics/answer_type_single_span_score', 0.5579228911168002), ('infographics/evidence_type_figure_score', 0.5337600881331508), ('infographics/evidence_type_map_score', 0.46992766503541117), ('infographics/evidence_type_table_list_score', 0.5152377475690411), ('infographics/evidence_type_text_score', 0.5866106648369201), ('infographics/evidence_type_visual_layout_score', 0.5032870488300436), ('infographics/reasoning_type_arithmetic_score', 0.49987831802900295), ('infographics/reasoning_type_comparison_score', 0.4659806705776409), ('infographics/reasoning_type_counting_score', 0.562062937062937)]\n",
      "Got result for mmbench - 1400: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.4567901234567901), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7389200432584532)]\n",
      "1400\n",
      "Got result for mmmu - 1600: [('mmmu/accounting', 0.5666666666666667), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.7333333333333333), ('mmmu/electronics', 0.26666666666666666), ('mmmu/energy_and_power', 0.6333333333333333), ('mmmu/finance', 0.5), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.43333333333333335), ('mmmu/materials', 0.4), ('mmmu/math', 0.6666666666666666), ('mmmu/mechanical_engineering', 0.3), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5677777777777778), ('mmmu/mllm_eval_accuracy', 0.5811111111111111)]\n",
      "Got result for docvqa - 1600: [('docvqa/anls_total_score', 0.6633950645141165), ('docvqa/mllm_evaluation_anls_score', 0.6621732866379921), ('docvqa/mmllm_fixed_anls_score', 0.7018963353076103)]\n",
      "Got result for mathvista - 1600: [('mathvista/accuracy', 0.387)]\n",
      "Got result for ai2d - 1600: [('ai2d/accuracy', 0.8018134715025906)]\n",
      "Got result for chartqa - 1600: [('chartqa/accuracy', 0.501776176087941)]\n",
      "Got result for vqa - 1600: [('vqa/accuracy', 0.7266159999999767), ('vqa/recall', 0.7553039999999749), ('vqa/bleu', 0.022225230932235718), ('vqa/mllm_evaluation_accuracy', 0.7506439999999759)]\n",
      "Got result for textvqa - 1600: [('textvqa/accuracy', 68.8460000000003), ('textvqa/mllm_eval_accuracy', 73.40600000000033)]\n",
      "Got result for infographics_w_ocr - 1600: [('infographics_w_ocr/anls_total_score', 0.6457023973979295), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5932751674743095), ('infographics_w_ocr/answer_type_multi_span_score', 0.5497372595047608), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6206169809696033), ('infographics_w_ocr/answer_type_question_span_score', 0.7205321921668075), ('infographics_w_ocr/answer_type_single_span_score', 0.660572165359513), ('infographics_w_ocr/evidence_type_figure_score', 0.6278178162645915), ('infographics_w_ocr/evidence_type_map_score', 0.594640137090632), ('infographics_w_ocr/evidence_type_table_list_score', 0.642145557871301), ('infographics_w_ocr/evidence_type_text_score', 0.6875644335225235), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5671063387468899), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6081090998043052), ('infographics_w_ocr/reasoning_type_comparison_score', 0.552772060569731), ('infographics_w_ocr/reasoning_type_counting_score', 0.6180069930069929)]\n",
      "Got result for infographics - 1600: [('infographics/anls_total_score', 0.5495695703528087), ('infographics/mllm_evaluation_anls_score', 0.484123202352065), ('infographics/answer_type_multi_span_score', 0.4011947975827871), ('infographics/answer_type_non_extractive_score', 0.5340847017158049), ('infographics/answer_type_question_span_score', 0.681049452684068), ('infographics/answer_type_single_span_score', 0.5635314091849846), ('infographics/evidence_type_figure_score', 0.5365480551777553), ('infographics/evidence_type_map_score', 0.5282933046250156), ('infographics/evidence_type_table_list_score', 0.5134098914630114), ('infographics/evidence_type_text_score', 0.5994608136374032), ('infographics/evidence_type_visual_layout_score', 0.5114665083623571), ('infographics/reasoning_type_arithmetic_score', 0.48055158312007623), ('infographics/reasoning_type_comparison_score', 0.4764306825263266), ('infographics/reasoning_type_counting_score', 0.5786713286713286)]\n",
      "Got result for mmbench - 1600: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8354430379746836), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8490566037735849), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6753246753246753), ('mmbench/overall', 0.7391074399707203)]\n",
      "1600\n",
      "Got result for mmmu - 1800: [('mmmu/accounting', 0.5333333333333333), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.4666666666666667), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.6333333333333333), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.6), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.4), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.8), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.57), ('mmmu/mllm_eval_accuracy', 0.5822222222222222)]\n",
      "Got result for docvqa - 1800: [('docvqa/anls_total_score', 0.6843588119133742), ('docvqa/mllm_evaluation_anls_score', 0.684458344039418), ('docvqa/mmllm_fixed_anls_score', 0.7224850768908115)]\n",
      "Got result for mathvista - 1800: [('mathvista/accuracy', 0.479)]\n",
      "Got result for ai2d - 1800: [('ai2d/accuracy', 0.8173575129533679)]\n",
      "Got result for vqa - 1800: [('vqa/accuracy', 0.7503119999999752), ('vqa/recall', 0.7713479999999748), ('vqa/bleu', 0.037375714629888535), ('vqa/mllm_evaluation_accuracy', 0.7684439999999758)]\n",
      "Got result for textvqa - 1800: [('textvqa/accuracy', 70.31600000000034), ('textvqa/mllm_eval_accuracy', 74.32400000000037)]\n",
      "Got result for infographics_w_ocr - 1800: [('infographics_w_ocr/anls_total_score', 0.6526072837441547), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5975676709033039), ('infographics_w_ocr/answer_type_multi_span_score', 0.5427162487344895), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6393067450933637), ('infographics_w_ocr/answer_type_question_span_score', 0.714310472709794), ('infographics_w_ocr/answer_type_single_span_score', 0.664294212037485), ('infographics_w_ocr/evidence_type_figure_score', 0.634466514054076), ('infographics_w_ocr/evidence_type_map_score', 0.6252231953964628), ('infographics_w_ocr/evidence_type_table_list_score', 0.6469956922890664), ('infographics_w_ocr/evidence_type_text_score', 0.7085295462550012), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5564597474059932), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6558332078880021), ('infographics_w_ocr/reasoning_type_comparison_score', 0.542371694441681), ('infographics_w_ocr/reasoning_type_counting_score', 0.6127622377622378)]\n",
      "Got result for infographics - 1800: [('infographics/anls_total_score', 0.5585906774640167), ('infographics/mllm_evaluation_anls_score', 0.4935091271938477), ('infographics/answer_type_multi_span_score', 0.3778451767180989), ('infographics/answer_type_non_extractive_score', 0.5463894353586939), ('infographics/answer_type_question_span_score', 0.7421515450361604), ('infographics/answer_type_single_span_score', 0.5719845867047859), ('infographics/evidence_type_figure_score', 0.545387545278541), ('infographics/evidence_type_map_score', 0.5535764732659879), ('infographics/evidence_type_table_list_score', 0.5195161862322201), ('infographics/evidence_type_text_score', 0.5951663128244008), ('infographics/evidence_type_visual_layout_score', 0.5096500748563842), ('infographics/reasoning_type_arithmetic_score', 0.5097717731279374), ('infographics/reasoning_type_comparison_score', 0.48293097685132386), ('infographics/reasoning_type_counting_score', 0.5772144522144521)]\n",
      "Got result for mmbench - 1800: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.5), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6103896103896104), ('mmbench/overall', 0.7516339808169767)]\n",
      "1800\n",
      "Got result for mmmu - 2000: [('mmmu/accounting', 0.4), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.3333333333333333), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.6333333333333333), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.4), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.7333333333333333), ('mmmu/public_health', 0.7), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.5433333333333333), ('mmmu/mllm_eval_accuracy', 0.5522222222222222)]\n",
      "Got result for docvqa - 2000: [('docvqa/anls_total_score', 0.6627813263594035), ('docvqa/mllm_evaluation_anls_score', 0.6624657918881379), ('docvqa/mmllm_fixed_anls_score', 0.7000551475450792)]\n",
      "Got result for mathvista - 2000: [('mathvista/accuracy', 0.378)]\n",
      "Got result for ai2d - 2000: [('ai2d/accuracy', 0.8021373056994818)]\n",
      "Got result for chartqa - 2000: [('chartqa/accuracy', 0.5066379600544395)]\n",
      "Got result for vqa - 2000: [('vqa/accuracy', 0.7267079999999778), ('vqa/recall', 0.755859999999976), ('vqa/bleu', 0.028023675084114075), ('vqa/mllm_evaluation_accuracy', 0.7509839999999772)]\n",
      "Got result for textvqa - 2000: [('textvqa/accuracy', 68.4740000000003), ('textvqa/mllm_eval_accuracy', 72.94000000000037)]\n",
      "Got result for infographics_w_ocr - 2000: [('infographics_w_ocr/anls_total_score', 0.6420000592264764), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5881253633181263), ('infographics_w_ocr/answer_type_multi_span_score', 0.538551872230699), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6317661241711875), ('infographics_w_ocr/answer_type_question_span_score', 0.6880230346576501), ('infographics_w_ocr/answer_type_single_span_score', 0.6557812322531061), ('infographics_w_ocr/evidence_type_figure_score', 0.6238483673800576), ('infographics_w_ocr/evidence_type_map_score', 0.6055312261995429), ('infographics_w_ocr/evidence_type_table_list_score', 0.6355094520789897), ('infographics_w_ocr/evidence_type_text_score', 0.6921671264690452), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.545479832830823), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6246575342465752), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5360197634885258), ('infographics_w_ocr/reasoning_type_counting_score', 0.6349067599067598)]\n",
      "Got result for infographics - 2000: [('infographics/anls_total_score', 0.5559116590884859), ('infographics/mllm_evaluation_anls_score', 0.4922647155064431), ('infographics/answer_type_multi_span_score', 0.40190802622300553), ('infographics/answer_type_non_extractive_score', 0.5323262833208585), ('infographics/answer_type_question_span_score', 0.7382339882339882), ('infographics/answer_type_single_span_score', 0.5707760379116904), ('infographics/evidence_type_figure_score', 0.5461863530518095), ('infographics/evidence_type_map_score', 0.4773830324208892), ('infographics/evidence_type_table_list_score', 0.5306038912986368), ('infographics/evidence_type_text_score', 0.5889429538375155), ('infographics/evidence_type_visual_layout_score', 0.5069370005363462), ('infographics/reasoning_type_arithmetic_score', 0.5054558265174702), ('infographics/reasoning_type_comparison_score', 0.47156348737507503), ('infographics/reasoning_type_counting_score', 0.5577505827505826)]\n",
      "Got result for mmbench - 2000: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7360026953942295)]\n",
      "2000\n",
      "Got result for mmmu - 2200: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.6333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.3333333333333333), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.9), ('mmmu/manage', 0.6), ('mmmu/marketing', 0.43333333333333335), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7333333333333333), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5655555555555556), ('mmmu/mllm_eval_accuracy', 0.5744444444444444)]\n",
      "Got result for docvqa - 2200: [('docvqa/anls_total_score', 0.6694143120287874), ('docvqa/mllm_evaluation_anls_score', 0.6698830748144646), ('docvqa/mmllm_fixed_anls_score', 0.7089631949147517)]\n",
      "Got result for mathvista - 2200: [('mathvista/accuracy', 0.394)]\n",
      "Got result for ai2d - 2200: [('ai2d/accuracy', 0.7943652849740933)]\n",
      "Got result for chartqa - 2200: [('chartqa/accuracy', 0.5129263856993226)]\n",
      "Got result for vqa - 2200: [('vqa/accuracy', 0.7266359999999765), ('vqa/recall', 0.7551799999999745), ('vqa/bleu', 0.03049882873892784), ('vqa/mllm_evaluation_accuracy', 0.7500119999999759)]\n",
      "Got result for textvqa - 2200: [('textvqa/accuracy', 68.84600000000026), ('textvqa/mllm_eval_accuracy', 73.47200000000032)]\n",
      "Got result for infographics_w_ocr - 2200: [('infographics_w_ocr/anls_total_score', 0.6432614946945413), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5898985995137185), ('infographics_w_ocr/answer_type_multi_span_score', 0.5609760024596901), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6271207996343621), ('infographics_w_ocr/answer_type_question_span_score', 0.7017118480998673), ('infographics_w_ocr/answer_type_single_span_score', 0.6541986604117707), ('infographics_w_ocr/evidence_type_figure_score', 0.6196211386623298), ('infographics_w_ocr/evidence_type_map_score', 0.604873586906202), ('infographics_w_ocr/evidence_type_table_list_score', 0.6486229501939967), ('infographics_w_ocr/evidence_type_text_score', 0.6843432069801682), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5501802028066695), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6098098248783178), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5325424540774624), ('infographics_w_ocr/reasoning_type_counting_score', 0.6267482517482517)]\n",
      "Got result for infographics - 2200: [('infographics/anls_total_score', 0.5439055755378869), ('infographics/mllm_evaluation_anls_score', 0.4804403199543547), ('infographics/answer_type_multi_span_score', 0.40435471419347996), ('infographics/answer_type_non_extractive_score', 0.5377988196252211), ('infographics/answer_type_question_span_score', 0.7116529304029304), ('infographics/answer_type_single_span_score', 0.5534635948693101), ('infographics/evidence_type_figure_score', 0.5323806197783365), ('infographics/evidence_type_map_score', 0.5120273875579316), ('infographics/evidence_type_table_list_score', 0.5094437698041304), ('infographics/evidence_type_text_score', 0.5876053942462758), ('infographics/evidence_type_visual_layout_score', 0.4987959308190131), ('infographics/reasoning_type_arithmetic_score', 0.4924751618244768), ('infographics/reasoning_type_comparison_score', 0.47829183989204593), ('infographics/reasoning_type_counting_score', 0.5775641025641025)]\n",
      "Got result for mmbench - 2200: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8227848101265823), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9423076923076923), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.4583333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.7421294421341338)]\n",
      "2200\n",
      "Got result for mmmu - 2400: [('mmmu/accounting', 0.5666666666666667), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.6), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.7333333333333333), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.5666666666666667), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.4), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.7666666666666667), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.57), ('mmmu/mllm_eval_accuracy', 0.5755555555555556)]\n",
      "Got result for docvqa - 2400: [('docvqa/anls_total_score', 0.6676134044250891), ('docvqa/mllm_evaluation_anls_score', 0.6670305691017008), ('docvqa/mmllm_fixed_anls_score', 0.7057330541748089)]\n",
      "Got result for mathvista - 2400: [('mathvista/accuracy', 0.398)]\n",
      "Got result for ai2d - 2400: [('ai2d/accuracy', 0.7979274611398963)]\n",
      "Got result for chartqa - 2400: [('chartqa/accuracy', 0.5218296375019621)]\n",
      "Got result for vqa - 2400: [('vqa/accuracy', 0.7270279999999776), ('vqa/recall', 0.7573719999999755), ('vqa/bleu', 0.02922874130308628), ('vqa/mllm_evaluation_accuracy', 0.751963999999977)]\n",
      "Got result for textvqa - 2400: [('textvqa/accuracy', 68.84400000000032), ('textvqa/mllm_eval_accuracy', 73.39000000000034)]\n",
      "Got result for infographics_w_ocr - 2400: [('infographics_w_ocr/anls_total_score', 0.6400734706551193), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.585863259324377), ('infographics_w_ocr/answer_type_multi_span_score', 0.5417920903366263), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6338675489670067), ('infographics_w_ocr/answer_type_question_span_score', 0.7080475785158653), ('infographics_w_ocr/answer_type_single_span_score', 0.6491220125524187), ('infographics_w_ocr/evidence_type_figure_score', 0.6199140081783565), ('infographics_w_ocr/evidence_type_map_score', 0.5909161160398784), ('infographics_w_ocr/evidence_type_table_list_score', 0.6407164745338039), ('infographics_w_ocr/evidence_type_text_score', 0.6857923617404651), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5831775879892245), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6181350042651409), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5459358815316889), ('infographics_w_ocr/reasoning_type_counting_score', 0.636072261072261)]\n",
      "Got result for infographics - 2400: [('infographics/anls_total_score', 0.544973853529598), ('infographics/mllm_evaluation_anls_score', 0.4838577817721142), ('infographics/answer_type_multi_span_score', 0.38867847013850343), ('infographics/answer_type_non_extractive_score', 0.5411728014802157), ('infographics/answer_type_question_span_score', 0.7359868763714917), ('infographics/answer_type_single_span_score', 0.553158868574983), ('infographics/evidence_type_figure_score', 0.5362464378933626), ('infographics/evidence_type_map_score', 0.5250805894542943), ('infographics/evidence_type_table_list_score', 0.513031272764928), ('infographics/evidence_type_text_score', 0.5837412142641485), ('infographics/evidence_type_visual_layout_score', 0.5033652960913895), ('infographics/reasoning_type_arithmetic_score', 0.484367211935705), ('infographics/reasoning_type_comparison_score', 0.46061815255837635), ('infographics/reasoning_type_counting_score', 0.5921328671328671)]\n",
      "Got result for mmbench - 2400: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.4444444444444444), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7414569084930678)]\n",
      "2400\n",
      "Got result for mmmu - 2600: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.7), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.5333333333333333), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.5333333333333333), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.3), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.4666666666666667), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.6), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.7666666666666667), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.7333333333333333), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.56), ('mmmu/mllm_eval_accuracy', 0.5755555555555556)]\n",
      "Got result for docvqa - 2600: [('docvqa/anls_total_score', 0.6672832913320215), ('docvqa/mllm_evaluation_anls_score', 0.6673006278030491), ('docvqa/mmllm_fixed_anls_score', 0.7044682330535644)]\n",
      "Got result for mathvista - 2600: [('mathvista/accuracy', 0.399)]\n",
      "Got result for ai2d - 2600: [('ai2d/accuracy', 0.8031088082901554)]\n",
      "Got result for chartqa - 2600: [('chartqa/accuracy', 0.5075724195093866)]\n",
      "Got result for vqa - 2600: [('vqa/accuracy', 0.7276079999999777), ('vqa/recall', 0.7578119999999756), ('vqa/bleu', 0.025044534355401993), ('vqa/mllm_evaluation_accuracy', 0.752243999999977)]\n",
      "Got result for textvqa - 2600: [('textvqa/accuracy', 69.08600000000031), ('textvqa/mllm_eval_accuracy', 73.62400000000038)]\n",
      "Got result for infographics_w_ocr - 2600: [('infographics_w_ocr/anls_total_score', 0.6391358859067168), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.58720405521593), ('infographics_w_ocr/answer_type_multi_span_score', 0.5183272630795799), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6373589942306038), ('infographics_w_ocr/answer_type_question_span_score', 0.6850774930457799), ('infographics_w_ocr/answer_type_single_span_score', 0.650204492794494), ('infographics_w_ocr/evidence_type_figure_score', 0.618197567547635), ('infographics_w_ocr/evidence_type_map_score', 0.567577430820005), ('infographics_w_ocr/evidence_type_table_list_score', 0.634900000476396), ('infographics_w_ocr/evidence_type_text_score', 0.6902275560916012), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5841190209004171), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6293134377038485), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5314698440141242), ('infographics_w_ocr/reasoning_type_counting_score', 0.629079254079254)]\n",
      "Got result for infographics - 2600: [('infographics/anls_total_score', 0.5483576725059027), ('infographics/mllm_evaluation_anls_score', 0.4845595042568303), ('infographics/answer_type_multi_span_score', 0.38210103376855065), ('infographics/answer_type_non_extractive_score', 0.545565977121131), ('infographics/answer_type_question_span_score', 0.6999177371773525), ('infographics/answer_type_single_span_score', 0.5598667452952026), ('infographics/evidence_type_figure_score', 0.5339724209563811), ('infographics/evidence_type_map_score', 0.5298752669384585), ('infographics/evidence_type_table_list_score', 0.513928773891564), ('infographics/evidence_type_text_score', 0.5853495720663191), ('infographics/evidence_type_visual_layout_score', 0.49361744114562295), ('infographics/reasoning_type_arithmetic_score', 0.511979401876662), ('infographics/reasoning_type_comparison_score', 0.46496719694322264), ('infographics/reasoning_type_counting_score', 0.5664918414918414)]\n",
      "Got result for mmbench - 2600: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.575), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4444444444444444), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6883116883116883), ('mmbench/overall', 0.7422790001338214)]\n",
      "2600\n",
      "Got result for mmmu - 2800: [('mmmu/accounting', 0.6), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.23333333333333334), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.5), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.6333333333333333), ('mmmu/finance', 0.4), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.6), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.2), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.6), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5544444444444444), ('mmmu/mllm_eval_accuracy', 0.5588888888888889)]\n",
      "Got result for docvqa - 2800: [('docvqa/anls_total_score', 0.66395975782975), ('docvqa/mllm_evaluation_anls_score', 0.6646201677106403), ('docvqa/mmllm_fixed_anls_score', 0.7016689256113126)]\n",
      "Got result for mathvista - 2800: [('mathvista/accuracy', 0.404)]\n",
      "Got result for ai2d - 2800: [('ai2d/accuracy', 0.7914507772020726)]\n",
      "Got result for chartqa - 2800: [('chartqa/accuracy', 0.49694777699724874)]\n",
      "Got result for vqa - 2800: [('vqa/accuracy', 0.7272559999999783), ('vqa/recall', 0.7581439999999756), ('vqa/bleu', 0.026664312928915024), ('vqa/mllm_evaluation_accuracy', 0.7524279999999774)]\n",
      "Got result for textvqa - 2800: [('textvqa/accuracy', 69.33200000000035), ('textvqa/mllm_eval_accuracy', 73.94000000000035)]\n",
      "Got result for infographics_w_ocr - 2800: [('infographics_w_ocr/anls_total_score', 0.6432594786147546), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5899130470744622), ('infographics_w_ocr/answer_type_multi_span_score', 0.5223361370497703), ('infographics_w_ocr/answer_type_non_extractive_score', 0.630910981724725), ('infographics_w_ocr/answer_type_question_span_score', 0.6947714347250156), ('infographics_w_ocr/answer_type_single_span_score', 0.6575083871676799), ('infographics_w_ocr/evidence_type_figure_score', 0.6260361349563158), ('infographics_w_ocr/evidence_type_map_score', 0.5789635694338664), ('infographics_w_ocr/evidence_type_table_list_score', 0.6410390694843162), ('infographics_w_ocr/evidence_type_text_score', 0.6852052266417392), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.561785131836383), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6281750213257061), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5453501433473905), ('infographics_w_ocr/reasoning_type_counting_score', 0.6264568764568765)]\n",
      "Got result for infographics - 2800: [('infographics/anls_total_score', 0.5571266134294298), ('infographics/mllm_evaluation_anls_score', 0.4941745670638031), ('infographics/answer_type_multi_span_score', 0.3930490997398355), ('infographics/answer_type_non_extractive_score', 0.5392835063269061), ('infographics/answer_type_question_span_score', 0.705545789680405), ('infographics/answer_type_single_span_score', 0.5731622156604242), ('infographics/evidence_type_figure_score', 0.5451085548933411), ('infographics/evidence_type_map_score', 0.5133124468633476), ('infographics/evidence_type_table_list_score', 0.5221442105952693), ('infographics/evidence_type_text_score', 0.6002041240064658), ('infographics/evidence_type_visual_layout_score', 0.5164415606965644), ('infographics/reasoning_type_arithmetic_score', 0.4932321198588321), ('infographics/reasoning_type_comparison_score', 0.4763705210815633), ('infographics/reasoning_type_counting_score', 0.5743589743589743)]\n",
      "Got result for mmbench - 2800: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.741590612788053)]\n",
      "2800\n",
      "3000\n",
      "Got result for mmmu - 3200: [('mmmu/accounting', 0.6333333333333333), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.6), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.3333333333333333), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.5), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.5), ('mmmu/math', 0.5666666666666667), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.7666666666666667), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5611111111111111), ('mmmu/mllm_eval_accuracy', 0.5688888888888889)]\n",
      "Got result for docvqa - 3200: [('docvqa/anls_total_score', 0.6640880634124493), ('docvqa/mllm_evaluation_anls_score', 0.6634133266479652), ('docvqa/mmllm_fixed_anls_score', 0.7021403504963396)]\n",
      "Got result for mathvista - 3200: [('mathvista/accuracy', 0.379)]\n",
      "Got result for ai2d - 3200: [('ai2d/accuracy', 0.7908031088082902)]\n",
      "Got result for chartqa - 3200: [('chartqa/accuracy', 0.5030746512008956)]\n",
      "Got result for vqa - 3200: [('vqa/accuracy', 0.7282199999999778), ('vqa/recall', 0.7576399999999741), ('vqa/bleu', 0.023279016837477684), ('vqa/mllm_evaluation_accuracy', 0.7522039999999759)]\n",
      "Got result for textvqa - 3200: [('textvqa/accuracy', 69.40600000000035), ('textvqa/mllm_eval_accuracy', 74.03000000000036)]\n",
      "Got result for infographics_w_ocr - 3200: [('infographics_w_ocr/anls_total_score', 0.6429400579120222), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5884253631895616), ('infographics_w_ocr/answer_type_multi_span_score', 0.5402503683724129), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6324377852406786), ('infographics_w_ocr/answer_type_question_span_score', 0.6962243921237135), ('infographics_w_ocr/answer_type_single_span_score', 0.6538366030276421), ('infographics_w_ocr/evidence_type_figure_score', 0.6214902776544455), ('infographics_w_ocr/evidence_type_map_score', 0.5762054364777137), ('infographics_w_ocr/evidence_type_table_list_score', 0.6436928996343314), ('infographics_w_ocr/evidence_type_text_score', 0.6936491906854457), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5767280602473487), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6210779517286364), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5125325401030927), ('infographics_w_ocr/reasoning_type_counting_score', 0.6277972027972027)]\n",
      "Got result for infographics - 3200: [('infographics/anls_total_score', 0.5442911927818022), ('infographics/mllm_evaluation_anls_score', 0.48164092110639845), ('infographics/answer_type_multi_span_score', 0.36575683119083485), ('infographics/answer_type_non_extractive_score', 0.5436266165018426), ('infographics/answer_type_question_span_score', 0.6816990879490878), ('infographics/answer_type_single_span_score', 0.5579889843295365), ('infographics/evidence_type_figure_score', 0.5314863415557626), ('infographics/evidence_type_map_score', 0.5370796835781139), ('infographics/evidence_type_table_list_score', 0.5050513875051371), ('infographics/evidence_type_text_score', 0.5819960900001305), ('infographics/evidence_type_visual_layout_score', 0.5087834108404902), ('infographics/reasoning_type_arithmetic_score', 0.5032837406125076), ('infographics/reasoning_type_comparison_score', 0.4798490257466455), ('infographics/reasoning_type_counting_score', 0.5682400932400932)]\n",
      "Got result for mmbench - 3200: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9191919191919192), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7392020300278188)]\n",
      "3200\n",
      "Got result for mmmu - 3400: [('mmmu/accounting', 0.36666666666666664), ('mmmu/agriculture', 0.5), ('mmmu/architecture_and_engineering', 0.5), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.7333333333333333), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.7), ('mmmu/computer_science', 0.7333333333333333), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.5), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.6666666666666666), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.36666666666666664), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5622222222222222), ('mmmu/mllm_eval_accuracy', 0.5766666666666667)]\n",
      "Got result for docvqa - 3400: [('docvqa/anls_total_score', 0.6662842594918789), ('docvqa/mllm_evaluation_anls_score', 0.6652847094750739), ('docvqa/mmllm_fixed_anls_score', 0.7038201111199454)]\n",
      "Got result for mathvista - 3400: [('mathvista/accuracy', 0.393)]\n",
      "Got result for ai2d - 3400: [('ai2d/accuracy', 0.7924222797927462)]\n",
      "Got result for chartqa - 3400: [('chartqa/accuracy', 0.5096414068533369)]\n",
      "Got result for vqa - 3400: [('vqa/accuracy', 0.7282879999999772), ('vqa/recall', 0.7565879999999746), ('vqa/bleu', 0.0), ('vqa/mllm_evaluation_accuracy', 0.7515359999999767)]\n",
      "Got result for textvqa - 3400: [('textvqa/accuracy', 68.54800000000031), ('textvqa/mllm_eval_accuracy', 73.23600000000035)]\n",
      "Got result for infographics_w_ocr - 3400: [('infographics_w_ocr/anls_total_score', 0.6471130009735296), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5970165747874483), ('infographics_w_ocr/answer_type_multi_span_score', 0.5277120512481788), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6390618272625507), ('infographics_w_ocr/answer_type_question_span_score', 0.6982436569472379), ('infographics_w_ocr/answer_type_single_span_score', 0.659930244634008), ('infographics_w_ocr/evidence_type_figure_score', 0.6248130697058113), ('infographics_w_ocr/evidence_type_map_score', 0.5709190149784209), ('infographics_w_ocr/evidence_type_table_list_score', 0.6465013479258935), ('infographics_w_ocr/evidence_type_text_score', 0.6959895225650007), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5823225535681384), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.624204990215264), ('infographics_w_ocr/reasoning_type_comparison_score', 0.541245895871541), ('infographics_w_ocr/reasoning_type_counting_score', 0.6452797202797204)]\n",
      "Got result for infographics - 3400: [('infographics/anls_total_score', 0.5471520650683339), ('infographics/mllm_evaluation_anls_score', 0.4829317131228439), ('infographics/answer_type_multi_span_score', 0.4026163000965545), ('infographics/answer_type_non_extractive_score', 0.5401711609668212), ('infographics/answer_type_question_span_score', 0.7024932866279021), ('infographics/answer_type_single_span_score', 0.5578557144446681), ('infographics/evidence_type_figure_score', 0.5357241352483342), ('infographics/evidence_type_map_score', 0.49143320011514335), ('infographics/evidence_type_table_list_score', 0.5112195185734529), ('infographics/evidence_type_text_score', 0.5846294022832487), ('infographics/evidence_type_visual_layout_score', 0.5070947987109717), ('infographics/reasoning_type_arithmetic_score', 0.49051821466205014), ('infographics/reasoning_type_comparison_score', 0.48506441211224355), ('infographics/reasoning_type_counting_score', 0.580128205128205)]\n",
      "Got result for mmbench - 3400: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4567901234567901), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6753246753246753), ('mmbench/overall', 0.7446592804291632)]\n",
      "3400\n",
      "Got result for mmmu - 3600: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.6), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.4666666666666667), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8), ('mmmu/literature', 0.9), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.6), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.7), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5455555555555556), ('mmmu/mllm_eval_accuracy', 0.5466666666666666)]\n",
      "Got result for docvqa - 3600: [('docvqa/anls_total_score', 0.6673320053456222), ('docvqa/mllm_evaluation_anls_score', 0.66831914101801), ('docvqa/mmllm_fixed_anls_score', 0.7068702981118987)]\n",
      "Got result for mathvista - 3600: [('mathvista/accuracy', 0.39)]\n",
      "Got result for ai2d - 3600: [('ai2d/accuracy', 0.8069948186528497)]\n",
      "Got result for chartqa - 3600: [('chartqa/accuracy', 0.5095766623262165)]\n",
      "Got result for vqa - 3600: [('vqa/accuracy', 0.7248559999999767), ('vqa/recall', 0.7542519999999742), ('vqa/bleu', 0.027971595525741577), ('vqa/mllm_evaluation_accuracy', 0.7488479999999759)]\n",
      "Got result for textvqa - 3600: [('textvqa/accuracy', 68.49600000000034), ('textvqa/mllm_eval_accuracy', 73.19000000000035)]\n",
      "Got result for infographics_w_ocr - 3600: [('infographics_w_ocr/anls_total_score', 0.6484489445356911), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5965838510927669), ('infographics_w_ocr/answer_type_multi_span_score', 0.5357902204494556), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6404180659605616), ('infographics_w_ocr/answer_type_question_span_score', 0.7011794466477335), ('infographics_w_ocr/answer_type_single_span_score', 0.6604707771186143), ('infographics_w_ocr/evidence_type_figure_score', 0.6247968013105916), ('infographics_w_ocr/evidence_type_map_score', 0.5896708901659397), ('infographics_w_ocr/evidence_type_table_list_score', 0.6551548544784905), ('infographics_w_ocr/evidence_type_text_score', 0.6966182941099536), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5722394099010877), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6259743966079581), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5448196517831402), ('infographics_w_ocr/reasoning_type_counting_score', 0.6404428904428904)]\n",
      "Got result for infographics - 3600: [('infographics/anls_total_score', 0.5496827239760246), ('infographics/mllm_evaluation_anls_score', 0.4876385734970513), ('infographics/answer_type_multi_span_score', 0.3991648633927753), ('infographics/answer_type_non_extractive_score', 0.5463191118706491), ('infographics/answer_type_question_span_score', 0.6709304050650204), ('infographics/answer_type_single_span_score', 0.5620574935011181), ('infographics/evidence_type_figure_score', 0.5381051174079604), ('infographics/evidence_type_map_score', 0.5299723184988182), ('infographics/evidence_type_table_list_score', 0.5186647897424309), ('infographics/evidence_type_text_score', 0.5916394584698487), ('infographics/evidence_type_visual_layout_score', 0.4699430454156177), ('infographics/reasoning_type_arithmetic_score', 0.4979376787595964), ('infographics/reasoning_type_comparison_score', 0.49499427146429703), ('infographics/reasoning_type_counting_score', 0.5825174825174824)]\n",
      "Got result for mmbench - 3600: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7708333333333334), ('mmbench/object_localization', 0.4444444444444444), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7346576364351671)]\n",
      "3600\n",
      "Got result for mmmu - 3800: [('mmmu/accounting', 0.4), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.3333333333333333), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.6666666666666666), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.5666666666666667), ('mmmu/finance', 0.4), ('mmmu/geography', 0.6), ('mmmu/history', 0.8), ('mmmu/literature', 0.9), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.5), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7), ('mmmu/sociology', 0.7666666666666667), ('mmmu/accuracy', 0.5677777777777778), ('mmmu/mllm_eval_accuracy', 0.5844444444444444)]\n",
      "Got result for docvqa - 3800: [('docvqa/anls_total_score', 0.6616529263076446), ('docvqa/mllm_evaluation_anls_score', 0.6633989390774641), ('docvqa/mmllm_fixed_anls_score', 0.7019972711244351)]\n",
      "Got result for mathvista - 3800: [('mathvista/accuracy', 0.387)]\n",
      "Got result for ai2d - 3800: [('ai2d/accuracy', 0.7924222797927462)]\n",
      "Got result for chartqa - 3800: [('chartqa/accuracy', 0.5189239580937337)]\n",
      "Got result for vqa - 3800: [('vqa/accuracy', 0.7256639999999778), ('vqa/recall', 0.7551119999999752), ('vqa/bleu', 0.021624036133289337), ('vqa/mllm_evaluation_accuracy', 0.7498759999999771)]\n",
      "Got result for textvqa - 3800: [('textvqa/accuracy', 68.29400000000028), ('textvqa/mllm_eval_accuracy', 72.97200000000034)]\n",
      "Got result for infographics_w_ocr - 3800: [('infographics_w_ocr/anls_total_score', 0.6465777910239343), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5956844353866315), ('infographics_w_ocr/answer_type_multi_span_score', 0.5321523182587027), ('infographics_w_ocr/answer_type_non_extractive_score', 0.638779815723758), ('infographics_w_ocr/answer_type_question_span_score', 0.6960717669710882), ('infographics_w_ocr/answer_type_single_span_score', 0.6598077568921017), ('infographics_w_ocr/evidence_type_figure_score', 0.627244964614269), ('infographics_w_ocr/evidence_type_map_score', 0.5873794110180248), ('infographics_w_ocr/evidence_type_table_list_score', 0.6430205201144769), ('infographics_w_ocr/evidence_type_text_score', 0.7007880602575256), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5689540112345787), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6256115459882582), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5353694962307665), ('infographics_w_ocr/reasoning_type_counting_score', 0.6344988344988345)]\n",
      "Got result for infographics - 3800: [('infographics/anls_total_score', 0.5462979824952026), ('infographics/mllm_evaluation_anls_score', 0.4831643531581884), ('infographics/answer_type_multi_span_score', 0.3811784973782347), ('infographics/answer_type_non_extractive_score', 0.5436857897979058), ('infographics/answer_type_question_span_score', 0.6914723631069785), ('infographics/answer_type_single_span_score', 0.5585001179081762), ('infographics/evidence_type_figure_score', 0.5339068782328559), ('infographics/evidence_type_map_score', 0.5076392933410988), ('infographics/evidence_type_table_list_score', 0.505617724731024), ('infographics/evidence_type_text_score', 0.5899017580794799), ('infographics/evidence_type_visual_layout_score', 0.5166286708060807), ('infographics/reasoning_type_arithmetic_score', 0.5059642982588186), ('infographics/reasoning_type_comparison_score', 0.4820025249966273), ('infographics/reasoning_type_counting_score', 0.5829170829170829)]\n",
      "Got result for mmbench - 3800: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4567901234567901), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6133333333333333), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7414173634937057)]\n",
      "3800\n",
      "Got result for mmmu - 4000: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.23333333333333334), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.7333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.3), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.6), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.4), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.5333333333333333), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.55), ('mmmu/mllm_eval_accuracy', 0.5655555555555556)]\n",
      "Got result for docvqa - 4000: [('docvqa/anls_total_score', 0.6638353050311572), ('docvqa/mllm_evaluation_anls_score', 0.6635608945241366), ('docvqa/mmllm_fixed_anls_score', 0.7007195639643756)]\n",
      "Got result for mathvista - 4000: [('mathvista/accuracy', 0.397)]\n",
      "Got result for ai2d - 4000: [('ai2d/accuracy', 0.7908031088082902)]\n",
      "Got result for chartqa - 4000: [('chartqa/accuracy', 0.5120386010834749)]\n",
      "Got result for vqa - 4000: [('vqa/accuracy', 0.7281759999999776), ('vqa/recall', 0.7565479999999756), ('vqa/bleu', 0.023881062865257263), ('vqa/mllm_evaluation_accuracy', 0.751255999999977)]\n",
      "Got result for textvqa - 4000: [('textvqa/accuracy', 68.79800000000034), ('textvqa/mllm_eval_accuracy', 73.24600000000038)]\n",
      "Got result for infographics_w_ocr - 4000: [('infographics_w_ocr/anls_total_score', 0.639619658756057), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5871979608324914), ('infographics_w_ocr/answer_type_multi_span_score', 0.5167015009137605), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6394733355854516), ('infographics_w_ocr/answer_type_question_span_score', 0.6793956824647361), ('infographics_w_ocr/answer_type_single_span_score', 0.6514077794543481), ('infographics_w_ocr/evidence_type_figure_score', 0.6222197764554883), ('infographics_w_ocr/evidence_type_map_score', 0.586554328509774), ('infographics_w_ocr/evidence_type_table_list_score', 0.6389060223586145), ('infographics_w_ocr/evidence_type_text_score', 0.6818067502022553), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.53589167955617), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6249272417080635), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5264517082179011), ('infographics_w_ocr/reasoning_type_counting_score', 0.6286713286713287)]\n",
      "Got result for infographics - 4000: [('infographics/anls_total_score', 0.553377349607767), ('infographics/mllm_evaluation_anls_score', 0.49012452921508914), ('infographics/answer_type_multi_span_score', 0.3789344164972565), ('infographics/answer_type_non_extractive_score', 0.5550825003808731), ('infographics/answer_type_question_span_score', 0.6676841854726471), ('infographics/answer_type_single_span_score', 0.5671087354420487), ('infographics/evidence_type_figure_score', 0.5460545267846331), ('infographics/evidence_type_map_score', 0.5520251055486932), ('infographics/evidence_type_table_list_score', 0.5221322629807379), ('infographics/evidence_type_text_score', 0.5897991623267063), ('infographics/evidence_type_visual_layout_score', 0.5085972289449961), ('infographics/reasoning_type_arithmetic_score', 0.5058582969541873), ('infographics/reasoning_type_comparison_score', 0.45285362534681767), ('infographics/reasoning_type_counting_score', 0.5944638694638694)]\n",
      "Got result for mmbench - 4000: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8354430379746836), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7394341674884854)]\n",
      "4000\n",
      "Got result for mmmu - 4200: [('mmmu/accounting', 0.5333333333333333), ('mmmu/agriculture', 0.5333333333333333), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.5333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.6666666666666666), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.5333333333333333), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.5), ('mmmu/sociology', 0.5666666666666667), ('mmmu/accuracy', 0.5444444444444444), ('mmmu/mllm_eval_accuracy', 0.5411111111111111)]\n",
      "Got result for docvqa - 4200: [('docvqa/anls_total_score', 0.6654590780706102), ('docvqa/mllm_evaluation_anls_score', 0.6634939321224607), ('docvqa/mmllm_fixed_anls_score', 0.7014731985007547)]\n",
      "Got result for mathvista - 4200: [('mathvista/accuracy', 0.385)]\n",
      "Got result for ai2d - 4200: [('ai2d/accuracy', 0.7943652849740933)]\n",
      "Got result for chartqa - 4200: [('chartqa/accuracy', 0.5135012984250816)]\n",
      "Got result for vqa - 4200: [('vqa/accuracy', 0.7239439999999766), ('vqa/recall', 0.7538879999999742), ('vqa/bleu', 0.023069756105542183), ('vqa/mllm_evaluation_accuracy', 0.7478439999999751)]\n",
      "Got result for textvqa - 4200: [('textvqa/accuracy', 69.11800000000038), ('textvqa/mllm_eval_accuracy', 73.70000000000041)]\n",
      "Got result for infographics_w_ocr - 4200: [('infographics_w_ocr/anls_total_score', 0.6480012644357671), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5943138508713856), ('infographics_w_ocr/answer_type_multi_span_score', 0.5395363560727504), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6394816154309827), ('infographics_w_ocr/answer_type_question_span_score', 0.7004215838209051), ('infographics_w_ocr/answer_type_single_span_score', 0.6603548033746832), ('infographics_w_ocr/evidence_type_figure_score', 0.6262296275973861), ('infographics_w_ocr/evidence_type_map_score', 0.5936500380807311), ('infographics_w_ocr/evidence_type_table_list_score', 0.6547058701066608), ('infographics_w_ocr/evidence_type_text_score', 0.6926244806151663), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5633052172612127), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6367579908675798), ('infographics_w_ocr/reasoning_type_comparison_score', 0.544022030878238), ('infographics_w_ocr/reasoning_type_counting_score', 0.6311188811188811)]\n",
      "Got result for infographics - 4200: [('infographics/anls_total_score', 0.5425830774462975), ('infographics/mllm_evaluation_anls_score', 0.4797971188207303), ('infographics/answer_type_multi_span_score', 0.3900616811709289), ('infographics/answer_type_non_extractive_score', 0.544308434181852), ('infographics/answer_type_question_span_score', 0.6514284326784328), ('infographics/answer_type_single_span_score', 0.5543465593999105), ('infographics/evidence_type_figure_score', 0.5320936165918101), ('infographics/evidence_type_map_score', 0.5010386332750922), ('infographics/evidence_type_table_list_score', 0.49799591553747263), ('infographics/evidence_type_text_score', 0.5837858283938127), ('infographics/evidence_type_visual_layout_score', 0.514470924619566), ('infographics/reasoning_type_arithmetic_score', 0.5099973656480504), ('infographics/reasoning_type_comparison_score', 0.44134443464461104), ('infographics/reasoning_type_counting_score', 0.5716783216783217)]\n",
      "Got result for mmbench - 4200: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.575), ('mmbench/image_quality', 0.5121951219512195), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6753246753246753), ('mmbench/overall', 0.7373213318922573)]\n",
      "4200\n",
      "Got result for mmmu - 4400: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.4666666666666667), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.9), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.5333333333333333), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.3333333333333333), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5), ('mmmu/history', 0.8), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.4), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.2), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5477777777777778), ('mmmu/mllm_eval_accuracy', 0.55)]\n",
      "Got result for docvqa - 4400: [('docvqa/anls_total_score', 0.6577835036688473), ('docvqa/mllm_evaluation_anls_score', 0.6582541095077417), ('docvqa/mmllm_fixed_anls_score', 0.6963264761710404)]\n",
      "Got result for mathvista - 4400: [('mathvista/accuracy', 0.384)]\n",
      "Got result for ai2d - 4400: [('ai2d/accuracy', 0.8005181347150259)]\n",
      "Got result for chartqa - 4400: [('chartqa/accuracy', 0.4991762741209335)]\n",
      "Got result for vqa - 4400: [('vqa/accuracy', 0.7234039999999777), ('vqa/recall', 0.7527559999999749), ('vqa/bleu', 0.01938352733850479), ('vqa/mllm_evaluation_accuracy', 0.7470599999999753)]\n",
      "Got result for textvqa - 4400: [('textvqa/accuracy', 68.61400000000032), ('textvqa/mllm_eval_accuracy', 73.15600000000038)]\n",
      "Got result for infographics_w_ocr - 4400: [('infographics_w_ocr/anls_total_score', 0.6441712683161956), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.592151199201444), ('infographics_w_ocr/answer_type_multi_span_score', 0.542195857955229), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6345314269816984), ('infographics_w_ocr/answer_type_question_span_score', 0.6972164556157769), ('infographics_w_ocr/answer_type_single_span_score', 0.6570649688924293), ('infographics_w_ocr/evidence_type_figure_score', 0.6223226352659003), ('infographics_w_ocr/evidence_type_map_score', 0.5568513582127443), ('infographics_w_ocr/evidence_type_table_list_score', 0.6485196267161363), ('infographics_w_ocr/evidence_type_text_score', 0.6850861006639555), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5905931652218812), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6239584901399967), ('infographics_w_ocr/reasoning_type_comparison_score', 0.553092497102961), ('infographics_w_ocr/reasoning_type_counting_score', 0.6328671328671329)]\n",
      "Got result for infographics - 4400: [('infographics/anls_total_score', 0.5427382446906195), ('infographics/mllm_evaluation_anls_score', 0.4805578932362299), ('infographics/answer_type_multi_span_score', 0.3943458312960511), ('infographics/answer_type_non_extractive_score', 0.5389742264968306), ('infographics/answer_type_question_span_score', 0.67802747466209), ('infographics/answer_type_single_span_score', 0.5535217133635124), ('infographics/evidence_type_figure_score', 0.5281696747337464), ('infographics/evidence_type_map_score', 0.5429002416156735), ('infographics/evidence_type_table_list_score', 0.5099792044432347), ('infographics/evidence_type_text_score', 0.5845648038746332), ('infographics/evidence_type_visual_layout_score', 0.5284417721023548), ('infographics/reasoning_type_arithmetic_score', 0.4996098650208238), ('infographics/reasoning_type_comparison_score', 0.455277659407749), ('infographics/reasoning_type_counting_score', 0.5635198135198136)]\n",
      "Got result for mmbench - 4400: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6103896103896104), ('mmbench/overall', 0.7404643794534864)]\n",
      "4400\n",
      "Got result for mmmu - 4600: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.5), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.3), ('mmmu/energy_and_power', 0.36666666666666664), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.6333333333333333), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.9), ('mmmu/manage', 0.6), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.5), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.5333333333333333), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5655555555555556), ('mmmu/mllm_eval_accuracy', 0.5833333333333334)]\n",
      "Got result for docvqa - 4600: [('docvqa/anls_total_score', 0.6649139118450663), ('docvqa/mllm_evaluation_anls_score', 0.6664944150599198), ('docvqa/mmllm_fixed_anls_score', 0.7036872840615839)]\n",
      "Got result for mathvista - 4600: [('mathvista/accuracy', 0.396)]\n",
      "Got result for ai2d - 4600: [('ai2d/accuracy', 0.7865932642487047)]\n",
      "Got result for chartqa - 4600: [('chartqa/accuracy', 0.5067031887108536)]\n",
      "Got result for vqa - 4600: [('vqa/accuracy', 0.7231519999999776), ('vqa/recall', 0.752971999999975), ('vqa/bleu', 0.028394801542162895), ('vqa/mllm_evaluation_accuracy', 0.7475439999999761)]\n",
      "Got result for textvqa - 4600: [('textvqa/accuracy', 68.3180000000003), ('textvqa/mllm_eval_accuracy', 72.77000000000035)]\n",
      "Got result for infographics_w_ocr - 4600: [('infographics_w_ocr/anls_total_score', 0.6433603004791333), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5912851946288605), ('infographics_w_ocr/answer_type_multi_span_score', 0.5177825851123312), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6307091190906744), ('infographics_w_ocr/answer_type_question_span_score', 0.6975598622091835), ('infographics_w_ocr/answer_type_single_span_score', 0.6566383586330448), ('infographics_w_ocr/evidence_type_figure_score', 0.6236438921927718), ('infographics_w_ocr/evidence_type_map_score', 0.5573464077176947), ('infographics_w_ocr/evidence_type_table_list_score', 0.6365635749629702), ('infographics_w_ocr/evidence_type_text_score', 0.6895555646007121), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5868064096004332), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6095278864970644), ('infographics_w_ocr/reasoning_type_comparison_score', 0.538524967067676), ('infographics_w_ocr/reasoning_type_counting_score', 0.6416083916083916)]\n",
      "Got result for infographics - 4600: [('infographics/anls_total_score', 0.5465704977363636), ('infographics/mllm_evaluation_anls_score', 0.48339642069589106), ('infographics/answer_type_multi_span_score', 0.3552702863567235), ('infographics/answer_type_non_extractive_score', 0.5460481953248681), ('infographics/answer_type_question_span_score', 0.7087509178855332), ('infographics/answer_type_single_span_score', 0.5580794874915083), ('infographics/evidence_type_figure_score', 0.5357241784906924), ('infographics/evidence_type_map_score', 0.551946224034168), ('infographics/evidence_type_table_list_score', 0.5176125767609223), ('infographics/evidence_type_text_score', 0.5782280019500535), ('infographics/evidence_type_visual_layout_score', 0.49255177096170427), ('infographics/reasoning_type_arithmetic_score', 0.5163173014200411), ('infographics/reasoning_type_comparison_score', 0.46498154371650674), ('infographics/reasoning_type_counting_score', 0.5685314685314685)]\n",
      "Got result for mmbench - 4600: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4444444444444444), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7417084303855573)]\n",
      "4600\n",
      "Got result for mmmu - 4800: [('mmmu/accounting', 0.4), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.43333333333333335), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.5), ('mmmu/clinical_medicine', 0.5333333333333333), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.7), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.6666666666666666), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.6), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5811111111111111), ('mmmu/mllm_eval_accuracy', 0.5911111111111111)]\n",
      "Got result for docvqa - 4800: [('docvqa/anls_total_score', 0.6656203103508205), ('docvqa/mllm_evaluation_anls_score', 0.6646199881328083), ('docvqa/mmllm_fixed_anls_score', 0.7012822552137186)]\n",
      "Got result for mathvista - 4800: [('mathvista/accuracy', 0.384)]\n",
      "Got result for ai2d - 4800: [('ai2d/accuracy', 0.7891839378238342)]\n",
      "Got result for chartqa - 4800: [('chartqa/accuracy', 0.5180198972882273)]\n",
      "Got result for vqa - 4800: [('vqa/accuracy', 0.7249759999999768), ('vqa/recall', 0.7533599999999744), ('vqa/bleu', 0.030800124630331993), ('vqa/mllm_evaluation_accuracy', 0.7481919999999755)]\n",
      "Got result for textvqa - 4800: [('textvqa/accuracy', 68.86200000000034), ('textvqa/mllm_eval_accuracy', 73.45800000000037)]\n",
      "Got result for infographics_w_ocr - 4800: [('infographics_w_ocr/anls_total_score', 0.6461024126003606), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.593693098418034), ('infographics_w_ocr/answer_type_multi_span_score', 0.5505649040052891), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6318737621630931), ('infographics_w_ocr/answer_type_question_span_score', 0.6885572226918381), ('infographics_w_ocr/answer_type_single_span_score', 0.6594201869657167), ('infographics_w_ocr/evidence_type_figure_score', 0.6292872406604259), ('infographics_w_ocr/evidence_type_map_score', 0.6053662096978929), ('infographics_w_ocr/evidence_type_table_list_score', 0.6403314300016312), ('infographics_w_ocr/evidence_type_text_score', 0.701623764565011), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5297610549227327), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6311399217221133), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5328399597858798), ('infographics_w_ocr/reasoning_type_counting_score', 0.6175990675990678)]\n",
      "Got result for infographics - 4800: [('infographics/anls_total_score', 0.5529616122127399), ('infographics/mllm_evaluation_anls_score', 0.48861416586293815), ('infographics/answer_type_multi_span_score', 0.3598815257435469), ('infographics/answer_type_non_extractive_score', 0.5428128732830362), ('infographics/answer_type_question_span_score', 0.6904867746213901), ('infographics/answer_type_single_span_score', 0.5702777400696709), ('infographics/evidence_type_figure_score', 0.5439963428025235), ('infographics/evidence_type_map_score', 0.5443871989008856), ('infographics/evidence_type_table_list_score', 0.5173909875716537), ('infographics/evidence_type_text_score', 0.5913938033564236), ('infographics/evidence_type_visual_layout_score', 0.489056743627179), ('infographics/reasoning_type_arithmetic_score', 0.4954641059093113), ('infographics/reasoning_type_comparison_score', 0.482062192335367), ('infographics/reasoning_type_counting_score', 0.5807109557109557)]\n",
      "Got result for mmbench - 4800: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7434887286382224)]\n",
      "4800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.5444</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.7937</td>\n",
       "      <td>0.4937</td>\n",
       "      <td>0.7255</td>\n",
       "      <td>0.68126</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.5379</td>\n",
       "      <td>0.7381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.6534</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.7895</td>\n",
       "      <td>0.4979</td>\n",
       "      <td>0.7258</td>\n",
       "      <td>0.68664</td>\n",
       "      <td>0.6334</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.7353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.5567</td>\n",
       "      <td>0.5433</td>\n",
       "      <td>0.6576</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.7947</td>\n",
       "      <td>0.4977</td>\n",
       "      <td>0.7262</td>\n",
       "      <td>0.68338</td>\n",
       "      <td>0.6451</td>\n",
       "      <td>0.5388</td>\n",
       "      <td>0.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.5756</td>\n",
       "      <td>0.5622</td>\n",
       "      <td>0.6862</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.8258</td>\n",
       "      <td>0.5386</td>\n",
       "      <td>0.7527</td>\n",
       "      <td>0.7053</td>\n",
       "      <td>0.6473</td>\n",
       "      <td>0.5585</td>\n",
       "      <td>0.7507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.5511</td>\n",
       "      <td>0.6915</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.5447</td>\n",
       "      <td>0.7517</td>\n",
       "      <td>0.70046</td>\n",
       "      <td>0.6548</td>\n",
       "      <td>0.5557</td>\n",
       "      <td>0.7557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>0.5833</td>\n",
       "      <td>0.5778</td>\n",
       "      <td>0.6882</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.8251</td>\n",
       "      <td>0.5338</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.69956</td>\n",
       "      <td>0.6509</td>\n",
       "      <td>0.5539</td>\n",
       "      <td>0.7495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>0.5633</td>\n",
       "      <td>0.5578</td>\n",
       "      <td>0.6576</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.5053</td>\n",
       "      <td>0.7249</td>\n",
       "      <td>0.68468</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.5447</td>\n",
       "      <td>0.7389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>0.5811</td>\n",
       "      <td>0.5678</td>\n",
       "      <td>0.6634</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.5018</td>\n",
       "      <td>0.7266</td>\n",
       "      <td>0.68846</td>\n",
       "      <td>0.6457</td>\n",
       "      <td>0.5496</td>\n",
       "      <td>0.7391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>0.5822</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.6844</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.8174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7503</td>\n",
       "      <td>0.70316</td>\n",
       "      <td>0.6526</td>\n",
       "      <td>0.5586</td>\n",
       "      <td>0.7516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.5522</td>\n",
       "      <td>0.5433</td>\n",
       "      <td>0.6628</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.8021</td>\n",
       "      <td>0.5066</td>\n",
       "      <td>0.7267</td>\n",
       "      <td>0.68474</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.5559</td>\n",
       "      <td>0.736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>0.5744</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.6694</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.7266</td>\n",
       "      <td>0.68846</td>\n",
       "      <td>0.6433</td>\n",
       "      <td>0.5439</td>\n",
       "      <td>0.7421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>0.5756</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.6676</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.7979</td>\n",
       "      <td>0.5218</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.68844</td>\n",
       "      <td>0.6401</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.7415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>0.5756</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.6673</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.8031</td>\n",
       "      <td>0.5076</td>\n",
       "      <td>0.7276</td>\n",
       "      <td>0.69086</td>\n",
       "      <td>0.6391</td>\n",
       "      <td>0.5484</td>\n",
       "      <td>0.7423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>0.5589</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.7915</td>\n",
       "      <td>0.4969</td>\n",
       "      <td>0.7273</td>\n",
       "      <td>0.69332</td>\n",
       "      <td>0.6433</td>\n",
       "      <td>0.5571</td>\n",
       "      <td>0.7416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.5611</td>\n",
       "      <td>0.6641</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.7908</td>\n",
       "      <td>0.5031</td>\n",
       "      <td>0.7282</td>\n",
       "      <td>0.69406</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.5443</td>\n",
       "      <td>0.7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>0.5767</td>\n",
       "      <td>0.5622</td>\n",
       "      <td>0.6663</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.7924</td>\n",
       "      <td>0.5096</td>\n",
       "      <td>0.7283</td>\n",
       "      <td>0.68548</td>\n",
       "      <td>0.6471</td>\n",
       "      <td>0.5472</td>\n",
       "      <td>0.7447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>0.5467</td>\n",
       "      <td>0.5456</td>\n",
       "      <td>0.6673</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.5096</td>\n",
       "      <td>0.7249</td>\n",
       "      <td>0.68496</td>\n",
       "      <td>0.6484</td>\n",
       "      <td>0.5497</td>\n",
       "      <td>0.7347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.5678</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.7924</td>\n",
       "      <td>0.5189</td>\n",
       "      <td>0.7257</td>\n",
       "      <td>0.68294</td>\n",
       "      <td>0.6466</td>\n",
       "      <td>0.5463</td>\n",
       "      <td>0.7414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.7908</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.7282</td>\n",
       "      <td>0.68798</td>\n",
       "      <td>0.6396</td>\n",
       "      <td>0.5534</td>\n",
       "      <td>0.7394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>0.5411</td>\n",
       "      <td>0.5444</td>\n",
       "      <td>0.6655</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.5135</td>\n",
       "      <td>0.7239</td>\n",
       "      <td>0.69118</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.5426</td>\n",
       "      <td>0.7373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.6578</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.8005</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.7234</td>\n",
       "      <td>0.68614</td>\n",
       "      <td>0.6442</td>\n",
       "      <td>0.5427</td>\n",
       "      <td>0.7405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.5833</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.6649</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.7866</td>\n",
       "      <td>0.5067</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>0.68318</td>\n",
       "      <td>0.6434</td>\n",
       "      <td>0.5466</td>\n",
       "      <td>0.7417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>0.5911</td>\n",
       "      <td>0.5811</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.68862</td>\n",
       "      <td>0.6461</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.7435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1  docvqa mathvista    ai2d chartqa     vqa  textvqa  \\\n",
       "200   0.5556  0.5444   0.649     0.391  0.7937  0.4937  0.7255  0.68126   \n",
       "400     0.57  0.5656  0.6534     0.399  0.7895  0.4979  0.7258  0.68664   \n",
       "600   0.5567  0.5433  0.6576     0.379  0.7947  0.4977  0.7262  0.68338   \n",
       "800   0.5756  0.5622  0.6862     0.435  0.8258  0.5386  0.7527   0.7053   \n",
       "1000  0.5667  0.5511  0.6915     0.441  0.8229  0.5447  0.7517  0.70046   \n",
       "1200  0.5833  0.5778  0.6882     0.443  0.8251  0.5338   0.753  0.69956   \n",
       "1400  0.5633  0.5578  0.6576      0.39  0.8015  0.5053  0.7249  0.68468   \n",
       "1600  0.5811  0.5678  0.6634     0.387  0.8018  0.5018  0.7266  0.68846   \n",
       "1800  0.5822    0.57  0.6844     0.479  0.8174     NaN  0.7503  0.70316   \n",
       "2000  0.5522  0.5433  0.6628     0.378  0.8021  0.5066  0.7267  0.68474   \n",
       "2200  0.5744  0.5656  0.6694     0.394  0.7944  0.5129  0.7266  0.68846   \n",
       "2400  0.5756    0.57  0.6676     0.398  0.7979  0.5218   0.727  0.68844   \n",
       "2600  0.5756    0.56  0.6673     0.399  0.8031  0.5076  0.7276  0.69086   \n",
       "2800  0.5589  0.5544   0.664     0.404  0.7915  0.4969  0.7273  0.69332   \n",
       "3200  0.5689  0.5611  0.6641     0.379  0.7908  0.5031  0.7282  0.69406   \n",
       "3400  0.5767  0.5622  0.6663     0.393  0.7924  0.5096  0.7283  0.68548   \n",
       "3600  0.5467  0.5456  0.6673      0.39   0.807  0.5096  0.7249  0.68496   \n",
       "3800  0.5844  0.5678  0.6617     0.387  0.7924  0.5189  0.7257  0.68294   \n",
       "4000  0.5656    0.55  0.6638     0.397  0.7908   0.512  0.7282  0.68798   \n",
       "4200  0.5411  0.5444  0.6655     0.385  0.7944  0.5135  0.7239  0.69118   \n",
       "4400    0.55  0.5478  0.6578     0.384  0.8005  0.4992  0.7234  0.68614   \n",
       "4600  0.5833  0.5656  0.6649     0.396  0.7866  0.5067  0.7232  0.68318   \n",
       "4800  0.5911  0.5811  0.6656     0.384  0.7892   0.518   0.725  0.68862   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "200               0.641       0.5379  0.7381  \n",
       "400              0.6334       0.5391  0.7353  \n",
       "600              0.6451       0.5388   0.737  \n",
       "800              0.6473       0.5585  0.7507  \n",
       "1000             0.6548       0.5557  0.7557  \n",
       "1200             0.6509       0.5539  0.7495  \n",
       "1400              0.637       0.5447  0.7389  \n",
       "1600             0.6457       0.5496  0.7391  \n",
       "1800             0.6526       0.5586  0.7516  \n",
       "2000              0.642       0.5559   0.736  \n",
       "2200             0.6433       0.5439  0.7421  \n",
       "2400             0.6401        0.545  0.7415  \n",
       "2600             0.6391       0.5484  0.7423  \n",
       "2800             0.6433       0.5571  0.7416  \n",
       "3200             0.6429       0.5443  0.7392  \n",
       "3400             0.6471       0.5472  0.7447  \n",
       "3600             0.6484       0.5497  0.7347  \n",
       "3800             0.6466       0.5463  0.7414  \n",
       "4000             0.6396       0.5534  0.7394  \n",
       "4200              0.648       0.5426  0.7373  \n",
       "4400             0.6442       0.5427  0.7405  \n",
       "4600             0.6434       0.5466  0.7417  \n",
       "4800             0.6461        0.553  0.7435  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores_all(\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New checkpoints: [4400]\n",
      "Start eval for /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp28/checkpoint-4400\n",
      "eval jobs saved to: /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp28/evals/eval_jobs_checkpoint-4400.json\n"
     ]
    }
   ],
   "source": [
    "eval_helper.run_eval_sweep(\n",
    "    output_dir=f\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp28\",\n",
    "    eval_sbatch=EVAL_SBATCH,\n",
    "    eval_config_dir=EVAL_CONFIG_DIR,\n",
    "    aligner_parent_dir=ALIGNER_CODE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2600, 2800, 3000, 3200, 3400, 3600, 3800, 4000, 4200, 4400]\n",
      "Got result for mmmu - 200: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.43333333333333335), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.5333333333333333), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.5666666666666667), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.8), ('mmmu/accuracy', 0.5577777777777778), ('mmmu/mllm_eval_accuracy', 0.5688888888888889)]\n",
      "Got result for docvqa - 200: [('docvqa/anls_total_score', 0.6851530745117566), ('docvqa/mllm_evaluation_anls_score', 0.6844053884846867), ('docvqa/mmllm_fixed_anls_score', 0.7223642689741734)]\n",
      "Got result for mathvista - 200: [('mathvista/accuracy', 0.411)]\n",
      "Got result for ai2d - 200: [('ai2d/accuracy', 0.8173575129533679)]\n",
      "Got result for chartqa - 200: [('chartqa/accuracy', 0.518923699207068)]\n",
      "Got result for vqa - 200: [('vqa/accuracy', 0.7468399999999757), ('vqa/recall', 0.7745999999999753), ('vqa/bleu', 0.03156115487217903), ('vqa/mllm_evaluation_accuracy', 0.7705199999999759)]\n",
      "Got result for textvqa - 200: [('textvqa/accuracy', 69.68800000000036), ('textvqa/mllm_eval_accuracy', 73.83400000000039)]\n",
      "Got result for infographics_w_ocr - 200: [('infographics_w_ocr/anls_total_score', 0.6447185406390223), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5938385075230164), ('infographics_w_ocr/answer_type_multi_span_score', 0.5503691938469056), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6404162443945446), ('infographics_w_ocr/answer_type_question_span_score', 0.7162564434057648), ('infographics_w_ocr/answer_type_single_span_score', 0.6523644566708748), ('infographics_w_ocr/evidence_type_figure_score', 0.6264635524853811), ('infographics_w_ocr/evidence_type_map_score', 0.6020658796648897), ('infographics_w_ocr/evidence_type_table_list_score', 0.6435081450113043), ('infographics_w_ocr/evidence_type_text_score', 0.6909712626814268), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5598980170463517), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6296239148978872), ('infographics_w_ocr/reasoning_type_comparison_score', 0.54614763522812), ('infographics_w_ocr/reasoning_type_counting_score', 0.6343240093240093)]\n",
      "Got result for infographics - 200: [('infographics/anls_total_score', 0.5586695556928462), ('infographics/mllm_evaluation_anls_score', 0.49428537870339406), ('infographics/answer_type_multi_span_score', 0.39405287016749907), ('infographics/answer_type_non_extractive_score', 0.5501736559602743), ('infographics/answer_type_question_span_score', 0.6900682117028271), ('infographics/answer_type_single_span_score', 0.5732820784316588), ('infographics/evidence_type_figure_score', 0.5410299600923666), ('infographics/evidence_type_map_score', 0.5383822615371227), ('infographics/evidence_type_table_list_score', 0.5363205973740994), ('infographics/evidence_type_text_score', 0.5959148594734742), ('infographics/evidence_type_visual_layout_score', 0.5204674401109819), ('infographics/reasoning_type_arithmetic_score', 0.5001576429658621), ('infographics/reasoning_type_comparison_score', 0.48523278426784316), ('infographics/reasoning_type_counting_score', 0.5865384615384616)]\n",
      "Got result for mmbench - 200: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.5121951219512195), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7430291030449682)]\n",
      "200\n",
      "Got result for mmmu - 400: [('mmmu/accounting', 0.43333333333333335), ('mmmu/agriculture', 0.5), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.9), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.6666666666666666), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.7), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.5), ('mmmu/geography', 0.5), ('mmmu/history', 0.8), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.3), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7666666666666667), ('mmmu/accuracy', 0.5544444444444444), ('mmmu/mllm_eval_accuracy', 0.5766666666666667)]\n",
      "Got result for docvqa - 400: [('docvqa/anls_total_score', 0.694550159268945), ('docvqa/mllm_evaluation_anls_score', 0.6942296440524113), ('docvqa/mmllm_fixed_anls_score', 0.7304485879581767)]\n",
      "Got result for mathvista - 400: [('mathvista/accuracy', 0.42)]\n",
      "Got result for ai2d - 400: [('ai2d/accuracy', 0.8180051813471503)]\n",
      "Got result for chartqa - 400: [('chartqa/accuracy', 0.5345047208714526)]\n",
      "Got result for vqa - 400: [('vqa/accuracy', 0.7513399999999752), ('vqa/recall', 0.7769959999999749), ('vqa/bleu', 0.038769494742155075), ('vqa/mllm_evaluation_accuracy', 0.7737159999999762)]\n",
      "Got result for textvqa - 400: [('textvqa/accuracy', 70.67000000000037), ('textvqa/mllm_eval_accuracy', 74.85400000000044)]\n",
      "Got result for infographics_w_ocr - 400: [('infographics_w_ocr/anls_total_score', 0.6488734426426309), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5961101948309698), ('infographics_w_ocr/answer_type_multi_span_score', 0.517284821077321), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6378799621114272), ('infographics_w_ocr/answer_type_question_span_score', 0.7106474690467903), ('infographics_w_ocr/answer_type_single_span_score', 0.6632571398308017), ('infographics_w_ocr/evidence_type_figure_score', 0.6325310760629169), ('infographics_w_ocr/evidence_type_map_score', 0.5698876618431074), ('infographics_w_ocr/evidence_type_table_list_score', 0.6525122710947869), ('infographics_w_ocr/evidence_type_text_score', 0.6869910781037137), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5879749226782212), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6350945857795169), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5523507971019843), ('infographics_w_ocr/reasoning_type_counting_score', 0.6255827505827506)]\n",
      "Got result for infographics - 400: [('infographics/anls_total_score', 0.5459186927299082), ('infographics/mllm_evaluation_anls_score', 0.4839021941403154), ('infographics/answer_type_multi_span_score', 0.3500961852690957), ('infographics/answer_type_non_extractive_score', 0.5448996151527796), ('infographics/answer_type_question_span_score', 0.6931397928994083), ('infographics/answer_type_single_span_score', 0.5603107000740348), ('infographics/evidence_type_figure_score', 0.5339557398702707), ('infographics/evidence_type_map_score', 0.5209845941685759), ('infographics/evidence_type_table_list_score', 0.5138916064950364), ('infographics/evidence_type_text_score', 0.5846427918166275), ('infographics/evidence_type_visual_layout_score', 0.4980518059551699), ('infographics/reasoning_type_arithmetic_score', 0.49713294696171395), ('infographics/reasoning_type_comparison_score', 0.4810125595603904), ('infographics/reasoning_type_counting_score', 0.5818764568764568)]\n",
      "Got result for mmbench - 400: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.9166666666666666), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.7142857142857143), ('mmbench/overall', 0.7529135901708386)]\n",
      "400\n",
      "Got result for mmmu - 600: [('mmmu/accounting', 0.43333333333333335), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.7), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.6), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.8), ('mmmu/sociology', 0.7666666666666667), ('mmmu/accuracy', 0.5688888888888889), ('mmmu/mllm_eval_accuracy', 0.58)]\n",
      "Got result for docvqa - 600: [('docvqa/anls_total_score', 0.6835048246017047), ('docvqa/mllm_evaluation_anls_score', 0.6836101710515065), ('docvqa/mmllm_fixed_anls_score', 0.7210585631034013)]\n",
      "Got result for mathvista - 600: [('mathvista/accuracy', 0.432)]\n",
      "Got result for ai2d - 600: [('ai2d/accuracy', 0.8235103626943006)]\n",
      "Got result for chartqa - 600: [('chartqa/accuracy', 0.5300919808810313)]\n",
      "Got result for vqa - 600: [('vqa/accuracy', 0.7490999999999756), ('vqa/recall', 0.7748799999999743), ('vqa/bleu', 0.026821967214345932), ('vqa/mllm_evaluation_accuracy', 0.7706199999999754)]\n",
      "Got result for textvqa - 600: [('textvqa/accuracy', 70.01200000000031), ('textvqa/mllm_eval_accuracy', 74.30600000000035)]\n",
      "Got result for infographics_w_ocr - 600: [('infographics_w_ocr/anls_total_score', 0.6515065401246783), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6014267269645682), ('infographics_w_ocr/answer_type_multi_span_score', 0.507566383835137), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6516980307215392), ('infographics_w_ocr/answer_type_question_span_score', 0.7197538038884194), ('infographics_w_ocr/answer_type_single_span_score', 0.6616220186156623), ('infographics_w_ocr/evidence_type_figure_score', 0.6285914637931848), ('infographics_w_ocr/evidence_type_map_score', 0.5881656661141226), ('infographics_w_ocr/evidence_type_table_list_score', 0.6571715774068906), ('infographics_w_ocr/evidence_type_text_score', 0.6951097903193735), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5605610061452192), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6561267499623661), ('infographics_w_ocr/reasoning_type_comparison_score', 0.557049368550685), ('infographics_w_ocr/reasoning_type_counting_score', 0.6378205128205128)]\n",
      "Got result for infographics - 600: [('infographics/anls_total_score', 0.5574156881593187), ('infographics/mllm_evaluation_anls_score', 0.49462318399407484), ('infographics/answer_type_multi_span_score', 0.39092636618510734), ('infographics/answer_type_non_extractive_score', 0.5554227468332352), ('infographics/answer_type_question_span_score', 0.699454658589274), ('infographics/answer_type_single_span_score', 0.5691865031249267), ('infographics/evidence_type_figure_score', 0.5415843319005622), ('infographics/evidence_type_map_score', 0.5580465782780866), ('infographics/evidence_type_table_list_score', 0.5314606966184777), ('infographics/evidence_type_text_score', 0.5992339394229137), ('infographics/evidence_type_visual_layout_score', 0.5035985331204011), ('infographics/reasoning_type_arithmetic_score', 0.516833718032348), ('infographics/reasoning_type_comparison_score', 0.4657378681362684), ('infographics/reasoning_type_counting_score', 0.5865384615384615)]\n",
      "Got result for mmbench - 600: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.743029470672095)]\n",
      "600\n",
      "Got result for mmmu - 800: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.7333333333333333), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.4), ('mmmu/geography', 0.43333333333333335), ('mmmu/history', 0.7), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.5333333333333333), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5622222222222222), ('mmmu/mllm_eval_accuracy', 0.5755555555555556)]\n",
      "Got result for docvqa - 800: [('docvqa/anls_total_score', 0.6861651868027577), ('docvqa/mllm_evaluation_anls_score', 0.6849665255082817), ('docvqa/mmllm_fixed_anls_score', 0.720501167940428)]\n",
      "Got result for mathvista - 800: [('mathvista/accuracy', 0.435)]\n",
      "Got result for ai2d - 800: [('ai2d/accuracy', 0.8257772020725389)]\n",
      "Got result for chartqa - 800: [('chartqa/accuracy', 0.5386423595754996)]\n",
      "Got result for vqa - 800: [('vqa/accuracy', 0.7527439999999767), ('vqa/recall', 0.7740319999999759), ('vqa/bleu', 0.04545086994767189), ('vqa/mllm_evaluation_accuracy', 0.7709879999999767)]\n",
      "Got result for textvqa - 800: [('textvqa/accuracy', 70.53000000000036), ('textvqa/mllm_eval_accuracy', 74.64600000000038)]\n",
      "Got result for infographics_w_ocr - 800: [('infographics_w_ocr/anls_total_score', 0.6472951114150778), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5973177554748575), ('infographics_w_ocr/answer_type_multi_span_score', 0.503323993986201), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6466639398375387), ('infographics_w_ocr/answer_type_question_span_score', 0.7170577254570468), ('infographics_w_ocr/answer_type_single_span_score', 0.6575469899729094), ('infographics_w_ocr/evidence_type_figure_score', 0.6270001994179981), ('infographics_w_ocr/evidence_type_map_score', 0.608501523229246), ('infographics_w_ocr/evidence_type_table_list_score', 0.6534260886396096), ('infographics_w_ocr/evidence_type_text_score', 0.692856945110573), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5523783679965781), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.652300771906936), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5411386472815425), ('infographics_w_ocr/reasoning_type_counting_score', 0.6284965034965035)]\n",
      "Got result for infographics - 800: [('infographics/anls_total_score', 0.5584713034221711), ('infographics/mllm_evaluation_anls_score', 0.4991246766993003), ('infographics/answer_type_multi_span_score', 0.3821692950630614), ('infographics/answer_type_non_extractive_score', 0.5402885139774832), ('infographics/answer_type_question_span_score', 0.694036465671081), ('infographics/answer_type_single_span_score', 0.5761809310517206), ('infographics/evidence_type_figure_score', 0.5486501418344766), ('infographics/evidence_type_map_score', 0.5589467317772501), ('infographics/evidence_type_table_list_score', 0.524121350581692), ('infographics/evidence_type_text_score', 0.5997578810480703), ('infographics/evidence_type_visual_layout_score', 0.5009458084050376), ('infographics/reasoning_type_arithmetic_score', 0.4844048455349825), ('infographics/reasoning_type_comparison_score', 0.4813541596300644), ('infographics/reasoning_type_counting_score', 0.5786713286713286)]\n",
      "Got result for mmbench - 800: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.9166666666666666), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6753246753246753), ('mmbench/overall', 0.750688446377466)]\n",
      "800\n",
      "Got result for mmmu - 1000: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5333333333333333), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.6), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.4666666666666667), ('mmmu/energy_and_power', 0.36666666666666664), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.43333333333333335), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.6333333333333333), ('mmmu/materials', 0.4), ('mmmu/math', 0.6), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5511111111111111), ('mmmu/mllm_eval_accuracy', 0.5666666666666667)]\n",
      "Got result for docvqa - 1000: [('docvqa/anls_total_score', 0.6914809452898901), ('docvqa/mllm_evaluation_anls_score', 0.6910337685765978), ('docvqa/mmllm_fixed_anls_score', 0.7266098512822747)]\n",
      "Got result for mathvista - 1000: [('mathvista/accuracy', 0.441)]\n",
      "Got result for ai2d - 1000: [('ai2d/accuracy', 0.8228626943005182)]\n",
      "Got result for chartqa - 1000: [('chartqa/accuracy', 0.5446625354709164)]\n",
      "Got result for vqa - 1000: [('vqa/accuracy', 0.7517119999999763), ('vqa/recall', 0.7744439999999746), ('vqa/bleu', 0.03847002610564232), ('vqa/mllm_evaluation_accuracy', 0.7715879999999756)]\n",
      "Got result for textvqa - 1000: [('textvqa/accuracy', 70.04600000000036), ('textvqa/mllm_eval_accuracy', 74.15000000000038)]\n",
      "Got result for infographics_w_ocr - 1000: [('infographics_w_ocr/anls_total_score', 0.6547973502072989), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6048048545226773), ('infographics_w_ocr/answer_type_multi_span_score', 0.503602583214263), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6551580125721177), ('infographics_w_ocr/answer_type_question_span_score', 0.7178612519958674), ('infographics_w_ocr/answer_type_single_span_score', 0.6665106562611337), ('infographics_w_ocr/evidence_type_figure_score', 0.6354380330971015), ('infographics_w_ocr/evidence_type_map_score', 0.5891945925361767), ('infographics_w_ocr/evidence_type_table_list_score', 0.6538581681693623), ('infographics_w_ocr/evidence_type_text_score', 0.7021856039991057), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5823333066147026), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.65662915851272), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5496088289389683), ('infographics_w_ocr/reasoning_type_counting_score', 0.6491841491841491)]\n",
      "Got result for infographics - 1000: [('infographics/anls_total_score', 0.5557274631323967), ('infographics/mllm_evaluation_anls_score', 0.48987016150522056), ('infographics/answer_type_multi_span_score', 0.35456009292459895), ('infographics/answer_type_non_extractive_score', 0.5457338923885037), ('infographics/answer_type_question_span_score', 0.6946469662815817), ('infographics/answer_type_single_span_score', 0.5732867615153577), ('infographics/evidence_type_figure_score', 0.5433856843943352), ('infographics/evidence_type_map_score', 0.5117018932850822), ('infographics/evidence_type_table_list_score', 0.5286447006472307), ('infographics/evidence_type_text_score', 0.5924922026101045), ('infographics/evidence_type_visual_layout_score', 0.5158001118642461), ('infographics/reasoning_type_arithmetic_score', 0.49825630990014547), ('infographics/reasoning_type_comparison_score', 0.47883552954855163), ('infographics/reasoning_type_counting_score', 0.5812937062937062)]\n",
      "Got result for mmbench - 1000: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.5185185185185185), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7557467484518534)]\n",
      "1000\n",
      "Got result for mmmu - 1200: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.5), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.5666666666666667), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.6333333333333333), ('mmmu/marketing', 0.4), ('mmmu/materials', 0.26666666666666666), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5777777777777777), ('mmmu/mllm_eval_accuracy', 0.5833333333333334)]\n",
      "Got result for docvqa - 1200: [('docvqa/anls_total_score', 0.688159004463097), ('docvqa/mllm_evaluation_anls_score', 0.687887849937589), ('docvqa/mmllm_fixed_anls_score', 0.7248432274205141)]\n",
      "Got result for mathvista - 1200: [('mathvista/accuracy', 0.443)]\n",
      "Got result for ai2d - 1200: [('ai2d/accuracy', 0.8251295336787565)]\n",
      "Got result for chartqa - 1200: [('chartqa/accuracy', 0.5338041688877015)]\n",
      "Got result for vqa - 1200: [('vqa/accuracy', 0.7530399999999744), ('vqa/recall', 0.7722879999999732), ('vqa/bleu', 0.042081840336322784), ('vqa/mllm_evaluation_accuracy', 0.7701319999999744)]\n",
      "Got result for textvqa - 1200: [('textvqa/accuracy', 69.95600000000036), ('textvqa/mllm_eval_accuracy', 74.09000000000042)]\n",
      "Got result for infographics_w_ocr - 1200: [('infographics_w_ocr/anls_total_score', 0.6508986976368093), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5996206615245506), ('infographics_w_ocr/answer_type_multi_span_score', 0.5430357056045166), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6591065053090372), ('infographics_w_ocr/answer_type_question_span_score', 0.7079765288758502), ('infographics_w_ocr/answer_type_single_span_score', 0.6569890842073937), ('infographics_w_ocr/evidence_type_figure_score', 0.6324564858914928), ('infographics_w_ocr/evidence_type_map_score', 0.6080064737242955), ('infographics_w_ocr/evidence_type_table_list_score', 0.646020786828472), ('infographics_w_ocr/evidence_type_text_score', 0.6924020368555358), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5810923917569475), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6589699683877763), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5452584973158334), ('infographics_w_ocr/reasoning_type_counting_score', 0.6386946386946387)]\n",
      "Got result for infographics - 1200: [('infographics/anls_total_score', 0.5538923675620057), ('infographics/mllm_evaluation_anls_score', 0.49112106346003614), ('infographics/answer_type_multi_span_score', 0.39007474218426597), ('infographics/answer_type_non_extractive_score', 0.5509604620816195), ('infographics/answer_type_question_span_score', 0.7153276744622898), ('infographics/answer_type_single_span_score', 0.5647906983169795), ('infographics/evidence_type_figure_score', 0.54006393378161), ('infographics/evidence_type_map_score', 0.49796591220988223), ('infographics/evidence_type_table_list_score', 0.5145635853743483), ('infographics/evidence_type_text_score', 0.6015344858227378), ('infographics/evidence_type_visual_layout_score', 0.5387917275282756), ('infographics/reasoning_type_arithmetic_score', 0.5091819709970394), ('infographics/reasoning_type_comparison_score', 0.4477963166018653), ('infographics/reasoning_type_counting_score', 0.5775058275058275)]\n",
      "Got result for mmbench - 1200: [('mmbench/attribute_comparison', 0.6590909090909091), ('mmbench/attribute_recognition', 0.9014084507042254), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.7494843283432574)]\n",
      "1200\n",
      "Got result for mmmu - 1400: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5333333333333333), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.8), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.7333333333333333), ('mmmu/electronics', 0.5333333333333333), ('mmmu/energy_and_power', 0.5666666666666667), ('mmmu/finance', 0.5666666666666667), ('mmmu/geography', 0.6), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.6), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.8), ('mmmu/accuracy', 0.5944444444444444), ('mmmu/mllm_eval_accuracy', 0.5966666666666667)]\n",
      "Got result for docvqa - 1400: [('docvqa/anls_total_score', 0.6930404464982352), ('docvqa/mllm_evaluation_anls_score', 0.6927990890884685), ('docvqa/mmllm_fixed_anls_score', 0.729133455674974)]\n",
      "Got result for mathvista - 1400: [('mathvista/accuracy', 0.449)]\n",
      "Got result for ai2d - 1400: [('ai2d/accuracy', 0.8189766839378239)]\n",
      "Got result for chartqa - 1400: [('chartqa/accuracy', 0.5360363717944453)]\n",
      "Got result for vqa - 1400: [('vqa/accuracy', 0.7513519999999766), ('vqa/recall', 0.7735159999999754), ('vqa/bleu', 0.03825804218649864), ('vqa/mllm_evaluation_accuracy', 0.7701839999999763)]\n",
      "Got result for textvqa - 1400: [('textvqa/accuracy', 70.01000000000035), ('textvqa/mllm_eval_accuracy', 73.9480000000004)]\n",
      "Got result for infographics_w_ocr - 1400: [('infographics_w_ocr/anls_total_score', 0.6499254459825651), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5994573807403643), ('infographics_w_ocr/answer_type_multi_span_score', 0.5539215005211982), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6378966873994001), ('infographics_w_ocr/answer_type_question_span_score', 0.7172714006707219), ('infographics_w_ocr/answer_type_single_span_score', 0.6611922024724622), ('infographics_w_ocr/evidence_type_figure_score', 0.6279122413414823), ('infographics_w_ocr/evidence_type_map_score', 0.6209732967223848), ('infographics_w_ocr/evidence_type_table_list_score', 0.641478413065964), ('infographics_w_ocr/evidence_type_text_score', 0.701503783852162), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5746796895374385), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6325577675748906), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5264763709147012), ('infographics_w_ocr/reasoning_type_counting_score', 0.6311188811188811)]\n",
      "Got result for infographics - 1400: [('infographics/anls_total_score', 0.5688860273165649), ('infographics/mllm_evaluation_anls_score', 0.5073781456586762), ('infographics/answer_type_multi_span_score', 0.39297840984481774), ('infographics/answer_type_non_extractive_score', 0.5489512195660479), ('infographics/answer_type_question_span_score', 0.7260877477223631), ('infographics/answer_type_single_span_score', 0.5855738858885448), ('infographics/evidence_type_figure_score', 0.555901520109363), ('infographics/evidence_type_map_score', 0.5659534612489847), ('infographics/evidence_type_table_list_score', 0.5360086889679313), ('infographics/evidence_type_text_score', 0.6162989623287427), ('infographics/evidence_type_visual_layout_score', 0.5324573180136688), ('infographics/reasoning_type_arithmetic_score', 0.5050914078311337), ('infographics/reasoning_type_comparison_score', 0.5023171452819319), ('infographics/reasoning_type_counting_score', 0.5932400932400932)]\n",
      "Got result for mmbench - 1400: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.7510641699138133)]\n",
      "1400\n",
      "Got result for mmmu - 1600: [('mmmu/accounting', 0.5333333333333333), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.8), ('mmmu/biology', 0.4666666666666667), ('mmmu/chemistry', 0.5), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.7), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.6333333333333333), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.7666666666666667), ('mmmu/accuracy', 0.5833333333333334), ('mmmu/mllm_eval_accuracy', 0.59)]\n",
      "Got result for docvqa - 1600: [('docvqa/anls_total_score', 0.6895491186186158), ('docvqa/mllm_evaluation_anls_score', 0.6894087841419814), ('docvqa/mmllm_fixed_anls_score', 0.7256799113549901)]\n",
      "Got result for mathvista - 1600: [('mathvista/accuracy', 0.462)]\n",
      "Got result for ai2d - 1600: [('ai2d/accuracy', 0.8167098445595855)]\n",
      "Got result for chartqa - 1600: [('chartqa/accuracy', 0.5228806060184572)]\n",
      "Got result for vqa - 1600: [('vqa/accuracy', 0.7529279999999762), ('vqa/recall', 0.7734919999999755), ('vqa/bleu', 0.04158240929245949), ('vqa/mllm_evaluation_accuracy', 0.7710599999999765)]\n",
      "Got result for textvqa - 1600: [('textvqa/accuracy', 69.82600000000032), ('textvqa/mllm_eval_accuracy', 73.9440000000004)]\n",
      "Got result for infographics_w_ocr - 1600: [('infographics_w_ocr/anls_total_score', 0.6523406873214214), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6013651697734672), ('infographics_w_ocr/answer_type_multi_span_score', 0.5173389492531302), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6550700806125765), ('infographics_w_ocr/answer_type_question_span_score', 0.6839380673373886), ('infographics_w_ocr/answer_type_single_span_score', 0.6647116277133357), ('infographics_w_ocr/evidence_type_figure_score', 0.6346603727706368), ('infographics_w_ocr/evidence_type_map_score', 0.5943101040873318), ('infographics_w_ocr/evidence_type_table_list_score', 0.6483615753947224), ('infographics_w_ocr/evidence_type_text_score', 0.6955143605705562), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5588978694301922), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6600014426213051), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5605359506802862), ('infographics_w_ocr/reasoning_type_counting_score', 0.6375291375291375)]\n",
      "Got result for infographics - 1600: [('infographics/anls_total_score', 0.5537297630445153), ('infographics/mllm_evaluation_anls_score', 0.4899764745382736), ('infographics/answer_type_multi_span_score', 0.36928185097523347), ('infographics/answer_type_non_extractive_score', 0.5412542751602428), ('infographics/answer_type_question_span_score', 0.7004056306940922), ('infographics/answer_type_single_span_score', 0.5701383665801465), ('infographics/evidence_type_figure_score', 0.5448171919394893), ('infographics/evidence_type_map_score', 0.5520504927027933), ('infographics/evidence_type_table_list_score', 0.5141683598078697), ('infographics/evidence_type_text_score', 0.601637879789039), ('infographics/evidence_type_visual_layout_score', 0.4912309098181136), ('infographics/reasoning_type_arithmetic_score', 0.48395073343703465), ('infographics/reasoning_type_comparison_score', 0.46657430730600613), ('infographics/reasoning_type_counting_score', 0.5956293706293705)]\n",
      "Got result for mmbench - 1600: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6266666666666667), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7524386672207198)]\n",
      "1600\n",
      "Got result for mmmu - 1800: [('mmmu/accounting', 0.5333333333333333), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.4666666666666667), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.6333333333333333), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.6), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.4), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.8), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.57), ('mmmu/mllm_eval_accuracy', 0.5822222222222222)]\n",
      "Got result for docvqa - 1800: [('docvqa/anls_total_score', 0.6843588119133742), ('docvqa/mllm_evaluation_anls_score', 0.684458344039418), ('docvqa/mmllm_fixed_anls_score', 0.7224850768908115)]\n",
      "Got result for mathvista - 1800: [('mathvista/accuracy', 0.479)]\n",
      "Got result for ai2d - 1800: [('ai2d/accuracy', 0.8173575129533679)]\n",
      "Got result for vqa - 1800: [('vqa/accuracy', 0.7503119999999752), ('vqa/recall', 0.7713479999999748), ('vqa/bleu', 0.037375714629888535), ('vqa/mllm_evaluation_accuracy', 0.7684439999999758)]\n",
      "Got result for textvqa - 1800: [('textvqa/accuracy', 70.31600000000034), ('textvqa/mllm_eval_accuracy', 74.32400000000037)]\n",
      "Got result for infographics_w_ocr - 1800: [('infographics_w_ocr/anls_total_score', 0.6526072837441547), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5975676709033039), ('infographics_w_ocr/answer_type_multi_span_score', 0.5427162487344895), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6393067450933637), ('infographics_w_ocr/answer_type_question_span_score', 0.714310472709794), ('infographics_w_ocr/answer_type_single_span_score', 0.664294212037485), ('infographics_w_ocr/evidence_type_figure_score', 0.634466514054076), ('infographics_w_ocr/evidence_type_map_score', 0.6252231953964628), ('infographics_w_ocr/evidence_type_table_list_score', 0.6469956922890664), ('infographics_w_ocr/evidence_type_text_score', 0.7085295462550012), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5564597474059932), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6558332078880021), ('infographics_w_ocr/reasoning_type_comparison_score', 0.542371694441681), ('infographics_w_ocr/reasoning_type_counting_score', 0.6127622377622378)]\n",
      "Got result for infographics - 1800: [('infographics/anls_total_score', 0.5585906774640167), ('infographics/mllm_evaluation_anls_score', 0.4935091271938477), ('infographics/answer_type_multi_span_score', 0.3778451767180989), ('infographics/answer_type_non_extractive_score', 0.5463894353586939), ('infographics/answer_type_question_span_score', 0.7421515450361604), ('infographics/answer_type_single_span_score', 0.5719845867047859), ('infographics/evidence_type_figure_score', 0.545387545278541), ('infographics/evidence_type_map_score', 0.5535764732659879), ('infographics/evidence_type_table_list_score', 0.5195161862322201), ('infographics/evidence_type_text_score', 0.5951663128244008), ('infographics/evidence_type_visual_layout_score', 0.5096500748563842), ('infographics/reasoning_type_arithmetic_score', 0.5097717731279374), ('infographics/reasoning_type_comparison_score', 0.48293097685132386), ('infographics/reasoning_type_counting_score', 0.5772144522144521)]\n",
      "Got result for mmbench - 1800: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.5), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6103896103896104), ('mmbench/overall', 0.7516339808169767)]\n",
      "1800\n",
      "Got result for mmmu - 2000: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.5333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.26666666666666666), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.8), ('mmmu/sociology', 0.8), ('mmmu/accuracy', 0.5655555555555556), ('mmmu/mllm_eval_accuracy', 0.5811111111111111)]\n",
      "Got result for docvqa - 2000: [('docvqa/anls_total_score', 0.6899574637869383), ('docvqa/mllm_evaluation_anls_score', 0.6898156482086736), ('docvqa/mmllm_fixed_anls_score', 0.7262972673636102)]\n",
      "Got result for mathvista - 2000: [('mathvista/accuracy', 0.465)]\n",
      "Got result for ai2d - 2000: [('ai2d/accuracy', 0.8189766839378239)]\n",
      "Got result for chartqa - 2000: [('chartqa/accuracy', 0.5427615168847472)]\n",
      "Got result for vqa - 2000: [('vqa/accuracy', 0.7523479999999746), ('vqa/recall', 0.7745559999999742), ('vqa/bleu', 0.03253495320677757), ('vqa/mllm_evaluation_accuracy', 0.771683999999975)]\n",
      "Got result for textvqa - 2000: [('textvqa/accuracy', 70.33400000000033), ('textvqa/mllm_eval_accuracy', 74.4280000000004)]\n",
      "Got result for infographics_w_ocr - 2000: [('infographics_w_ocr/anls_total_score', 0.6562296117789616), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6045113204547179), ('infographics_w_ocr/answer_type_multi_span_score', 0.5408687933693975), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6511757380654309), ('infographics_w_ocr/answer_type_question_span_score', 0.7020241479234692), ('infographics_w_ocr/answer_type_single_span_score', 0.6674771707181895), ('infographics_w_ocr/evidence_type_figure_score', 0.632734958589794), ('infographics_w_ocr/evidence_type_map_score', 0.5919998730642295), ('infographics_w_ocr/evidence_type_table_list_score', 0.6616010535428127), ('infographics_w_ocr/evidence_type_text_score', 0.6993930197273216), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5689288603291461), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6440645541672936), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5584972618671974), ('infographics_w_ocr/reasoning_type_counting_score', 0.6480186480186481)]\n",
      "Got result for infographics - 2000: [('infographics/anls_total_score', 0.5637488889001301), ('infographics/mllm_evaluation_anls_score', 0.49996109506085806), ('infographics/answer_type_multi_span_score', 0.4020599697203107), ('infographics/answer_type_non_extractive_score', 0.5517984928473174), ('infographics/answer_type_question_span_score', 0.7089174180520335), ('infographics/answer_type_single_span_score', 0.5774753394974147), ('infographics/evidence_type_figure_score', 0.5555100740191045), ('infographics/evidence_type_map_score', 0.5700781234310044), ('infographics/evidence_type_table_list_score', 0.5235230656604243), ('infographics/evidence_type_text_score', 0.5978651676113317), ('infographics/evidence_type_visual_layout_score', 0.5138399457639763), ('infographics/reasoning_type_arithmetic_score', 0.5072873283489722), ('infographics/reasoning_type_comparison_score', 0.4857721513532679), ('infographics/reasoning_type_counting_score', 0.5920745920745921)]\n",
      "Got result for mmbench - 2000: [('mmbench/attribute_comparison', 0.6590909090909091), ('mmbench/attribute_recognition', 0.9014084507042254), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.5185185185185185), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7527252344913367)]\n",
      "2000\n",
      "Got result for mmmu - 2200: [('mmmu/accounting', 0.7333333333333333), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7666666666666667), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.6666666666666666), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.7333333333333333), ('mmmu/electronics', 0.3333333333333333), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.6), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.5333333333333333), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.5), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.7666666666666667), ('mmmu/accuracy', 0.5955555555555555), ('mmmu/mllm_eval_accuracy', 0.5966666666666667)]\n",
      "Got result for docvqa - 2200: [('docvqa/anls_total_score', 0.6922914190279911), ('docvqa/mllm_evaluation_anls_score', 0.6911179245164714), ('docvqa/mmllm_fixed_anls_score', 0.7283848687729948)]\n",
      "Got result for mathvista - 2200: [('mathvista/accuracy', 0.479)]\n",
      "Got result for ai2d - 2200: [('ai2d/accuracy', 0.8196243523316062)]\n",
      "Got result for chartqa - 2200: [('chartqa/accuracy', 0.5444108819674306)]\n",
      "Got result for vqa - 2200: [('vqa/accuracy', 0.7515439999999755), ('vqa/recall', 0.7717879999999748), ('vqa/bleu', 0.0398288257420063), ('vqa/mllm_evaluation_accuracy', 0.7689359999999761)]\n",
      "Got result for textvqa - 2200: [('textvqa/accuracy', 69.97200000000032), ('textvqa/mllm_eval_accuracy', 74.03600000000041)]\n",
      "Got result for infographics_w_ocr - 2200: [('infographics_w_ocr/anls_total_score', 0.6550591443045888), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6044840273997697), ('infographics_w_ocr/answer_type_multi_span_score', 0.5582450594392037), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6471497459743394), ('infographics_w_ocr/answer_type_question_span_score', 0.7106474690467903), ('infographics_w_ocr/answer_type_single_span_score', 0.6646520567500551), ('infographics_w_ocr/evidence_type_figure_score', 0.6378259399773384), ('infographics_w_ocr/evidence_type_map_score', 0.6151493308671525), ('infographics_w_ocr/evidence_type_table_list_score', 0.6478554446518198), ('infographics_w_ocr/evidence_type_text_score', 0.6893174769062259), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.575264602052212), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6398646444879319), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5689727738266208), ('infographics_w_ocr/reasoning_type_counting_score', 0.6452797202797204)]\n",
      "Got result for infographics - 2200: [('infographics/anls_total_score', 0.5597182089159649), ('infographics/mllm_evaluation_anls_score', 0.49565477722568685), ('infographics/answer_type_multi_span_score', 0.3724094870637148), ('infographics/answer_type_non_extractive_score', 0.5421696948821723), ('infographics/answer_type_question_span_score', 0.7482183948530102), ('infographics/answer_type_single_span_score', 0.5740709828417373), ('infographics/evidence_type_figure_score', 0.5491495980033484), ('infographics/evidence_type_map_score', 0.5814642620448657), ('infographics/evidence_type_table_list_score', 0.5235315391499971), ('infographics/evidence_type_text_score', 0.5908737240147329), ('infographics/evidence_type_visual_layout_score', 0.5217730708301629), ('infographics/reasoning_type_arithmetic_score', 0.500524570558817), ('infographics/reasoning_type_comparison_score', 0.47951700544005643), ('infographics/reasoning_type_counting_score', 0.5705710955710955)]\n",
      "Got result for mmbench - 2200: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.9014084507042254), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.25), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.742391080083362)]\n",
      "2200\n",
      "Got result for mmmu - 2400: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.23333333333333334), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.5), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.4), ('mmmu/geography', 0.5), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.7333333333333333), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5644444444444444), ('mmmu/mllm_eval_accuracy', 0.5866666666666667)]\n",
      "Got result for docvqa - 2400: [('docvqa/anls_total_score', 0.6955738547753733), ('docvqa/mllm_evaluation_anls_score', 0.6952901555391034), ('docvqa/mmllm_fixed_anls_score', 0.7341761510206308)]\n",
      "Got result for mathvista - 2400: [('mathvista/accuracy', 0.467)]\n",
      "Got result for ai2d - 2400: [('ai2d/accuracy', 0.8251295336787565)]\n",
      "Got result for chartqa - 2400: [('chartqa/accuracy', 0.5427929311938108)]\n",
      "Got result for vqa - 2400: [('vqa/accuracy', 0.7522399999999761), ('vqa/recall', 0.7724039999999743), ('vqa/bleu', 0.0341523103415966), ('vqa/mllm_evaluation_accuracy', 0.7697839999999759)]\n",
      "Got result for textvqa - 2400: [('textvqa/accuracy', 71.00000000000037), ('textvqa/mllm_eval_accuracy', 75.18400000000042)]\n",
      "Got result for infographics_w_ocr - 2400: [('infographics_w_ocr/anls_total_score', 0.6606641518011869), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6086817445911927), ('infographics_w_ocr/answer_type_multi_span_score', 0.5224283464538997), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6460753532182106), ('infographics_w_ocr/answer_type_question_span_score', 0.7320149904143117), ('infographics_w_ocr/answer_type_single_span_score', 0.6742958949726682), ('infographics_w_ocr/evidence_type_figure_score', 0.6417492057183953), ('infographics_w_ocr/evidence_type_map_score', 0.626653338410764), ('infographics_w_ocr/evidence_type_table_list_score', 0.6590362453335669), ('infographics_w_ocr/evidence_type_text_score', 0.7077916775898918), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5628328344128831), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6458207888002406), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5783300207435994), ('infographics_w_ocr/reasoning_type_counting_score', 0.633916083916084)]\n",
      "Got result for infographics - 2400: [('infographics/anls_total_score', 0.560204735984656), ('infographics/mllm_evaluation_anls_score', 0.49773481712398854), ('infographics/answer_type_multi_span_score', 0.39924180236048384), ('infographics/answer_type_non_extractive_score', 0.5462003236867613), ('infographics/answer_type_question_span_score', 0.7151339579224194), ('infographics/answer_type_single_span_score', 0.5739897261324877), ('infographics/evidence_type_figure_score', 0.5543242736495142), ('infographics/evidence_type_map_score', 0.5631220432075974), ('infographics/evidence_type_table_list_score', 0.5212411681694655), ('infographics/evidence_type_text_score', 0.6088891955161908), ('infographics/evidence_type_visual_layout_score', 0.4818020442753523), ('infographics/reasoning_type_arithmetic_score', 0.5174615719136266), ('infographics/reasoning_type_comparison_score', 0.47793978932395753), ('infographics/reasoning_type_counting_score', 0.5670163170163169)]\n",
      "Got result for mmbench - 2400: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9423076923076923), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.5974025974025974), ('mmbench/overall', 0.7429375017100799)]\n",
      "2400\n",
      "Got result for mmmu - 2600: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.5), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.6333333333333333), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.7), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.4), ('mmmu/geography', 0.6), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.26666666666666666), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.5666666666666667), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.57), ('mmmu/mllm_eval_accuracy', 0.5788888888888889)]\n",
      "Got result for docvqa - 2600: [('docvqa/anls_total_score', 0.6954683645192988), ('docvqa/mllm_evaluation_anls_score', 0.6942062397260953), ('docvqa/mmllm_fixed_anls_score', 0.7308686722779693)]\n",
      "Got result for mathvista - 2600: [('mathvista/accuracy', 0.473)]\n",
      "Got result for ai2d - 2600: [('ai2d/accuracy', 0.8309585492227979)]\n",
      "Got result for chartqa - 2600: [('chartqa/accuracy', 0.5408773089242913)]\n",
      "Got result for vqa - 2600: [('vqa/accuracy', 0.7553119999999753), ('vqa/recall', 0.7759119999999748), ('vqa/bleu', 0.03813748061656952), ('vqa/mllm_evaluation_accuracy', 0.773291999999976)]\n",
      "Got result for textvqa - 2600: [('textvqa/accuracy', 70.18000000000038), ('textvqa/mllm_eval_accuracy', 74.16800000000043)]\n",
      "Got result for infographics_w_ocr - 2600: [('infographics_w_ocr/anls_total_score', 0.651164724928565), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6013482421058045), ('infographics_w_ocr/answer_type_multi_span_score', 0.5294426409286509), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6439028873477339), ('infographics_w_ocr/answer_type_question_span_score', 0.7077984661977874), ('infographics_w_ocr/answer_type_single_span_score', 0.6636168014911752), ('infographics_w_ocr/evidence_type_figure_score', 0.6277577706777167), ('infographics_w_ocr/evidence_type_map_score', 0.5909161160398783), ('infographics_w_ocr/evidence_type_table_list_score', 0.6500928113222694), ('infographics_w_ocr/evidence_type_text_score', 0.6991038018012479), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5959493163988981), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6465010161071801), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5506955322634512), ('infographics_w_ocr/reasoning_type_counting_score', 0.6258741258741258)]\n",
      "Got result for infographics - 2600: [('infographics/anls_total_score', 0.5619697467117214), ('infographics/mllm_evaluation_anls_score', 0.4980160659819635), ('infographics/answer_type_multi_span_score', 0.39913251393046323), ('infographics/answer_type_non_extractive_score', 0.5500272682912828), ('infographics/answer_type_question_span_score', 0.7284607753357754), ('infographics/answer_type_single_span_score', 0.5738720440481327), ('infographics/evidence_type_figure_score', 0.547114900359438), ('infographics/evidence_type_map_score', 0.5254432753308097), ('infographics/evidence_type_table_list_score', 0.5310237024243538), ('infographics/evidence_type_text_score', 0.6104759745010804), ('infographics/evidence_type_visual_layout_score', 0.5460960893108907), ('infographics/reasoning_type_arithmetic_score', 0.5014785823005), ('infographics/reasoning_type_comparison_score', 0.4698321601005313), ('infographics/reasoning_type_counting_score', 0.5891608391608392)]\n",
      "Got result for mmbench - 2600: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.55), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8679245283018868), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.4583333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6103896103896104), ('mmbench/overall', 0.7475072074801593)]\n",
      "2600\n",
      "Got result for mmmu - 2800: [('mmmu/accounting', 0.7), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.7333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.5), ('mmmu/geography', 0.6666666666666666), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.5333333333333333), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.8), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5844444444444444), ('mmmu/mllm_eval_accuracy', 0.5944444444444444)]\n",
      "Got result for docvqa - 2800: [('docvqa/anls_total_score', 0.6976693869159347), ('docvqa/mllm_evaluation_anls_score', 0.6977460875410136), ('docvqa/mmllm_fixed_anls_score', 0.736589807646552)]\n",
      "Got result for mathvista - 2800: [('mathvista/accuracy', 0.457)]\n",
      "Got result for ai2d - 2800: [('ai2d/accuracy', 0.8228626943005182)]\n",
      "Got result for chartqa - 2800: [('chartqa/accuracy', 0.5281946093052187)]\n",
      "Got result for vqa - 2800: [('vqa/accuracy', 0.7512839999999753), ('vqa/recall', 0.7739279999999741), ('vqa/bleu', 0.027705319225788116), ('vqa/mllm_evaluation_accuracy', 0.7707359999999759)]\n",
      "Got result for textvqa - 2800: [('textvqa/accuracy', 70.66800000000038), ('textvqa/mllm_eval_accuracy', 74.80000000000044)]\n",
      "Got result for infographics_w_ocr - 2800: [('infographics_w_ocr/anls_total_score', 0.6531590609303768), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6018078191959201), ('infographics_w_ocr/answer_type_multi_span_score', 0.5239336808881732), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6392575628109085), ('infographics_w_ocr/answer_type_question_span_score', 0.7090449049442262), ('infographics_w_ocr/answer_type_single_span_score', 0.6672114646029506), ('infographics_w_ocr/evidence_type_figure_score', 0.6341152637744527), ('infographics_w_ocr/evidence_type_map_score', 0.6179074638233054), ('infographics_w_ocr/evidence_type_table_list_score', 0.656370440292388), ('infographics_w_ocr/evidence_type_text_score', 0.6951664809076201), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5573929783707291), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.628799425460384), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5582325241961964), ('infographics_w_ocr/reasoning_type_counting_score', 0.6413170163170162)]\n",
      "Got result for infographics - 2800: [('infographics/anls_total_score', 0.550622000958938), ('infographics/mllm_evaluation_anls_score', 0.4931451983890792), ('infographics/answer_type_multi_span_score', 0.376389502056889), ('infographics/answer_type_non_extractive_score', 0.5522363310790074), ('infographics/answer_type_question_span_score', 0.6960088522588523), ('infographics/answer_type_single_span_score', 0.5615219833702023), ('infographics/evidence_type_figure_score', 0.5411372190688342), ('infographics/evidence_type_map_score', 0.5812660004604655), ('infographics/evidence_type_table_list_score', 0.5207637487848971), ('infographics/evidence_type_text_score', 0.5824082087759574), ('infographics/evidence_type_visual_layout_score', 0.5091396442761439), ('infographics/reasoning_type_arithmetic_score', 0.5070320471005402), ('infographics/reasoning_type_comparison_score', 0.44567070537587594), ('infographics/reasoning_type_counting_score', 0.5888694638694638)]\n",
      "Got result for mmbench - 2800: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.5), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7571794299865611)]\n",
      "2800\n",
      "Got result for mmmu - 3000: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.6666666666666666), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.6), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.5), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7), ('mmmu/diagnostics_and_laboratory_medicine', 0.3333333333333333), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.4666666666666667), ('mmmu/energy_and_power', 0.6333333333333333), ('mmmu/finance', 0.4), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.7), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.5666666666666667), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.8666666666666667), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5711111111111111), ('mmmu/mllm_eval_accuracy', 0.58)]\n",
      "Got result for docvqa - 3000: [('docvqa/anls_total_score', 0.6854900885286632), ('docvqa/mllm_evaluation_anls_score', 0.6844136228000655), ('docvqa/mmllm_fixed_anls_score', 0.7223934884987383)]\n",
      "Got result for mathvista - 3000: [('mathvista/accuracy', 0.482)]\n",
      "Got result for ai2d - 3000: [('ai2d/accuracy', 0.8244818652849741)]\n",
      "Got result for chartqa - 3000: [('chartqa/accuracy', 0.5383905956706266)]\n",
      "Got result for vqa - 3000: [('vqa/accuracy', 0.7551679999999747), ('vqa/recall', 0.7745479999999736), ('vqa/bleu', 0.0471799261868), ('vqa/mllm_evaluation_accuracy', 0.7725599999999748)]\n",
      "Got result for textvqa - 3000: [('textvqa/accuracy', 70.89600000000037), ('textvqa/mllm_eval_accuracy', 74.85400000000043)]\n",
      "Got result for infographics_w_ocr - 3000: [('infographics_w_ocr/anls_total_score', 0.6471103621065909), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5940251732315768), ('infographics_w_ocr/answer_type_multi_span_score', 0.5305115866254025), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6333591664513909), ('infographics_w_ocr/answer_type_question_span_score', 0.7215754299747512), ('infographics_w_ocr/answer_type_single_span_score', 0.6598633424504501), ('infographics_w_ocr/evidence_type_figure_score', 0.627550796673074), ('infographics_w_ocr/evidence_type_map_score', 0.6357763935734232), ('infographics_w_ocr/evidence_type_table_list_score', 0.6438686695747734), ('infographics_w_ocr/evidence_type_text_score', 0.6974593683302565), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5314446412894178), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6320123939986951), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5410602019159332), ('infographics_w_ocr/reasoning_type_counting_score', 0.6203379953379953)]\n",
      "Got result for infographics - 3000: [('infographics/anls_total_score', 0.5547711623340286), ('infographics/mllm_evaluation_anls_score', 0.4929341511735586), ('infographics/answer_type_multi_span_score', 0.3897428668607784), ('infographics/answer_type_non_extractive_score', 0.5338115772112156), ('infographics/answer_type_question_span_score', 0.7501420587959049), ('infographics/answer_type_single_span_score', 0.5673327543490795), ('infographics/evidence_type_figure_score', 0.5479717393199617), ('infographics/evidence_type_map_score', 0.5369796226814282), ('infographics/evidence_type_table_list_score', 0.5151056857905315), ('infographics/evidence_type_text_score', 0.5893852537548887), ('infographics/evidence_type_visual_layout_score', 0.5121682194999595), ('infographics/reasoning_type_arithmetic_score', 0.4890335691705555), ('infographics/reasoning_type_comparison_score', 0.48278496980994257), ('infographics/reasoning_type_counting_score', 0.574009324009324)]\n",
      "Got result for mmbench - 3000: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.9014084507042254), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6933333333333334), ('mmbench/physical_relation', 0.5), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6753246753246753), ('mmbench/overall', 0.7600706385215489)]\n",
      "3000\n",
      "Got result for mmmu - 3200: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.5666666666666667), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.6), ('mmmu/materials', 0.5), ('mmmu/math', 0.36666666666666664), ('mmmu/mechanical_engineering', 0.23333333333333334), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.7333333333333333), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5566666666666666), ('mmmu/mllm_eval_accuracy', 0.5677777777777778)]\n",
      "Got result for docvqa - 3200: [('docvqa/anls_total_score', 0.6884175614823189), ('docvqa/mllm_evaluation_anls_score', 0.6864299815427379), ('docvqa/mmllm_fixed_anls_score', 0.7239035558099187)]\n",
      "Got result for mathvista - 3200: [('mathvista/accuracy', 0.466)]\n",
      "Got result for ai2d - 3200: [('ai2d/accuracy', 0.8244818652849741)]\n",
      "Got result for chartqa - 3200: [('chartqa/accuracy', 0.5424814537419674)]\n",
      "Got result for vqa - 3200: [('vqa/accuracy', 0.7527399999999749), ('vqa/recall', 0.7726199999999747), ('vqa/bleu', 0.04370268061757088), ('vqa/mllm_evaluation_accuracy', 0.7700399999999754)]\n",
      "Got result for textvqa - 3200: [('textvqa/accuracy', 70.49400000000034), ('textvqa/mllm_eval_accuracy', 74.49600000000042)]\n",
      "Got result for infographics_w_ocr - 3200: [('infographics_w_ocr/anls_total_score', 0.652866522133115), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6020094594467755), ('infographics_w_ocr/answer_type_multi_span_score', 0.5300797866485243), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6575841729096706), ('infographics_w_ocr/answer_type_question_span_score', 0.7048499577345732), ('infographics_w_ocr/answer_type_single_span_score', 0.66191766580243), ('infographics_w_ocr/evidence_type_figure_score', 0.6322301243040545), ('infographics_w_ocr/evidence_type_map_score', 0.6239435937520877), ('infographics_w_ocr/evidence_type_table_list_score', 0.6490357967175435), ('infographics_w_ocr/evidence_type_text_score', 0.6979294761976276), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.572896105671597), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6533471950424002), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5348780193596397), ('infographics_w_ocr/reasoning_type_counting_score', 0.6401515151515151)]\n",
      "Got result for infographics - 3200: [('infographics/anls_total_score', 0.5580369040368354), ('infographics/mllm_evaluation_anls_score', 0.4922397548697688), ('infographics/answer_type_multi_span_score', 0.39734432821950927), ('infographics/answer_type_non_extractive_score', 0.5399067177548191), ('infographics/answer_type_question_span_score', 0.720631398516014), ('infographics/answer_type_single_span_score', 0.5729569186766982), ('infographics/evidence_type_figure_score', 0.5484160959838147), ('infographics/evidence_type_map_score', 0.5818768032989912), ('infographics/evidence_type_table_list_score', 0.5243729994886941), ('infographics/evidence_type_text_score', 0.5880637977591125), ('infographics/evidence_type_visual_layout_score', 0.5027206473250383), ('infographics/reasoning_type_arithmetic_score', 0.5058849825973112), ('infographics/reasoning_type_comparison_score', 0.5128840851798792), ('infographics/reasoning_type_counting_score', 0.5641608391608391)]\n",
      "Got result for mmbench - 3200: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6266666666666667), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.744608635227715)]\n",
      "3200\n",
      "Got result for mmmu - 3400: [('mmmu/accounting', 0.6), ('mmmu/agriculture', 0.6666666666666666), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.3), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.3333333333333333), ('mmmu/economics', 0.5), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.5), ('mmmu/finance', 0.5), ('mmmu/geography', 0.43333333333333335), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.26666666666666666), ('mmmu/math', 0.6), ('mmmu/mechanical_engineering', 0.26666666666666666), ('mmmu/music', 0.36666666666666664), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.5666666666666667), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7666666666666667), ('mmmu/accuracy', 0.5544444444444444), ('mmmu/mllm_eval_accuracy', 0.5688888888888889)]\n",
      "Got result for docvqa - 3400: [('docvqa/anls_total_score', 0.6920595635982579), ('docvqa/mllm_evaluation_anls_score', 0.6919151292039054), ('docvqa/mmllm_fixed_anls_score', 0.731105033051952)]\n",
      "Got result for mathvista - 3400: [('mathvista/accuracy', 0.485)]\n",
      "Got result for ai2d - 3400: [('ai2d/accuracy', 0.819300518134715)]\n",
      "Got result for chartqa - 3400: [('chartqa/accuracy', 0.5320210262027238)]\n",
      "Got result for vqa - 3400: [('vqa/accuracy', 0.7519959999999745), ('vqa/recall', 0.7713239999999741), ('vqa/bleu', 0.04670434445142746), ('vqa/mllm_evaluation_accuracy', 0.7694439999999754)]\n",
      "Got result for textvqa - 3400: [('textvqa/accuracy', 70.15600000000035), ('textvqa/mllm_eval_accuracy', 74.21200000000039)]\n",
      "Got result for infographics_w_ocr - 3400: [('infographics_w_ocr/anls_total_score', 0.6522810064646545), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5990834132139811), ('infographics_w_ocr/answer_type_multi_span_score', 0.5338257551027108), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6467316137750136), ('infographics_w_ocr/answer_type_question_span_score', 0.7310251714097868), ('infographics_w_ocr/answer_type_single_span_score', 0.6637450724193565), ('infographics_w_ocr/evidence_type_figure_score', 0.6315288521593815), ('infographics_w_ocr/evidence_type_map_score', 0.6296949939276671), ('infographics_w_ocr/evidence_type_table_list_score', 0.6554964226747275), ('infographics_w_ocr/evidence_type_text_score', 0.6997925222454932), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5458192272987659), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6551115836218573), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5496486449585314), ('infographics_w_ocr/reasoning_type_counting_score', 0.6255827505827506)]\n",
      "Got result for infographics - 3400: [('infographics/anls_total_score', 0.5578659676448343), ('infographics/mllm_evaluation_anls_score', 0.49840432062246415), ('infographics/answer_type_multi_span_score', 0.38973175378381825), ('infographics/answer_type_non_extractive_score', 0.545390058003078), ('infographics/answer_type_question_span_score', 0.7124659528505682), ('infographics/answer_type_single_span_score', 0.5717619075383957), ('infographics/evidence_type_figure_score', 0.5513982372533459), ('infographics/evidence_type_map_score', 0.5514784063164357), ('infographics/evidence_type_table_list_score', 0.5210992588409312), ('infographics/evidence_type_text_score', 0.5963277370090824), ('infographics/evidence_type_visual_layout_score', 0.5024110793585861), ('infographics/reasoning_type_arithmetic_score', 0.5016576555103951), ('infographics/reasoning_type_comparison_score', 0.47542669233864104), ('infographics/reasoning_type_counting_score', 0.5726107226107225)]\n",
      "Got result for mmbench - 3400: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.625), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7601470565840391)]\n",
      "3400\n",
      "Got result for mmmu - 3600: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.7666666666666667), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.4666666666666667), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.5), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.8), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.6333333333333333), ('mmmu/materials', 0.26666666666666666), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7666666666666667), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5688888888888889), ('mmmu/mllm_eval_accuracy', 0.5744444444444444)]\n",
      "Got result for docvqa - 3600: [('docvqa/anls_total_score', 0.6990456918028961), ('docvqa/mllm_evaluation_anls_score', 0.6984530184091614), ('docvqa/mmllm_fixed_anls_score', 0.7358869951384686)]\n",
      "Got result for mathvista - 3600: [('mathvista/accuracy', 0.487)]\n",
      "Got result for ai2d - 3600: [('ai2d/accuracy', 0.8280440414507773)]\n",
      "Got result for chartqa - 3600: [('chartqa/accuracy', 0.5595985179835642)]\n",
      "Got result for vqa - 3600: [('vqa/accuracy', 0.7528999999999746), ('vqa/recall', 0.7742439999999746), ('vqa/bleu', 0.045805346220731735), ('vqa/mllm_evaluation_accuracy', 0.7712079999999755)]\n",
      "Got result for textvqa - 3600: [('textvqa/accuracy', 69.80800000000033), ('textvqa/mllm_eval_accuracy', 74.09000000000037)]\n",
      "Got result for infographics_w_ocr - 3600: [('infographics_w_ocr/anls_total_score', 0.6530705749222349), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5995033460946686), ('infographics_w_ocr/answer_type_multi_span_score', 0.52920165540935), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6424669965357128), ('infographics_w_ocr/answer_type_question_span_score', 0.7270187611533766), ('infographics_w_ocr/answer_type_single_span_score', 0.6645258146587915), ('infographics_w_ocr/evidence_type_figure_score', 0.6388948497303798), ('infographics_w_ocr/evidence_type_map_score', 0.6112679763451457), ('infographics_w_ocr/evidence_type_table_list_score', 0.6450198037322603), ('infographics_w_ocr/evidence_type_text_score', 0.6987870181203262), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5631503294950186), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6380739123889807), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5482505209949791), ('infographics_w_ocr/reasoning_type_counting_score', 0.6349067599067598)]\n",
      "Got result for infographics - 3600: [('infographics/anls_total_score', 0.5580228017628018), ('infographics/mllm_evaluation_anls_score', 0.497256049589339), ('infographics/answer_type_multi_span_score', 0.36771584580512506), ('infographics/answer_type_non_extractive_score', 0.5543664593393347), ('infographics/answer_type_question_span_score', 0.7458527049873204), ('infographics/answer_type_single_span_score', 0.5690640269545204), ('infographics/evidence_type_figure_score', 0.5465349527564461), ('infographics/evidence_type_map_score', 0.5550476045420114), ('infographics/evidence_type_table_list_score', 0.5219323097454984), ('infographics/evidence_type_text_score', 0.6036297170852951), ('infographics/evidence_type_visual_layout_score', 0.5059497684427389), ('infographics/reasoning_type_arithmetic_score', 0.495426890461137), ('infographics/reasoning_type_comparison_score', 0.4579134140620034), ('infographics/reasoning_type_counting_score', 0.5964452214452214)]\n",
      "Got result for mmbench - 3600: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7522218380507352)]\n",
      "3600\n",
      "Got result for mmmu - 3800: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.4666666666666667), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.6), ('mmmu/biology', 0.4666666666666667), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5333333333333333), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.3), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.16666666666666666), ('mmmu/pharmacy', 0.6), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.7), ('mmmu/sociology', 0.7666666666666667), ('mmmu/accuracy', 0.5633333333333334), ('mmmu/mllm_eval_accuracy', 0.5666666666666667)]\n",
      "Got result for docvqa - 3800: [('docvqa/anls_total_score', 0.6920638334513775), ('docvqa/mllm_evaluation_anls_score', 0.6911548393759033), ('docvqa/mmllm_fixed_anls_score', 0.7283273600620433)]\n",
      "Got result for mathvista - 3800: [('mathvista/accuracy', 0.489)]\n",
      "Got result for ai2d - 3800: [('ai2d/accuracy', 0.8293393782383419)]\n",
      "Got result for chartqa - 3800: [('chartqa/accuracy', 0.5441613461705116)]\n",
      "Got result for vqa - 3800: [('vqa/accuracy', 0.7538279999999754), ('vqa/recall', 0.7744479999999747), ('vqa/bleu', 0.0488370917737484), ('vqa/mllm_evaluation_accuracy', 0.7718399999999757)]\n",
      "Got result for textvqa - 3800: [('textvqa/accuracy', 70.78400000000035), ('textvqa/mllm_eval_accuracy', 74.95200000000042)]\n",
      "Got result for infographics_w_ocr - 3800: [('infographics_w_ocr/anls_total_score', 0.6554426110524546), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6027773076894009), ('infographics_w_ocr/answer_type_multi_span_score', 0.5408017447028147), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6481809179367952), ('infographics_w_ocr/answer_type_question_span_score', 0.6916097257443412), ('infographics_w_ocr/answer_type_single_span_score', 0.6683206807714822), ('infographics_w_ocr/evidence_type_figure_score', 0.640820626600638), ('infographics_w_ocr/evidence_type_map_score', 0.6761582889058135), ('infographics_w_ocr/evidence_type_table_list_score', 0.649744304967638), ('infographics_w_ocr/evidence_type_text_score', 0.7033787063863371), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5647286591219027), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6589408023483363), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5572993120357926), ('infographics_w_ocr/reasoning_type_counting_score', 0.6197552447552448)]\n",
      "Got result for infographics - 3800: [('infographics/anls_total_score', 0.5633542383955912), ('infographics/mllm_evaluation_anls_score', 0.4976516791616558), ('infographics/answer_type_multi_span_score', 0.414230358323126), ('infographics/answer_type_non_extractive_score', 0.5551618213010618), ('infographics/answer_type_question_span_score', 0.70290537453999), ('infographics/answer_type_single_span_score', 0.57483458030508), ('infographics/evidence_type_figure_score', 0.5532512430758209), ('infographics/evidence_type_map_score', 0.5570064591217163), ('infographics/evidence_type_table_list_score', 0.5370266622109579), ('infographics/evidence_type_text_score', 0.605010198597999), ('infographics/evidence_type_visual_layout_score', 0.5024688354562336), ('infographics/reasoning_type_arithmetic_score', 0.5109171935370563), ('infographics/reasoning_type_comparison_score', 0.4692879414209123), ('infographics/reasoning_type_counting_score', 0.594114219114219)]\n",
      "Got result for mmbench - 3800: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.5432098765432098), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.68), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.7547104793823574)]\n",
      "3800\n",
      "Got result for mmmu - 4000: [('mmmu/accounting', 0.6333333333333333), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.9333333333333333), ('mmmu/basic_medical_science', 0.7666666666666667), ('mmmu/biology', 0.4666666666666667), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.4666666666666667), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.6), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.7333333333333333), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.5911111111111111), ('mmmu/mllm_eval_accuracy', 0.5944444444444444)]\n",
      "Got result for docvqa - 4000: [('docvqa/anls_total_score', 0.6860768388956411), ('docvqa/mllm_evaluation_anls_score', 0.68520896948228), ('docvqa/mmllm_fixed_anls_score', 0.7230157412987918)]\n",
      "Got result for mathvista - 4000: [('mathvista/accuracy', 0.488)]\n",
      "Got result for ai2d - 4000: [('ai2d/accuracy', 0.8238341968911918)]\n",
      "Got result for chartqa - 4000: [('chartqa/accuracy', 0.5506320224850985)]\n",
      "Got result for vqa - 4000: [('vqa/accuracy', 0.7519879999999751), ('vqa/recall', 0.7734279999999746), ('vqa/bleu', 0.04753536358475685), ('vqa/mllm_evaluation_accuracy', 0.7707319999999758)]\n",
      "Got result for textvqa - 4000: [('textvqa/accuracy', 70.56600000000037), ('textvqa/mllm_eval_accuracy', 74.54000000000042)]\n",
      "Got result for infographics_w_ocr - 4000: [('infographics_w_ocr/anls_total_score', 0.6530764427705144), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6005057891678327), ('infographics_w_ocr/answer_type_multi_span_score', 0.5368301074996702), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6460586881472962), ('infographics_w_ocr/answer_type_question_span_score', 0.7205833664826878), ('infographics_w_ocr/answer_type_single_span_score', 0.662973870385425), ('infographics_w_ocr/evidence_type_figure_score', 0.6283901732816229), ('infographics_w_ocr/evidence_type_map_score', 0.6383695100279257), ('infographics_w_ocr/evidence_type_table_list_score', 0.6628347362154484), ('infographics_w_ocr/evidence_type_text_score', 0.6970006803570912), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5687935990004084), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6452184516396842), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5562953848434483), ('infographics_w_ocr/reasoning_type_counting_score', 0.6366550116550116)]\n",
      "Got result for infographics - 4000: [('infographics/anls_total_score', 0.5615054244784362), ('infographics/mllm_evaluation_anls_score', 0.4939268392192973), ('infographics/answer_type_multi_span_score', 0.40748464442552734), ('infographics/answer_type_non_extractive_score', 0.5461232107253808), ('infographics/answer_type_question_span_score', 0.7422202263548418), ('infographics/answer_type_single_span_score', 0.5730403589469466), ('infographics/evidence_type_figure_score', 0.5437115338668302), ('infographics/evidence_type_map_score', 0.5281386342968662), ('infographics/evidence_type_table_list_score', 0.5435541824785185), ('infographics/evidence_type_text_score', 0.5969694222943102), ('infographics/evidence_type_visual_layout_score', 0.5145574490088946), ('infographics/reasoning_type_arithmetic_score', 0.4998497792162174), ('infographics/reasoning_type_comparison_score', 0.484593639311937), ('infographics/reasoning_type_counting_score', 0.5952797202797202)]\n",
      "Got result for mmbench - 4000: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.5185185185185185), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.7066666666666667), ('mmbench/physical_relation', 0.4583333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.5974025974025974), ('mmbench/overall', 0.7591957972071346)]\n",
      "4000\n",
      "Got result for mmmu - 4200: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.9), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.7), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.4), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.6333333333333333), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.3), ('mmmu/music', 0.2), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.8333333333333334), ('mmmu/sociology', 0.5666666666666667), ('mmmu/accuracy', 0.5622222222222222), ('mmmu/mllm_eval_accuracy', 0.5722222222222222)]\n",
      "Got result for docvqa - 4200: [('docvqa/anls_total_score', 0.6909663819942671), ('docvqa/mllm_evaluation_anls_score', 0.6890248476177889), ('docvqa/mmllm_fixed_anls_score', 0.7247965743751706)]\n",
      "Got result for mathvista - 4200: [('mathvista/accuracy', 0.472)]\n",
      "Got result for ai2d - 4200: [('ai2d/accuracy', 0.8254533678756477)]\n",
      "Got result for chartqa - 4200: [('chartqa/accuracy', 0.5546165839617534)]\n",
      "Got result for vqa - 4200: [('vqa/accuracy', 0.7541879999999751), ('vqa/recall', 0.7757559999999741), ('vqa/bleu', 0.04487704113125801), ('vqa/mllm_evaluation_accuracy', 0.7731199999999752)]\n",
      "Got result for textvqa - 4200: [('textvqa/accuracy', 70.55200000000039), ('textvqa/mllm_eval_accuracy', 74.56800000000044)]\n",
      "Got result for infographics_w_ocr - 4200: [('infographics_w_ocr/anls_total_score', 0.6554969780193748), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6035238223347938), ('infographics_w_ocr/answer_type_multi_span_score', 0.525866019058106), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6564341023653866), ('infographics_w_ocr/answer_type_question_span_score', 0.7215220111713325), ('infographics_w_ocr/answer_type_single_span_score', 0.6655972338335036), ('infographics_w_ocr/evidence_type_figure_score', 0.6352453504489496), ('infographics_w_ocr/evidence_type_map_score', 0.6253332063975627), ('infographics_w_ocr/evidence_type_table_list_score', 0.6625141628715077), ('infographics_w_ocr/evidence_type_text_score', 0.6996736298509091), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5621706640929052), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6591600180641273), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5464334005626414), ('infographics_w_ocr/reasoning_type_counting_score', 0.6424825174825175)]\n",
      "Got result for infographics - 4200: [('infographics/anls_total_score', 0.554261476452139), ('infographics/mllm_evaluation_anls_score', 0.49264494436226997), ('infographics/answer_type_multi_span_score', 0.39572288306518333), ('infographics/answer_type_non_extractive_score', 0.5356048261563634), ('infographics/answer_type_question_span_score', 0.6922049638395792), ('infographics/answer_type_single_span_score', 0.5686185912607391), ('infographics/evidence_type_figure_score', 0.5418123711190144), ('infographics/evidence_type_map_score', 0.5323349178419168), ('infographics/evidence_type_table_list_score', 0.5283550495801679), ('infographics/evidence_type_text_score', 0.6077979568910364), ('infographics/evidence_type_visual_layout_score', 0.5231948304061107), ('infographics/reasoning_type_arithmetic_score', 0.4914022906317425), ('infographics/reasoning_type_comparison_score', 0.46512953627105036), ('infographics/reasoning_type_counting_score', 0.5635198135198135)]\n",
      "Got result for mmbench - 4200: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8873239436619719), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9423076923076923), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.7066666666666667), ('mmbench/physical_relation', 0.5), ('mmbench/spatial_relationship', 0.5), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7597848193042702)]\n",
      "4200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.5578</td>\n",
       "      <td>0.6852</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.8174</td>\n",
       "      <td>0.5189</td>\n",
       "      <td>0.7468</td>\n",
       "      <td>0.69688</td>\n",
       "      <td>0.6447</td>\n",
       "      <td>0.5587</td>\n",
       "      <td>0.743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.5767</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.5345</td>\n",
       "      <td>0.7513</td>\n",
       "      <td>0.7067</td>\n",
       "      <td>0.6489</td>\n",
       "      <td>0.5459</td>\n",
       "      <td>0.7529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.6835</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.8235</td>\n",
       "      <td>0.5301</td>\n",
       "      <td>0.7491</td>\n",
       "      <td>0.70012</td>\n",
       "      <td>0.6515</td>\n",
       "      <td>0.5574</td>\n",
       "      <td>0.743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.5756</td>\n",
       "      <td>0.5622</td>\n",
       "      <td>0.6862</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.8258</td>\n",
       "      <td>0.5386</td>\n",
       "      <td>0.7527</td>\n",
       "      <td>0.7053</td>\n",
       "      <td>0.6473</td>\n",
       "      <td>0.5585</td>\n",
       "      <td>0.7507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.5511</td>\n",
       "      <td>0.6915</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.5447</td>\n",
       "      <td>0.7517</td>\n",
       "      <td>0.70046</td>\n",
       "      <td>0.6548</td>\n",
       "      <td>0.5557</td>\n",
       "      <td>0.7557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>0.5833</td>\n",
       "      <td>0.5778</td>\n",
       "      <td>0.6882</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.8251</td>\n",
       "      <td>0.5338</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.69956</td>\n",
       "      <td>0.6509</td>\n",
       "      <td>0.5539</td>\n",
       "      <td>0.7495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>0.5967</td>\n",
       "      <td>0.5944</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.7514</td>\n",
       "      <td>0.7001</td>\n",
       "      <td>0.6499</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.7511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.5833</td>\n",
       "      <td>0.6895</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.5229</td>\n",
       "      <td>0.7529</td>\n",
       "      <td>0.69826</td>\n",
       "      <td>0.6523</td>\n",
       "      <td>0.5537</td>\n",
       "      <td>0.7524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>0.5822</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.6844</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.8174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7503</td>\n",
       "      <td>0.70316</td>\n",
       "      <td>0.6526</td>\n",
       "      <td>0.5586</td>\n",
       "      <td>0.7516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.5811</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.5428</td>\n",
       "      <td>0.7523</td>\n",
       "      <td>0.70334</td>\n",
       "      <td>0.6562</td>\n",
       "      <td>0.5637</td>\n",
       "      <td>0.7527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>0.5967</td>\n",
       "      <td>0.5956</td>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.8196</td>\n",
       "      <td>0.5444</td>\n",
       "      <td>0.7515</td>\n",
       "      <td>0.69972</td>\n",
       "      <td>0.6551</td>\n",
       "      <td>0.5597</td>\n",
       "      <td>0.7424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>0.5867</td>\n",
       "      <td>0.5644</td>\n",
       "      <td>0.6956</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.8251</td>\n",
       "      <td>0.5428</td>\n",
       "      <td>0.7522</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.6607</td>\n",
       "      <td>0.5602</td>\n",
       "      <td>0.7429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>0.5789</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.6955</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.7018</td>\n",
       "      <td>0.6512</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.7475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>0.5944</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.6977</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.5282</td>\n",
       "      <td>0.7513</td>\n",
       "      <td>0.70668</td>\n",
       "      <td>0.6532</td>\n",
       "      <td>0.5506</td>\n",
       "      <td>0.7572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.5711</td>\n",
       "      <td>0.6855</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.8245</td>\n",
       "      <td>0.5384</td>\n",
       "      <td>0.7552</td>\n",
       "      <td>0.70896</td>\n",
       "      <td>0.6471</td>\n",
       "      <td>0.5548</td>\n",
       "      <td>0.7601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>0.5678</td>\n",
       "      <td>0.5567</td>\n",
       "      <td>0.6884</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.8245</td>\n",
       "      <td>0.5425</td>\n",
       "      <td>0.7527</td>\n",
       "      <td>0.70494</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.7446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.6921</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.8193</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.70156</td>\n",
       "      <td>0.6523</td>\n",
       "      <td>0.5579</td>\n",
       "      <td>0.7601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>0.5744</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.5596</td>\n",
       "      <td>0.7529</td>\n",
       "      <td>0.69808</td>\n",
       "      <td>0.6531</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.7522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.5633</td>\n",
       "      <td>0.6921</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.8293</td>\n",
       "      <td>0.5442</td>\n",
       "      <td>0.7538</td>\n",
       "      <td>0.70784</td>\n",
       "      <td>0.6554</td>\n",
       "      <td>0.5634</td>\n",
       "      <td>0.7547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.5944</td>\n",
       "      <td>0.5911</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.8238</td>\n",
       "      <td>0.5506</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.70566</td>\n",
       "      <td>0.6531</td>\n",
       "      <td>0.5615</td>\n",
       "      <td>0.7592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5622</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.8255</td>\n",
       "      <td>0.5546</td>\n",
       "      <td>0.7542</td>\n",
       "      <td>0.70552</td>\n",
       "      <td>0.6555</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>0.7598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1  docvqa mathvista    ai2d chartqa     vqa  textvqa  \\\n",
       "200   0.5689  0.5578  0.6852     0.411  0.8174  0.5189  0.7468  0.69688   \n",
       "400   0.5767  0.5544  0.6946      0.42   0.818  0.5345  0.7513   0.7067   \n",
       "600     0.58  0.5689  0.6835     0.432  0.8235  0.5301  0.7491  0.70012   \n",
       "800   0.5756  0.5622  0.6862     0.435  0.8258  0.5386  0.7527   0.7053   \n",
       "1000  0.5667  0.5511  0.6915     0.441  0.8229  0.5447  0.7517  0.70046   \n",
       "1200  0.5833  0.5778  0.6882     0.443  0.8251  0.5338   0.753  0.69956   \n",
       "1400  0.5967  0.5944   0.693     0.449   0.819   0.536  0.7514   0.7001   \n",
       "1600    0.59  0.5833  0.6895     0.462  0.8167  0.5229  0.7529  0.69826   \n",
       "1800  0.5822    0.57  0.6844     0.479  0.8174     NaN  0.7503  0.70316   \n",
       "2000  0.5811  0.5656    0.69     0.465   0.819  0.5428  0.7523  0.70334   \n",
       "2200  0.5967  0.5956  0.6923     0.479  0.8196  0.5444  0.7515  0.69972   \n",
       "2400  0.5867  0.5644  0.6956     0.467  0.8251  0.5428  0.7522     0.71   \n",
       "2600  0.5789    0.57  0.6955     0.473   0.831  0.5409  0.7553   0.7018   \n",
       "2800  0.5944  0.5844  0.6977     0.457  0.8229  0.5282  0.7513  0.70668   \n",
       "3000    0.58  0.5711  0.6855     0.482  0.8245  0.5384  0.7552  0.70896   \n",
       "3200  0.5678  0.5567  0.6884     0.466  0.8245  0.5425  0.7527  0.70494   \n",
       "3400  0.5689  0.5544  0.6921     0.485  0.8193   0.532   0.752  0.70156   \n",
       "3600  0.5744  0.5689   0.699     0.487   0.828  0.5596  0.7529  0.69808   \n",
       "3800  0.5667  0.5633  0.6921     0.489  0.8293  0.5442  0.7538  0.70784   \n",
       "4000  0.5944  0.5911  0.6861     0.488  0.8238  0.5506   0.752  0.70566   \n",
       "4200  0.5722  0.5622   0.691     0.472  0.8255  0.5546  0.7542  0.70552   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "200              0.6447       0.5587   0.743  \n",
       "400              0.6489       0.5459  0.7529  \n",
       "600              0.6515       0.5574   0.743  \n",
       "800              0.6473       0.5585  0.7507  \n",
       "1000             0.6548       0.5557  0.7557  \n",
       "1200             0.6509       0.5539  0.7495  \n",
       "1400             0.6499       0.5689  0.7511  \n",
       "1600             0.6523       0.5537  0.7524  \n",
       "1800             0.6526       0.5586  0.7516  \n",
       "2000             0.6562       0.5637  0.7527  \n",
       "2200             0.6551       0.5597  0.7424  \n",
       "2400             0.6607       0.5602  0.7429  \n",
       "2600             0.6512        0.562  0.7475  \n",
       "2800             0.6532       0.5506  0.7572  \n",
       "3000             0.6471       0.5548  0.7601  \n",
       "3200             0.6529        0.558  0.7446  \n",
       "3400             0.6523       0.5579  0.7601  \n",
       "3600             0.6531        0.558  0.7522  \n",
       "3800             0.6554       0.5634  0.7547  \n",
       "4000             0.6531       0.5615  0.7592  \n",
       "4200             0.6555       0.5543  0.7598  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores_all(\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp28\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 504px exp 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New checkpoints: []\n"
     ]
    }
   ],
   "source": [
    "eval_helper.run_eval_sweep(\n",
    "    output_dir=f\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_504px_64nodes_exp29\",\n",
    "    eval_sbatch=EVAL_SBATCH,\n",
    "    eval_config_dir=EVAL_CONFIG_DIR,\n",
    "    aligner_parent_dir=ALIGNER_CODE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3554613515.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[48], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    benchmarks=[\"mmmu\"]\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# output_dir = \"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_504px_64nodes_exp29\"\n",
    "# c = 5200\n",
    "# eval_helper.run_eval_plan(\n",
    "#     eval_base_sbatch=EVAL_SBATCH,\n",
    "#     aligner_parent_dir=ALIGNER_CODE_DIR,\n",
    "#     eval_config_dir=EVAL_CONFIG_DIR,\n",
    "#     checkpoint_dir=output_dir,\n",
    "#     checkpoints=[c],\n",
    "#     save_eval_jobs=eval_helper.get_eval_jobs_record(output_dir, c),\n",
    "#     benchmarks=[\"mmmu\"],\n",
    "#     rerun_if_exists=True,\n",
    "#     print_cmd=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2600, 2800, 3000, 3200, 3400, 3600, 3800, 4000, 4200, 4400, 4600, 4800, 5000, 5200, 5400, 5600]\n",
      "Got result for mmmu - 200: [('mmmu/accounting', 0.5666666666666667), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.23333333333333334), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.6333333333333333), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.3333333333333333), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.3333333333333333), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.36666666666666664), ('mmmu/geography', 0.6), ('mmmu/history', 0.8), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.6666666666666666), ('mmmu/math', 0.4), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.7333333333333333), ('mmmu/public_health', 0.6), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.5611111111111111), ('mmmu/mllm_eval_accuracy', 0.57)]\n",
      "Got result for docvqa - 200: [('docvqa/anls_total_score', 0.6495830256310844), ('docvqa/mllm_evaluation_anls_score', 0.6469872922545187), ('docvqa/mmllm_fixed_anls_score', 0.6857252307430404)]\n",
      "Got result for mathvista - 200: [('mathvista/accuracy', 0.4)]\n",
      "Got result for ai2d - 200: [('ai2d/accuracy', 0.790479274611399)]\n",
      "Got result for chartqa - 200: [('chartqa/accuracy', 0.5060526165107861)]\n",
      "Got result for vqa - 200: [('vqa/accuracy', 0.7204719999999762), ('vqa/recall', 0.7457039999999735), ('vqa/bleu', 0.029198721051216125), ('vqa/mllm_evaluation_accuracy', 0.7415239999999751)]\n",
      "Got result for textvqa - 200: [('textvqa/accuracy', 67.85800000000033), ('textvqa/mllm_eval_accuracy', 72.73800000000034)]\n",
      "Got result for infographics_w_ocr - 200: [('infographics_w_ocr/anls_total_score', 0.6417567352672021), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5868316950897341), ('infographics_w_ocr/answer_type_multi_span_score', 0.5112691793709985), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6224080771549126), ('infographics_w_ocr/answer_type_question_span_score', 0.7002330468676623), ('infographics_w_ocr/answer_type_single_span_score', 0.6578412616157974), ('infographics_w_ocr/evidence_type_figure_score', 0.6139613211186529), ('infographics_w_ocr/evidence_type_map_score', 0.5459602691038334), ('infographics_w_ocr/evidence_type_table_list_score', 0.6459042438836263), ('infographics_w_ocr/evidence_type_text_score', 0.6885818533558941), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5970985451890847), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.5974600456621004), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5513956938294452), ('infographics_w_ocr/reasoning_type_counting_score', 0.6379325802402724)]\n",
      "Got result for infographics - 200: [('infographics/anls_total_score', 0.538723836810694), ('infographics/mllm_evaluation_anls_score', 0.4736905768449339), ('infographics/answer_type_multi_span_score', 0.36800569673167427), ('infographics/answer_type_non_extractive_score', 0.5396347925733098), ('infographics/answer_type_question_span_score', 0.6801143301143301), ('infographics/answer_type_single_span_score', 0.5507822344652422), ('infographics/evidence_type_figure_score', 0.5238578139446852), ('infographics/evidence_type_map_score', 0.4688112411758306), ('infographics/evidence_type_table_list_score', 0.5064318036152718), ('infographics/evidence_type_text_score', 0.5804956632408937), ('infographics/evidence_type_visual_layout_score', 0.5200176686670512), ('infographics/reasoning_type_arithmetic_score', 0.5033722841085854), ('infographics/reasoning_type_comparison_score', 0.4499362629731356), ('infographics/reasoning_type_counting_score', 0.5586247086247087)]\n",
      "Got result for mmbench - 200: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8354430379746836), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5365853658536586), ('mmbench/image_scene', 0.9423076923076923), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.9166666666666666), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.43209876543209874), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6103896103896104), ('mmbench/overall', 0.7385913771216357)]\n",
      "200\n",
      "Got result for mmmu - 400: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.5), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.4666666666666667), ('mmmu/chemistry', 0.3333333333333333), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.3333333333333333), ('mmmu/energy_and_power', 0.5666666666666667), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.4), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.7666666666666667), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5622222222222222), ('mmmu/mllm_eval_accuracy', 0.5766666666666667)]\n",
      "Got result for docvqa - 400: [('docvqa/anls_total_score', 0.6530401429264384), ('docvqa/mllm_evaluation_anls_score', 0.651843850555476), ('docvqa/mmllm_fixed_anls_score', 0.6907322055026188)]\n",
      "Got result for mathvista - 400: [('mathvista/accuracy', 0.399)]\n",
      "Got result for ai2d - 400: [('ai2d/accuracy', 0.7976036269430051)]\n",
      "Got result for chartqa - 400: [('chartqa/accuracy', 0.5042354272608615)]\n",
      "Got result for vqa - 400: [('vqa/accuracy', 0.7205919999999759), ('vqa/recall', 0.748527999999974), ('vqa/bleu', 0.031290408223867416), ('vqa/mllm_evaluation_accuracy', 0.7439999999999751)]\n",
      "Got result for infographics_w_ocr - 400: [('infographics_w_ocr/anls_total_score', 0.6357860208350451), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5809158440134353), ('infographics_w_ocr/answer_type_multi_span_score', 0.5218413250702971), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6352006944231178), ('infographics_w_ocr/answer_type_question_span_score', 0.7141982483328637), ('infographics_w_ocr/answer_type_single_span_score', 0.6454423843700485), ('infographics_w_ocr/evidence_type_figure_score', 0.614580029836323), ('infographics_w_ocr/evidence_type_map_score', 0.5457952526021832), ('infographics_w_ocr/evidence_type_table_list_score', 0.6271608501099496), ('infographics_w_ocr/evidence_type_text_score', 0.6820887538836291), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.532740988364886), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6203172968583927), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5306197793580959), ('infographics_w_ocr/reasoning_type_counting_score', 0.6322843822843822)]\n",
      "Got result for infographics - 400: [('infographics/anls_total_score', 0.5427317880829673), ('infographics/mllm_evaluation_anls_score', 0.47801485238351193), ('infographics/answer_type_multi_span_score', 0.3815686258100756), ('infographics/answer_type_non_extractive_score', 0.5462774918471123), ('infographics/answer_type_question_span_score', 0.7154587933434087), ('infographics/answer_type_single_span_score', 0.5518344039059011), ('infographics/evidence_type_figure_score', 0.5323851736717112), ('infographics/evidence_type_map_score', 0.4921607454863134), ('infographics/evidence_type_table_list_score', 0.5141096272767598), ('infographics/evidence_type_text_score', 0.5751581031992167), ('infographics/evidence_type_visual_layout_score', 0.51889852737703), ('infographics/reasoning_type_arithmetic_score', 0.5100163915232407), ('infographics/reasoning_type_comparison_score', 0.4585046207948657), ('infographics/reasoning_type_counting_score', 0.5749417249417249)]\n",
      "Got result for mmbench - 400: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.8461538461538461), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.25), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7428649800642853)]\n",
      "400\n",
      "Got result for mmmu - 600: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.5), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.7333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.9), ('mmmu/manage', 0.6333333333333333), ('mmmu/marketing', 0.43333333333333335), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.5666666666666667), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.3333333333333333), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5655555555555556), ('mmmu/mllm_eval_accuracy', 0.5611111111111111)]\n",
      "Got result for docvqa - 600: [('docvqa/anls_total_score', 0.6535500777985616), ('docvqa/mllm_evaluation_anls_score', 0.6506957897964664), ('docvqa/mmllm_fixed_anls_score', 0.6885798965282175)]\n",
      "Got result for mathvista - 600: [('mathvista/accuracy', 0.394)]\n",
      "Got result for ai2d - 600: [('ai2d/accuracy', 0.7976036269430051)]\n",
      "Got result for chartqa - 600: [('chartqa/accuracy', 0.5135100058550608)]\n",
      "Got result for vqa - 600: [('vqa/accuracy', 0.7229319999999767), ('vqa/recall', 0.7503439999999746), ('vqa/bleu', 0.020888933911919594), ('vqa/mllm_evaluation_accuracy', 0.7458599999999757)]\n",
      "Got result for textvqa - 600: [('textvqa/accuracy', 68.19400000000032), ('textvqa/mllm_eval_accuracy', 72.68200000000034)]\n",
      "Got result for infographics_w_ocr - 600: [('infographics_w_ocr/anls_total_score', 0.6392479402845759), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5857553829817784), ('infographics_w_ocr/answer_type_multi_span_score', 0.5051634101605232), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6289700203353007), ('infographics_w_ocr/answer_type_question_span_score', 0.7266753545599699), ('infographics_w_ocr/answer_type_single_span_score', 0.6524559919220193), ('infographics_w_ocr/evidence_type_figure_score', 0.6179324630030353), ('infographics_w_ocr/evidence_type_map_score', 0.5703438839359048), ('infographics_w_ocr/evidence_type_table_list_score', 0.6390752463894918), ('infographics_w_ocr/evidence_type_text_score', 0.6829551169601298), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5376178876642183), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6250699357719904), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5360134399820489), ('infographics_w_ocr/reasoning_type_counting_score', 0.6223776223776224)]\n",
      "Got result for infographics - 600: [('infographics/anls_total_score', 0.5386932103993904), ('infographics/mllm_evaluation_anls_score', 0.4757710348593594), ('infographics/answer_type_multi_span_score', 0.3729209913885873), ('infographics/answer_type_non_extractive_score', 0.5393800493257999), ('infographics/answer_type_question_span_score', 0.7208998800344955), ('infographics/answer_type_single_span_score', 0.5474420568490214), ('infographics/evidence_type_figure_score', 0.5261434780167682), ('infographics/evidence_type_map_score', 0.49220297029702975), ('infographics/evidence_type_table_list_score', 0.5067427128921476), ('infographics/evidence_type_text_score', 0.575476457373868), ('infographics/evidence_type_visual_layout_score', 0.47306912512620447), ('infographics/reasoning_type_arithmetic_score', 0.492387559168381), ('infographics/reasoning_type_comparison_score', 0.46507639715303833), ('infographics/reasoning_type_counting_score', 0.5734848484848484)]\n",
      "Got result for mmbench - 600: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7442571007304652)]\n",
      "600\n",
      "Got result for mmmu - 800: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.5666666666666667), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.3333333333333333), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.4), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.4), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.2), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5466666666666666), ('mmmu/mllm_eval_accuracy', 0.5677777777777778)]\n",
      "Got result for docvqa - 800: [('docvqa/anls_total_score', 0.6537061081166712), ('docvqa/mllm_evaluation_anls_score', 0.6531277008164755), ('docvqa/mmllm_fixed_anls_score', 0.6916390586635989)]\n",
      "Got result for mathvista - 800: [('mathvista/accuracy', 0.385)]\n",
      "Got result for ai2d - 800: [('ai2d/accuracy', 0.7840025906735751)]\n",
      "Got result for chartqa - 800: [('chartqa/accuracy', 0.5162700601193239)]\n",
      "Got result for vqa - 800: [('vqa/accuracy', 0.7203559999999762), ('vqa/recall', 0.7501359999999738), ('vqa/bleu', 0.027356578037142754), ('vqa/mllm_evaluation_accuracy', 0.7447879999999748)]\n",
      "Got result for textvqa - 800: [('textvqa/accuracy', 68.47200000000034), ('textvqa/mllm_eval_accuracy', 73.35600000000038)]\n",
      "Got result for infographics_w_ocr - 800: [('infographics_w_ocr/anls_total_score', 0.6439528259351759), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5915795258960372), ('infographics_w_ocr/answer_type_multi_span_score', 0.5118220539489359), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6372513562386982), ('infographics_w_ocr/answer_type_question_span_score', 0.6971401430394644), ('infographics_w_ocr/answer_type_single_span_score', 0.6580425903876994), ('infographics_w_ocr/evidence_type_figure_score', 0.6267606015445166), ('infographics_w_ocr/evidence_type_map_score', 0.5951587603815326), ('infographics_w_ocr/evidence_type_table_list_score', 0.6293298806281771), ('infographics_w_ocr/evidence_type_text_score', 0.6966204779016284), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5533737650515986), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6199771689497715), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5342023920130963), ('infographics_w_ocr/reasoning_type_counting_score', 0.6386946386946387)]\n",
      "Got result for infographics - 800: [('infographics/anls_total_score', 0.5369839191561582), ('infographics/mllm_evaluation_anls_score', 0.4700193706950964), ('infographics/answer_type_multi_span_score', 0.35534853145727874), ('infographics/answer_type_non_extractive_score', 0.5281932052275634), ('infographics/answer_type_question_span_score', 0.6622765589111743), ('infographics/answer_type_single_span_score', 0.5537642244388425), ('infographics/evidence_type_figure_score', 0.5242930473038965), ('infographics/evidence_type_map_score', 0.48147280762347966), ('infographics/evidence_type_table_list_score', 0.5027871229574455), ('infographics/evidence_type_text_score', 0.5826025182357486), ('infographics/evidence_type_visual_layout_score', 0.4984887759814195), ('infographics/reasoning_type_arithmetic_score', 0.4805622459732048), ('infographics/reasoning_type_comparison_score', 0.4631633918525007), ('infographics/reasoning_type_counting_score', 0.5658508158508159)]\n",
      "Got result for mmbench - 800: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.9166666666666666), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7468070929513186)]\n",
      "800\n",
      "Got result for mmmu - 1000: [('mmmu/accounting', 0.5666666666666667), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.6), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.4666666666666667), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.4), ('mmmu/math', 0.5666666666666667), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.16666666666666666), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.55), ('mmmu/mllm_eval_accuracy', 0.5622222222222222)]\n",
      "Got result for docvqa - 1000: [('docvqa/anls_total_score', 0.6557545682377846), ('docvqa/mllm_evaluation_anls_score', 0.6553350215655798), ('docvqa/mmllm_fixed_anls_score', 0.6949208755070436)]\n",
      "Got result for mathvista - 1000: [('mathvista/accuracy', 0.385)]\n",
      "Got result for ai2d - 1000: [('ai2d/accuracy', 0.7862694300518135)]\n",
      "Got result for chartqa - 1000: [('chartqa/accuracy', 0.5142295791498059)]\n",
      "Got result for vqa - 1000: [('vqa/accuracy', 0.7228479999999766), ('vqa/recall', 0.7516519999999742), ('vqa/bleu', 0.0), ('vqa/mllm_evaluation_accuracy', 0.7467559999999753)]\n",
      "Got result for textvqa - 1000: [('textvqa/accuracy', 68.40600000000032), ('textvqa/mllm_eval_accuracy', 73.06400000000038)]\n",
      "Got result for infographics_w_ocr - 1000: [('infographics_w_ocr/anls_total_score', 0.6394778968051102), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.585778311464479), ('infographics_w_ocr/answer_type_multi_span_score', 0.5259406905423144), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6365671098039996), ('infographics_w_ocr/answer_type_question_span_score', 0.7011389220382434), ('infographics_w_ocr/answer_type_single_span_score', 0.65025767261831), ('infographics_w_ocr/evidence_type_figure_score', 0.6187017793947811), ('infographics_w_ocr/evidence_type_map_score', 0.5393596090378269), ('infographics_w_ocr/evidence_type_table_list_score', 0.6385992350340395), ('infographics_w_ocr/evidence_type_text_score', 0.6886887041036353), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.546674934475172), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6217635104621405), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5487569248181865), ('infographics_w_ocr/reasoning_type_counting_score', 0.6348484848484849)]\n",
      "Got result for infographics - 1000: [('infographics/anls_total_score', 0.540953412839171), ('infographics/mllm_evaluation_anls_score', 0.4760838002795279), ('infographics/answer_type_multi_span_score', 0.38808040999800747), ('infographics/answer_type_non_extractive_score', 0.5381966938024805), ('infographics/answer_type_question_span_score', 0.6995476211822365), ('infographics/answer_type_single_span_score', 0.551688581662162), ('infographics/evidence_type_figure_score', 0.528158620401149), ('infographics/evidence_type_map_score', 0.4862656970031368), ('infographics/evidence_type_table_list_score', 0.5113962034900671), ('infographics/evidence_type_text_score', 0.5822036760674831), ('infographics/evidence_type_visual_layout_score', 0.49351625934215854), ('infographics/reasoning_type_arithmetic_score', 0.4735939669158847), ('infographics/reasoning_type_comparison_score', 0.45793176868347446), ('infographics/reasoning_type_counting_score', 0.5908945221445222)]\n",
      "Got result for mmbench - 1000: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.9166666666666666), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7473465952315352)]\n",
      "1000\n",
      "Got result for mmmu - 1200: [('mmmu/accounting', 0.6), ('mmmu/agriculture', 0.7), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.5), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.5666666666666667), ('mmmu/finance', 0.5), ('mmmu/geography', 0.43333333333333335), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.43333333333333335), ('mmmu/materials', 0.4666666666666667), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.6), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5666666666666667), ('mmmu/mllm_eval_accuracy', 0.5611111111111111)]\n",
      "Got result for docvqa - 1200: [('docvqa/anls_total_score', 0.6666822449720468), ('docvqa/mllm_evaluation_anls_score', 0.6675776777748332), ('docvqa/mmllm_fixed_anls_score', 0.7052436277988019)]\n",
      "Got result for mathvista - 1200: [('mathvista/accuracy', 0.393)]\n",
      "Got result for ai2d - 1200: [('ai2d/accuracy', 0.7985751295336787)]\n",
      "Got result for chartqa - 1200: [('chartqa/accuracy', 0.49781056837702825)]\n",
      "Got result for vqa - 1200: [('vqa/accuracy', 0.7267839999999757), ('vqa/recall', 0.7540719999999742), ('vqa/bleu', 0.04150610417127609), ('vqa/mllm_evaluation_accuracy', 0.7492439999999752)]\n",
      "Got result for textvqa - 1200: [('textvqa/accuracy', 68.28800000000031), ('textvqa/mllm_eval_accuracy', 72.90000000000036)]\n",
      "Got result for infographics_w_ocr - 1200: [('infographics_w_ocr/anls_total_score', 0.642540861510529), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5902508097699339), ('infographics_w_ocr/answer_type_multi_span_score', 0.519094515582677), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6183095536169679), ('infographics_w_ocr/answer_type_question_span_score', 0.6837495303841458), ('infographics_w_ocr/answer_type_single_span_score', 0.6612000210187173), ('infographics_w_ocr/evidence_type_figure_score', 0.6260618461199126), ('infographics_w_ocr/evidence_type_map_score', 0.5641120842853515), ('infographics_w_ocr/evidence_type_table_list_score', 0.6346078432477129), ('infographics_w_ocr/evidence_type_text_score', 0.7042747259384314), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5135610999528867), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6048807642129559), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5452075517949478), ('infographics_w_ocr/reasoning_type_counting_score', 0.6153846153846154)]\n",
      "Got result for infographics - 1200: [('infographics/anls_total_score', 0.5536790899985301), ('infographics/mllm_evaluation_anls_score', 0.488456754854708), ('infographics/answer_type_multi_span_score', 0.39738983384501664), ('infographics/answer_type_non_extractive_score', 0.5363475283005121), ('infographics/answer_type_question_span_score', 0.710994507629123), ('infographics/answer_type_single_span_score', 0.5688423465968724), ('infographics/evidence_type_figure_score', 0.5399301469453179), ('infographics/evidence_type_map_score', 0.5031660476080374), ('infographics/evidence_type_table_list_score', 0.5351993312033504), ('infographics/evidence_type_text_score', 0.5918708568537273), ('infographics/evidence_type_visual_layout_score', 0.4818943246689853), ('infographics/reasoning_type_arithmetic_score', 0.48264902905313856), ('infographics/reasoning_type_comparison_score', 0.4921140707973707), ('infographics/reasoning_type_counting_score', 0.5836829836829835)]\n",
      "Got result for mmbench - 1200: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6933333333333334), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7484603613104175)]\n",
      "1200\n",
      "Got result for mmmu - 1400: [('mmmu/accounting', 0.5666666666666667), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.7), ('mmmu/computer_science', 0.4666666666666667), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.3), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8), ('mmmu/literature', 0.7666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.8), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.5), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5455555555555556), ('mmmu/mllm_eval_accuracy', 0.5455555555555556)]\n",
      "Got result for docvqa - 1400: [('docvqa/anls_total_score', 0.6563397986746698), ('docvqa/mllm_evaluation_anls_score', 0.655986122294391), ('docvqa/mmllm_fixed_anls_score', 0.6963064889528946)]\n",
      "Got result for mathvista - 1400: [('mathvista/accuracy', 0.378)]\n",
      "Got result for ai2d - 1400: [('ai2d/accuracy', 0.788860103626943)]\n",
      "Got result for chartqa - 1400: [('chartqa/accuracy', 0.5107855121183399)]\n",
      "Got result for vqa - 1400: [('vqa/accuracy', 0.7243359999999776), ('vqa/recall', 0.7545919999999755), ('vqa/bleu', 0.03514014557003975), ('vqa/mllm_evaluation_accuracy', 0.7491159999999764)]\n",
      "Got result for textvqa - 1400: [('textvqa/accuracy', 68.56000000000031), ('textvqa/mllm_eval_accuracy', 73.18000000000039)]\n",
      "Got result for infographics_w_ocr - 1400: [('infographics_w_ocr/anls_total_score', 0.6416485403488166), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5869820572743356), ('infographics_w_ocr/answer_type_multi_span_score', 0.5335816488905045), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6382118183203177), ('infographics_w_ocr/answer_type_question_span_score', 0.7134252468245681), ('infographics_w_ocr/answer_type_single_span_score', 0.6506962537516235), ('infographics_w_ocr/evidence_type_figure_score', 0.6235062655776301), ('infographics_w_ocr/evidence_type_map_score', 0.579505766510717), ('infographics_w_ocr/evidence_type_table_list_score', 0.6385246139426438), ('infographics_w_ocr/evidence_type_text_score', 0.6903570261272537), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5278127970300972), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6388623362938429), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5406454301619402), ('infographics_w_ocr/reasoning_type_counting_score', 0.6252913752913752)]\n",
      "Got result for infographics - 1400: [('infographics/anls_total_score', 0.5457488584738306), ('infographics/mllm_evaluation_anls_score', 0.4815835277341221), ('infographics/answer_type_multi_span_score', 0.4087978920773309), ('infographics/answer_type_non_extractive_score', 0.5389497181540581), ('infographics/answer_type_question_span_score', 0.6789009601509601), ('infographics/answer_type_single_span_score', 0.5567380464538944), ('infographics/evidence_type_figure_score', 0.5396220598227183), ('infographics/evidence_type_map_score', 0.5116511883005166), ('infographics/evidence_type_table_list_score', 0.5135996158853101), ('infographics/evidence_type_text_score', 0.5831061492204211), ('infographics/evidence_type_visual_layout_score', 0.4950722685652336), ('infographics/reasoning_type_arithmetic_score', 0.48478034522555075), ('infographics/reasoning_type_comparison_score', 0.45915561230617746), ('infographics/reasoning_type_counting_score', 0.5860139860139859)]\n",
      "Got result for mmbench - 1400: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6266666666666667), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6233766233766234), ('mmbench/overall', 0.7401760911125175)]\n",
      "1400\n",
      "Got result for mmmu - 1600: [('mmmu/accounting', 0.3333333333333333), ('mmmu/agriculture', 0.7), ('mmmu/architecture_and_engineering', 0.23333333333333334), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.6), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.3333333333333333), ('mmmu/clinical_medicine', 0.7), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.7333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.5), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.43333333333333335), ('mmmu/marketing', 0.4), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.5666666666666667), ('mmmu/public_health', 0.7), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5433333333333333), ('mmmu/mllm_eval_accuracy', 0.5566666666666666)]\n",
      "Got result for docvqa - 1600: [('docvqa/anls_total_score', 0.6644287152049572), ('docvqa/mllm_evaluation_anls_score', 0.6651489732682895), ('docvqa/mmllm_fixed_anls_score', 0.7039742230052848)]\n",
      "Got result for mathvista - 1600: [('mathvista/accuracy', 0.391)]\n",
      "Got result for ai2d - 1600: [('ai2d/accuracy', 0.788860103626943)]\n",
      "Got result for chartqa - 1600: [('chartqa/accuracy', 0.5160376921098347)]\n",
      "Got result for vqa - 1600: [('vqa/accuracy', 0.7263239999999763), ('vqa/recall', 0.7578839999999744), ('vqa/bleu', 0.02990030124783516), ('vqa/mllm_evaluation_accuracy', 0.7518879999999756)]\n",
      "Got result for textvqa - 1600: [('textvqa/accuracy', 68.7100000000003), ('textvqa/mllm_eval_accuracy', 73.34400000000038)]\n",
      "Got result for infographics_w_ocr - 1600: [('infographics_w_ocr/anls_total_score', 0.645544818461378), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5924518872416862), ('infographics_w_ocr/answer_type_multi_span_score', 0.5094741207574657), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6328074125994562), ('infographics_w_ocr/answer_type_question_span_score', 0.6901597867944022), ('infographics_w_ocr/answer_type_single_span_score', 0.660766931211908), ('infographics_w_ocr/evidence_type_figure_score', 0.6220284917155121), ('infographics_w_ocr/evidence_type_map_score', 0.604541127189642), ('infographics_w_ocr/evidence_type_table_list_score', 0.6495356211525841), ('infographics_w_ocr/evidence_type_text_score', 0.6856970931209461), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5555270668774287), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6240039925827596), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5475702845410706), ('infographics_w_ocr/reasoning_type_counting_score', 0.6315850815850815)]\n",
      "Got result for infographics - 1600: [('infographics/anls_total_score', 0.5485575916157431), ('infographics/mllm_evaluation_anls_score', 0.4847746305829234), ('infographics/answer_type_multi_span_score', 0.39646311906735465), ('infographics/answer_type_non_extractive_score', 0.5345597440534151), ('infographics/answer_type_question_span_score', 0.6921395331721418), ('infographics/answer_type_single_span_score', 0.563040485444359), ('infographics/evidence_type_figure_score', 0.5364675800152169), ('infographics/evidence_type_map_score', 0.5493875928394837), ('infographics/evidence_type_table_list_score', 0.5224832619964921), ('infographics/evidence_type_text_score', 0.5922814716470384), ('infographics/evidence_type_visual_layout_score', 0.4869774878462529), ('infographics/reasoning_type_arithmetic_score', 0.48691166139796277), ('infographics/reasoning_type_comparison_score', 0.4629275639835126), ('infographics/reasoning_type_counting_score', 0.5696969696969696)]\n",
      "Got result for mmbench - 1600: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6266666666666667), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7419363366275088)]\n",
      "1600\n",
      "Got result for mmmu - 1800: [('mmmu/accounting', 0.3), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.16666666666666666), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.7), ('mmmu/electronics', 0.3), ('mmmu/energy_and_power', 0.5), ('mmmu/finance', 0.5), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.3), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.43333333333333335), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5477777777777778), ('mmmu/mllm_eval_accuracy', 0.57)]\n",
      "Got result for docvqa - 1800: [('docvqa/anls_total_score', 0.6597377859718039), ('docvqa/mllm_evaluation_anls_score', 0.6608365012328522), ('docvqa/mmllm_fixed_anls_score', 0.7001946323368784)]\n",
      "Got result for mathvista - 1800: [('mathvista/accuracy', 0.399)]\n",
      "Got result for ai2d - 1800: [('ai2d/accuracy', 0.7823834196891192)]\n",
      "Got result for chartqa - 1800: [('chartqa/accuracy', 0.5115805827330591)]\n",
      "Got result for vqa - 1800: [('vqa/accuracy', 0.7248839999999763), ('vqa/recall', 0.7542479999999752), ('vqa/bleu', 0.025920256972312927), ('vqa/mllm_evaluation_accuracy', 0.7486319999999764)]\n",
      "Got result for textvqa - 1800: [('textvqa/accuracy', 68.3380000000003), ('textvqa/mllm_eval_accuracy', 73.13800000000033)]\n",
      "Got result for infographics_w_ocr - 1800: [('infographics_w_ocr/anls_total_score', 0.6447720528463113), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5905708527369199), ('infographics_w_ocr/answer_type_multi_span_score', 0.5212980093262578), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6386007502644032), ('infographics_w_ocr/answer_type_question_span_score', 0.7149157103839971), ('infographics_w_ocr/answer_type_single_span_score', 0.6555277668599845), ('infographics_w_ocr/evidence_type_figure_score', 0.6199615174381972), ('infographics_w_ocr/evidence_type_map_score', 0.5975868603343851), ('infographics_w_ocr/evidence_type_table_list_score', 0.6420330154968251), ('infographics_w_ocr/evidence_type_text_score', 0.6962721784146675), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5498681131550947), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6341194574071284), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5359962956259872), ('infographics_w_ocr/reasoning_type_counting_score', 0.6301864801864802)]\n",
      "Got result for infographics - 1800: [('infographics/anls_total_score', 0.5458010811271378), ('infographics/mllm_evaluation_anls_score', 0.47935993297202967), ('infographics/answer_type_multi_span_score', 0.3694972451968833), ('infographics/answer_type_non_extractive_score', 0.540534149394909), ('infographics/answer_type_question_span_score', 0.6780732622078776), ('infographics/answer_type_single_span_score', 0.5600309810146955), ('infographics/evidence_type_figure_score', 0.5314223488129529), ('infographics/evidence_type_map_score', 0.5777701865209145), ('infographics/evidence_type_table_list_score', 0.5160950996981051), ('infographics/evidence_type_text_score', 0.5862377769273421), ('infographics/evidence_type_visual_layout_score', 0.48036207630575106), ('infographics/reasoning_type_arithmetic_score', 0.4882946961714085), ('infographics/reasoning_type_comparison_score', 0.4757687679725792), ('infographics/reasoning_type_counting_score', 0.5842657342657341)]\n",
      "Got result for mmbench - 1800: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8732394366197183), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.5185185185185185), ('mmbench/ocr', 0.8461538461538461), ('mmbench/physical_property_reasoning', 0.6266666666666667), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7436351858406168)]\n",
      "1800\n",
      "Got result for mmmu - 2000: [('mmmu/accounting', 0.5666666666666667), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.43333333333333335), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.7), ('mmmu/computer_science', 0.5333333333333333), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.26666666666666666), ('mmmu/energy_and_power', 0.6333333333333333), ('mmmu/finance', 0.4), ('mmmu/geography', 0.5), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.4), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.7333333333333333), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5644444444444444), ('mmmu/mllm_eval_accuracy', 0.5788888888888889)]\n",
      "Got result for docvqa - 2000: [('docvqa/anls_total_score', 0.66273678661831), ('docvqa/mllm_evaluation_anls_score', 0.6637298314959085), ('docvqa/mmllm_fixed_anls_score', 0.70292356639024)]\n",
      "Got result for mathvista - 2000: [('mathvista/accuracy', 0.385)]\n",
      "Got result for ai2d - 2000: [('ai2d/accuracy', 0.7875647668393783)]\n",
      "Got result for chartqa - 2000: [('chartqa/accuracy', 0.5063251861373689)]\n",
      "Got result for vqa - 2000: [('vqa/accuracy', 0.7249759999999772), ('vqa/recall', 0.755143999999976), ('vqa/bleu', 0.02139463648200035), ('vqa/mllm_evaluation_accuracy', 0.7494079999999768)]\n",
      "Got result for textvqa - 2000: [('textvqa/accuracy', 68.39200000000032), ('textvqa/mllm_eval_accuracy', 72.98000000000036)]\n",
      "Got result for infographics_w_ocr - 2000: [('infographics_w_ocr/anls_total_score', 0.6347373099359277), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.581272769262349), ('infographics_w_ocr/answer_type_multi_span_score', 0.5068534454176334), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6225698653365924), ('infographics_w_ocr/answer_type_question_span_score', 0.688816809285096), ('infographics_w_ocr/answer_type_single_span_score', 0.6493726220959918), ('infographics_w_ocr/evidence_type_figure_score', 0.6154863398941363), ('infographics_w_ocr/evidence_type_map_score', 0.5568513582127443), ('infographics_w_ocr/evidence_type_table_list_score', 0.636313907373136), ('infographics_w_ocr/evidence_type_text_score', 0.6829483886898579), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5272857288973952), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6115792312710119), ('infographics_w_ocr/reasoning_type_comparison_score', 0.536840995382575), ('infographics_w_ocr/reasoning_type_counting_score', 0.6293706293706294)]\n",
      "Got result for infographics - 2000: [('infographics/anls_total_score', 0.542203837663547), ('infographics/mllm_evaluation_anls_score', 0.4772360340540915), ('infographics/answer_type_multi_span_score', 0.37523635948801776), ('infographics/answer_type_non_extractive_score', 0.5298852744603197), ('infographics/answer_type_question_span_score', 0.6862269674769674), ('infographics/answer_type_single_span_score', 0.5564722676378385), ('infographics/evidence_type_figure_score', 0.5352439863490462), ('infographics/evidence_type_map_score', 0.51006600660066), ('infographics/evidence_type_table_list_score', 0.503165617562979), ('infographics/evidence_type_text_score', 0.5732304091640709), ('infographics/evidence_type_visual_layout_score', 0.5370532664588286), ('infographics/reasoning_type_arithmetic_score', 0.4875338702393496), ('infographics/reasoning_type_comparison_score', 0.45329981873412084), ('infographics/reasoning_type_counting_score', 0.5603729603729605)]\n",
      "Got result for mmbench - 2000: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.5308641975308642), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7526309079087813)]\n",
      "2000\n",
      "Got result for mmmu - 2200: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7666666666666667), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.3333333333333333), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.3), ('mmmu/math', 0.6333333333333333), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5622222222222222), ('mmmu/mllm_eval_accuracy', 0.5788888888888889)]\n",
      "Got result for docvqa - 2200: [('docvqa/anls_total_score', 0.6608797607617891), ('docvqa/mllm_evaluation_anls_score', 0.6610003429120236), ('docvqa/mmllm_fixed_anls_score', 0.7000977440114353)]\n",
      "Got result for mathvista - 2200: [('mathvista/accuracy', 0.389)]\n",
      "Got result for ai2d - 2200: [('ai2d/accuracy', 0.7908031088082902)]\n",
      "Got result for chartqa - 2200: [('chartqa/accuracy', 0.5205110278082228)]\n",
      "Got result for vqa - 2200: [('vqa/accuracy', 0.7236719999999766), ('vqa/recall', 0.7564239999999749), ('vqa/bleu', 0.019906148314476013), ('vqa/mllm_evaluation_accuracy', 0.7501839999999766)]\n",
      "Got result for textvqa - 2200: [('textvqa/accuracy', 68.88200000000032), ('textvqa/mllm_eval_accuracy', 73.43200000000036)]\n",
      "Got result for infographics_w_ocr - 2200: [('infographics_w_ocr/anls_total_score', 0.6398971841054668), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5854884721961255), ('infographics_w_ocr/answer_type_multi_span_score', 0.5100465382287374), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6306789804529408), ('infographics_w_ocr/answer_type_question_span_score', 0.7320860400543269), ('infographics_w_ocr/answer_type_single_span_score', 0.6520193459625302), ('infographics_w_ocr/evidence_type_figure_score', 0.6238488484918598), ('infographics_w_ocr/evidence_type_map_score', 0.6038810611830413), ('infographics_w_ocr/evidence_type_table_list_score', 0.6346400065043866), ('infographics_w_ocr/evidence_type_text_score', 0.6859122423004151), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5223046159522092), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6260233202870187), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5279524694308487), ('infographics_w_ocr/reasoning_type_counting_score', 0.6177156177156177)]\n",
      "Got result for infographics - 2200: [('infographics/anls_total_score', 0.5479422535690872), ('infographics/mllm_evaluation_anls_score', 0.48828113061528705), ('infographics/answer_type_multi_span_score', 0.33804927612293495), ('infographics/answer_type_non_extractive_score', 0.5398426166961431), ('infographics/answer_type_question_span_score', 0.6986776578122732), ('infographics/answer_type_single_span_score', 0.5660721833772414), ('infographics/evidence_type_figure_score', 0.5459182070161408), ('infographics/evidence_type_map_score', 0.5246614087836015), ('infographics/evidence_type_table_list_score', 0.5001508567484836), ('infographics/evidence_type_text_score', 0.5853053633936911), ('infographics/evidence_type_visual_layout_score', 0.510738477398478), ('infographics/reasoning_type_arithmetic_score', 0.4920649555923528), ('infographics/reasoning_type_comparison_score', 0.475050140641518), ('infographics/reasoning_type_counting_score', 0.5775058275058275)]\n",
      "Got result for mmbench - 2200: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.9166666666666666), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7488370075904907)]\n",
      "2200\n",
      "Got result for mmmu - 2400: [('mmmu/accounting', 0.5666666666666667), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.23333333333333334), ('mmmu/art', 0.6), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.8333333333333334), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.43333333333333335), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.9), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.6), ('mmmu/mechanical_engineering', 0.3), ('mmmu/music', 0.2), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.7333333333333333), ('mmmu/accuracy', 0.5555555555555556), ('mmmu/mllm_eval_accuracy', 0.5633333333333334)]\n",
      "Got result for docvqa - 2400: [('docvqa/anls_total_score', 0.6567394987415107), ('docvqa/mllm_evaluation_anls_score', 0.6598085210332851), ('docvqa/mmllm_fixed_anls_score', 0.7000851258512037)]\n",
      "Got result for mathvista - 2400: [('mathvista/accuracy', 0.401)]\n",
      "Got result for ai2d - 2400: [('ai2d/accuracy', 0.7927461139896373)]\n",
      "Got result for chartqa - 2400: [('chartqa/accuracy', 0.5261223294648119)]\n",
      "Got result for vqa - 2400: [('vqa/accuracy', 0.7228999999999759), ('vqa/recall', 0.7538319999999747), ('vqa/bleu', 0.03636973723769188), ('vqa/mllm_evaluation_accuracy', 0.7478279999999756)]\n",
      "Got result for textvqa - 2400: [('textvqa/accuracy', 68.83000000000031), ('textvqa/mllm_eval_accuracy', 73.63800000000035)]\n",
      "Got result for infographics_w_ocr - 2400: [('infographics_w_ocr/anls_total_score', 0.6381768189713866), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5841078335166316), ('infographics_w_ocr/answer_type_multi_span_score', 0.5084328145300273), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6202556485106215), ('infographics_w_ocr/answer_type_question_span_score', 0.6804246703782513), ('infographics_w_ocr/answer_type_single_span_score', 0.6541671115289674), ('infographics_w_ocr/evidence_type_figure_score', 0.6160789868256121), ('infographics_w_ocr/evidence_type_map_score', 0.5832539984767707), ('infographics_w_ocr/evidence_type_table_list_score', 0.6346546165280733), ('infographics_w_ocr/evidence_type_text_score', 0.6818428955416979), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5411334760523815), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6089088137889507), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5426397504357874), ('infographics_w_ocr/reasoning_type_counting_score', 0.6188811188811189)]\n",
      "Got result for infographics - 2400: [('infographics/anls_total_score', 0.5500248951976076), ('infographics/mllm_evaluation_anls_score', 0.4844417618708488), ('infographics/answer_type_multi_span_score', 0.3848454741594797), ('infographics/answer_type_non_extractive_score', 0.536746616855116), ('infographics/answer_type_question_span_score', 0.6524968087468087), ('infographics/answer_type_single_span_score', 0.5672067725668424), ('infographics/evidence_type_figure_score', 0.5385779041612196), ('infographics/evidence_type_map_score', 0.5460775920581886), ('infographics/evidence_type_table_list_score', 0.5120592744562743), ('infographics/evidence_type_text_score', 0.594028183164202), ('infographics/evidence_type_visual_layout_score', 0.49248227767437397), ('infographics/reasoning_type_arithmetic_score', 0.5005281248431932), ('infographics/reasoning_type_comparison_score', 0.4845487584980131), ('infographics/reasoning_type_counting_score', 0.5638694638694639)]\n",
      "Got result for mmbench - 2400: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6103896103896104), ('mmbench/overall', 0.745224380373886)]\n",
      "2400\n",
      "Got result for mmmu - 2600: [('mmmu/accounting', 0.6333333333333333), ('mmmu/agriculture', 0.6666666666666666), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.7666666666666667), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.7666666666666667), ('mmmu/biology', 0.3), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.6), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.4), ('mmmu/math', 0.7), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5777777777777777), ('mmmu/mllm_eval_accuracy', 0.5844444444444444)]\n",
      "Got result for docvqa - 2600: [('docvqa/anls_total_score', 0.6614153514712111), ('docvqa/mllm_evaluation_anls_score', 0.6621488409706024), ('docvqa/mmllm_fixed_anls_score', 0.7003421744513758)]\n",
      "Got result for mathvista - 2600: [('mathvista/accuracy', 0.377)]\n",
      "Got result for ai2d - 2600: [('ai2d/accuracy', 0.7963082901554405)]\n",
      "Got result for chartqa - 2600: [('chartqa/accuracy', 0.5070751266774965)]\n",
      "Got result for vqa - 2600: [('vqa/accuracy', 0.7241119999999762), ('vqa/recall', 0.7536639999999741), ('vqa/bleu', 0.0333038829267025), ('vqa/mllm_evaluation_accuracy', 0.7481399999999756)]\n",
      "Got result for textvqa - 2600: [('textvqa/accuracy', 68.59800000000027), ('textvqa/mllm_eval_accuracy', 73.35800000000033)]\n",
      "Got result for infographics_w_ocr - 2600: [('infographics_w_ocr/anls_total_score', 0.6477815852867622), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5924480121571651), ('infographics_w_ocr/answer_type_multi_span_score', 0.5314479916131122), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6477169705921967), ('infographics_w_ocr/answer_type_question_span_score', 0.72278116868049), ('infographics_w_ocr/answer_type_single_span_score', 0.6584820972735885), ('infographics_w_ocr/evidence_type_figure_score', 0.6242356006950783), ('infographics_w_ocr/evidence_type_map_score', 0.5906797410510282), ('infographics_w_ocr/evidence_type_table_list_score', 0.6415104549180196), ('infographics_w_ocr/evidence_type_text_score', 0.6975184861820135), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5574795771460117), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6435073678224361), ('infographics_w_ocr/reasoning_type_comparison_score', 0.552916257560333), ('infographics_w_ocr/reasoning_type_counting_score', 0.6445221445221444)]\n",
      "Got result for infographics - 2600: [('infographics/anls_total_score', 0.5555270521499234), ('infographics/mllm_evaluation_anls_score', 0.4937745898642813), ('infographics/answer_type_multi_span_score', 0.3874528946518073), ('infographics/answer_type_non_extractive_score', 0.5505159999735046), ('infographics/answer_type_question_span_score', 0.7366050082396236), ('infographics/answer_type_single_span_score', 0.5657848432520486), ('infographics/evidence_type_figure_score', 0.5480558490814881), ('infographics/evidence_type_map_score', 0.5183720339833625), ('infographics/evidence_type_table_list_score', 0.5182223841552698), ('infographics/evidence_type_text_score', 0.5927241748502167), ('infographics/evidence_type_visual_layout_score', 0.48063450668381524), ('infographics/reasoning_type_arithmetic_score', 0.504287721411009), ('infographics/reasoning_type_comparison_score', 0.5006709490549481), ('infographics/reasoning_type_counting_score', 0.5898601398601399)]\n",
      "Got result for mmbench - 2600: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.25), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6103896103896104), ('mmbench/overall', 0.7345739420582841)]\n",
      "2600\n",
      "Got result for mmmu - 2800: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.4666666666666667), ('mmmu/architecture_and_engineering', 0.23333333333333334), ('mmmu/art', 0.7666666666666667), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.6), ('mmmu/history', 0.8), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.4), ('mmmu/materials', 0.4), ('mmmu/math', 0.4), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.2), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.5666666666666667), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5466666666666666), ('mmmu/mllm_eval_accuracy', 0.56)]\n",
      "Got result for docvqa - 2800: [('docvqa/anls_total_score', 0.6576092539794274), ('docvqa/mllm_evaluation_anls_score', 0.6578985819156855), ('docvqa/mmllm_fixed_anls_score', 0.6946399861881952)]\n",
      "Got result for mathvista - 2800: [('mathvista/accuracy', 0.379)]\n",
      "Got result for ai2d - 2800: [('ai2d/accuracy', 0.7930699481865285)]\n",
      "Got result for chartqa - 2800: [('chartqa/accuracy', 0.5191829067314322)]\n",
      "Got result for vqa - 2800: [('vqa/accuracy', 0.7198559999999765), ('vqa/recall', 0.7526439999999748), ('vqa/bleu', 0.030162779614329338), ('vqa/mllm_evaluation_accuracy', 0.7464919999999756)]\n",
      "Got result for textvqa - 2800: [('textvqa/accuracy', 68.63000000000032), ('textvqa/mllm_eval_accuracy', 73.07800000000036)]\n",
      "Got result for infographics_w_ocr - 2800: [('infographics_w_ocr/anls_total_score', 0.6432461303776194), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5903527817107579), ('infographics_w_ocr/answer_type_multi_span_score', 0.5140433500971887), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6300223887023164), ('infographics_w_ocr/answer_type_question_span_score', 0.7011031340714208), ('infographics_w_ocr/answer_type_single_span_score', 0.6582944513078912), ('infographics_w_ocr/evidence_type_figure_score', 0.6248735743579219), ('infographics_w_ocr/evidence_type_map_score', 0.5763704529793638), ('infographics_w_ocr/evidence_type_table_list_score', 0.6308431700022095), ('infographics_w_ocr/evidence_type_text_score', 0.7010447216677873), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5377427292728538), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6230675146771036), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5348909932942711), ('infographics_w_ocr/reasoning_type_counting_score', 0.6292540792540794)]\n",
      "Got result for infographics - 2800: [('infographics/anls_total_score', 0.5407010864497882), ('infographics/mllm_evaluation_anls_score', 0.4786583584495822), ('infographics/answer_type_multi_span_score', 0.37924429285242517), ('infographics/answer_type_non_extractive_score', 0.5450446780464866), ('infographics/answer_type_question_span_score', 0.6276189088689089), ('infographics/answer_type_single_span_score', 0.5558732112037154), ('infographics/evidence_type_figure_score', 0.5327997883249858), ('infographics/evidence_type_map_score', 0.5456801590102726), ('infographics/evidence_type_table_list_score', 0.5012194785850608), ('infographics/evidence_type_text_score', 0.5824815655361011), ('infographics/evidence_type_visual_layout_score', 0.4910753376999128), ('infographics/reasoning_type_arithmetic_score', 0.49723643935972694), ('infographics/reasoning_type_comparison_score', 0.4496885436790345), ('infographics/reasoning_type_counting_score', 0.5761072261072261)]\n",
      "Got result for mmbench - 2800: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7494603003181394)]\n",
      "2800\n",
      "Got result for mmmu - 3000: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.8333333333333334), ('mmmu/diagnostics_and_laboratory_medicine', 0.26666666666666666), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.3), ('mmmu/energy_and_power', 0.36666666666666664), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.43333333333333335), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.3333333333333333), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.6), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.5377777777777778), ('mmmu/mllm_eval_accuracy', 0.5533333333333333)]\n",
      "Got result for docvqa - 3000: [('docvqa/anls_total_score', 0.6675389336502594), ('docvqa/mllm_evaluation_anls_score', 0.6670058823889791), ('docvqa/mmllm_fixed_anls_score', 0.7038142170856171)]\n",
      "Got result for mathvista - 3000: [('mathvista/accuracy', 0.396)]\n",
      "Got result for ai2d - 3000: [('ai2d/accuracy', 0.8021373056994818)]\n",
      "Got result for chartqa - 3000: [('chartqa/accuracy', 0.5111770693124363)]\n",
      "Got result for vqa - 3000: [('vqa/accuracy', 0.7219119999999772), ('vqa/recall', 0.7544839999999758), ('vqa/bleu', 0.023971902206540108), ('vqa/mllm_evaluation_accuracy', 0.7483159999999767)]\n",
      "Got result for textvqa - 3000: [('textvqa/accuracy', 68.87000000000035), ('textvqa/mllm_eval_accuracy', 73.40600000000035)]\n",
      "Got result for infographics_w_ocr - 3000: [('infographics_w_ocr/anls_total_score', 0.6446101084230992), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5928037659059767), ('infographics_w_ocr/answer_type_multi_span_score', 0.5228958089145251), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6350639866462653), ('infographics_w_ocr/answer_type_question_span_score', 0.7176629631312499), ('infographics_w_ocr/answer_type_single_span_score', 0.6574136532554866), ('infographics_w_ocr/evidence_type_figure_score', 0.6245611723580851), ('infographics_w_ocr/evidence_type_map_score', 0.5949701700939325), ('infographics_w_ocr/evidence_type_table_list_score', 0.6345951714660947), ('infographics_w_ocr/evidence_type_text_score', 0.696078941680929), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5669554858762702), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6160629610115909), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5498987113582431), ('infographics_w_ocr/reasoning_type_counting_score', 0.640909090909091)]\n",
      "Got result for infographics - 3000: [('infographics/anls_total_score', 0.5518273098764847), ('infographics/mllm_evaluation_anls_score', 0.4886489546181942), ('infographics/answer_type_multi_span_score', 0.3806538753865866), ('infographics/answer_type_non_extractive_score', 0.544644595910419), ('infographics/answer_type_question_span_score', 0.6941634754134755), ('infographics/answer_type_single_span_score', 0.5650126401037908), ('infographics/evidence_type_figure_score', 0.5393181677077642), ('infographics/evidence_type_map_score', 0.5132045514584225), ('infographics/evidence_type_table_list_score', 0.5212356875123613), ('infographics/evidence_type_text_score', 0.5886987115850618), ('infographics/evidence_type_visual_layout_score', 0.5079418433221379), ('infographics/reasoning_type_arithmetic_score', 0.497791534949069), ('infographics/reasoning_type_comparison_score', 0.49337098082464814), ('infographics/reasoning_type_counting_score', 0.5833333333333333)]\n",
      "Got result for mmbench - 3000: [('mmbench/attribute_comparison', 0.6363636363636364), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.5185185185185185), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7489372852793508)]\n",
      "3000\n",
      "Got result for mmmu - 3200: [('mmmu/accounting', 0.5333333333333333), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.9), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.3333333333333333), ('mmmu/economics', 0.6666666666666666), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.4), ('mmmu/geography', 0.6), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.6), ('mmmu/materials', 0.4), ('mmmu/math', 0.6), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.43333333333333335), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.5333333333333333), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5777777777777777), ('mmmu/mllm_eval_accuracy', 0.5911111111111111)]\n",
      "Got result for docvqa - 3200: [('docvqa/anls_total_score', 0.6668043199844602), ('docvqa/mllm_evaluation_anls_score', 0.6671739939024892), ('docvqa/mmllm_fixed_anls_score', 0.7074423279221408)]\n",
      "Got result for mathvista - 3200: [('mathvista/accuracy', 0.401)]\n",
      "Got result for ai2d - 3200: [('ai2d/accuracy', 0.7995466321243523)]\n",
      "Got result for chartqa - 3200: [('chartqa/accuracy', 0.5038350988271535)]\n",
      "Got result for vqa - 3200: [('vqa/accuracy', 0.721971999999977), ('vqa/recall', 0.7553239999999753), ('vqa/bleu', 0.03437836095690727), ('vqa/mllm_evaluation_accuracy', 0.7492839999999762)]\n",
      "Got result for textvqa - 3200: [('textvqa/accuracy', 68.17400000000032), ('textvqa/mllm_eval_accuracy', 72.96400000000034)]\n",
      "Got result for infographics_w_ocr - 3200: [('infographics_w_ocr/anls_total_score', 0.6399957796418847), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5875320385713073), ('infographics_w_ocr/answer_type_multi_span_score', 0.5214146195718073), ('infographics_w_ocr/answer_type_non_extractive_score', 0.628612662202174), ('infographics_w_ocr/answer_type_question_span_score', 0.6912206554389423), ('infographics_w_ocr/answer_type_single_span_score', 0.6529593100304882), ('infographics_w_ocr/evidence_type_figure_score', 0.6193385185432044), ('infographics_w_ocr/evidence_type_map_score', 0.5922061436912922), ('infographics_w_ocr/evidence_type_table_list_score', 0.6339081268341268), ('infographics_w_ocr/evidence_type_text_score', 0.6836084045235195), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5338564807472171), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6158315143760348), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5386260147069619), ('infographics_w_ocr/reasoning_type_counting_score', 0.6276223776223776)]\n",
      "Got result for infographics - 3200: [('infographics/anls_total_score', 0.5455206499400604), ('infographics/mllm_evaluation_anls_score', 0.4784125785563937), ('infographics/answer_type_multi_span_score', 0.3624290450849336), ('infographics/answer_type_non_extractive_score', 0.5468549834734285), ('infographics/answer_type_question_span_score', 0.7024191086691085), ('infographics/answer_type_single_span_score', 0.5565564367192397), ('infographics/evidence_type_figure_score', 0.531692471177286), ('infographics/evidence_type_map_score', 0.4944379732090856), ('infographics/evidence_type_table_list_score', 0.5096768395368405), ('infographics/evidence_type_text_score', 0.5890983485165469), ('infographics/evidence_type_visual_layout_score', 0.5115119832317757), ('infographics/reasoning_type_arithmetic_score', 0.4971260474685132), ('infographics/reasoning_type_comparison_score', 0.46146287095957983), ('infographics/reasoning_type_counting_score', 0.581002331002331)]\n",
      "Got result for mmbench - 3200: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6585365853658537), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9433962264150944), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.8461538461538461), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7486606666521343)]\n",
      "3200\n",
      "Got result for mmmu - 3400: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.7666666666666667), ('mmmu/art_theory', 0.9), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.7), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.3), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.6), ('mmmu/materials', 0.26666666666666666), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.5), ('mmmu/music', 0.2), ('mmmu/pharmacy', 0.6), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.5666666666666667), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5544444444444444), ('mmmu/mllm_eval_accuracy', 0.5622222222222222)]\n",
      "Got result for docvqa - 3400: [('docvqa/anls_total_score', 0.6571367037036837), ('docvqa/mllm_evaluation_anls_score', 0.6575138629229147), ('docvqa/mmllm_fixed_anls_score', 0.695297683520854)]\n",
      "Got result for mathvista - 3400: [('mathvista/accuracy', 0.395)]\n",
      "Got result for ai2d - 3400: [('ai2d/accuracy', 0.7943652849740933)]\n",
      "Got result for chartqa - 3400: [('chartqa/accuracy', 0.513004930954913)]\n",
      "Got result for vqa - 3400: [('vqa/accuracy', 0.7205319999999775), ('vqa/recall', 0.7554439999999755), ('vqa/bleu', 0.025383716449141502), ('vqa/mllm_evaluation_accuracy', 0.7492119999999768)]\n",
      "Got result for textvqa - 3400: [('textvqa/accuracy', 67.88400000000028), ('textvqa/mllm_eval_accuracy', 72.42200000000031)]\n",
      "Got result for infographics_w_ocr - 3400: [('infographics_w_ocr/anls_total_score', 0.6448748892668578), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5896103140842558), ('infographics_w_ocr/answer_type_multi_span_score', 0.5195243754011822), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6306509945750454), ('infographics_w_ocr/answer_type_question_span_score', 0.721598323747645), ('infographics_w_ocr/answer_type_single_span_score', 0.6580141209394412), ('infographics_w_ocr/evidence_type_figure_score', 0.6294635786604383), ('infographics_w_ocr/evidence_type_map_score', 0.5981054836252856), ('infographics_w_ocr/evidence_type_table_list_score', 0.6340304403282954), ('infographics_w_ocr/evidence_type_text_score', 0.6802386217047215), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5473633013982188), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.616267123287671), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5547059299924945), ('infographics_w_ocr/reasoning_type_counting_score', 0.6363636363636362)]\n",
      "Got result for infographics - 3400: [('infographics/anls_total_score', 0.547064229591621), ('infographics/mllm_evaluation_anls_score', 0.48256186440605964), ('infographics/answer_type_multi_span_score', 0.37214943207098644), ('infographics/answer_type_non_extractive_score', 0.5399314870381781), ('infographics/answer_type_question_span_score', 0.6810377122877121), ('infographics/answer_type_single_span_score', 0.5620260007854009), ('infographics/evidence_type_figure_score', 0.5342178626598617), ('infographics/evidence_type_map_score', 0.5704471125845617), ('infographics/evidence_type_table_list_score', 0.5166732288139875), ('infographics/evidence_type_text_score', 0.5864220775084555), ('infographics/evidence_type_visual_layout_score', 0.5194077892951261), ('infographics/reasoning_type_arithmetic_score', 0.4814455901784668), ('infographics/reasoning_type_comparison_score', 0.4752465050390914), ('infographics/reasoning_type_counting_score', 0.5885780885780885)]\n",
      "Got result for mmbench - 3400: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9433962264150944), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7485162847607583)]\n",
      "3400\n",
      "Got result for mmmu - 3600: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7666666666666667), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.7666666666666667), ('mmmu/biology', 0.3), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6666666666666666), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.7333333333333333), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.36666666666666664), ('mmmu/finance', 0.4), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6), ('mmmu/sociology', 0.5666666666666667), ('mmmu/accuracy', 0.5544444444444444), ('mmmu/mllm_eval_accuracy', 0.5655555555555556)]\n",
      "Got result for docvqa - 3600: [('docvqa/anls_total_score', 0.6545976924978203), ('docvqa/mllm_evaluation_anls_score', 0.6559092161767598), ('docvqa/mmllm_fixed_anls_score', 0.6959292414744735)]\n",
      "Got result for mathvista - 3600: [('mathvista/accuracy', 0.398)]\n",
      "Got result for ai2d - 3600: [('ai2d/accuracy', 0.7814119170984456)]\n",
      "Got result for chartqa - 3600: [('chartqa/accuracy', 0.5149066837840831)]\n",
      "Got result for vqa - 3600: [('vqa/accuracy', 0.723723999999977), ('vqa/recall', 0.7564119999999747), ('vqa/bleu', 0.02895221672952175), ('vqa/mllm_evaluation_accuracy', 0.7501399999999764)]\n",
      "Got result for textvqa - 3600: [('textvqa/accuracy', 68.43400000000028), ('textvqa/mllm_eval_accuracy', 73.04400000000031)]\n",
      "Got result for infographics_w_ocr - 3600: [('infographics_w_ocr/anls_total_score', 0.6421557830312206), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5886864241143374), ('infographics_w_ocr/answer_type_multi_span_score', 0.527215277215585), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6263217945406012), ('infographics_w_ocr/answer_type_question_span_score', 0.6999637083630297), ('infographics_w_ocr/answer_type_single_span_score', 0.65636426673355), ('infographics_w_ocr/evidence_type_figure_score', 0.6190421179298456), ('infographics_w_ocr/evidence_type_map_score', 0.6003768872362348), ('infographics_w_ocr/evidence_type_table_list_score', 0.6390721074493145), ('infographics_w_ocr/evidence_type_text_score', 0.6949629418773297), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5680084365771081), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6103514350945857), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5363072879935318), ('infographics_w_ocr/reasoning_type_counting_score', 0.6298368298368298)]\n",
      "Got result for infographics - 3600: [('infographics/anls_total_score', 0.5415454273201623), ('infographics/mllm_evaluation_anls_score', 0.4771830062697623), ('infographics/answer_type_multi_span_score', 0.3614334118237638), ('infographics/answer_type_non_extractive_score', 0.5408045691499582), ('infographics/answer_type_question_span_score', 0.7062326028672182), ('infographics/answer_type_single_span_score', 0.5521575725254528), ('infographics/evidence_type_figure_score', 0.5315320182987131), ('infographics/evidence_type_map_score', 0.5285710018965697), ('infographics/evidence_type_table_list_score', 0.5035606161329714), ('infographics/evidence_type_text_score', 0.585516194961434), ('infographics/evidence_type_visual_layout_score', 0.505721217806479), ('infographics/reasoning_type_arithmetic_score', 0.49719723769381285), ('infographics/reasoning_type_comparison_score', 0.4732025910572443), ('infographics/reasoning_type_counting_score', 0.5751748251748252)]\n",
      "Got result for mmbench - 3600: [('mmbench/attribute_comparison', 0.5454545454545454), ('mmbench/attribute_recognition', 0.8591549295774648), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8481012658227848), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6623376623376623), ('mmbench/overall', 0.7485085732668024)]\n",
      "3600\n",
      "Got result for mmmu - 3800: [('mmmu/accounting', 0.5333333333333333), ('mmmu/agriculture', 0.6333333333333333), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.9333333333333333), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.4666666666666667), ('mmmu/finance', 0.4), ('mmmu/geography', 0.7), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.43333333333333335), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.5333333333333333), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5588888888888889), ('mmmu/mllm_eval_accuracy', 0.5733333333333334)]\n",
      "Got result for docvqa - 3800: [('docvqa/anls_total_score', 0.6680113316043674), ('docvqa/mllm_evaluation_anls_score', 0.6687727302491691), ('docvqa/mmllm_fixed_anls_score', 0.7074970336336047)]\n",
      "Got result for mathvista - 3800: [('mathvista/accuracy', 0.391)]\n",
      "Got result for ai2d - 3800: [('ai2d/accuracy', 0.7862694300518135)]\n",
      "Got result for chartqa - 3800: [('chartqa/accuracy', 0.4976599822037738)]\n",
      "Got result for vqa - 3800: [('vqa/accuracy', 0.7203999999999772), ('vqa/recall', 0.7548679999999742), ('vqa/bleu', 0.031595051288604736), ('vqa/mllm_evaluation_accuracy', 0.7478559999999761)]\n",
      "Got result for textvqa - 3800: [('textvqa/accuracy', 68.95800000000031), ('textvqa/mllm_eval_accuracy', 73.43000000000035)]\n",
      "Got result for infographics_w_ocr - 3800: [('infographics_w_ocr/anls_total_score', 0.648099956695732), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5937303797217145), ('infographics_w_ocr/answer_type_multi_span_score', 0.5340222583980586), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6381146129337814), ('infographics_w_ocr/answer_type_question_span_score', 0.7297485068978282), ('infographics_w_ocr/answer_type_single_span_score', 0.6594203293797183), ('infographics_w_ocr/evidence_type_figure_score', 0.6255002477740467), ('infographics_w_ocr/evidence_type_map_score', 0.5784296925167629), ('infographics_w_ocr/evidence_type_table_list_score', 0.6440797118271945), ('infographics_w_ocr/evidence_type_text_score', 0.6975914174550257), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5468874977904654), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6309727658186561), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5508412452095435), ('infographics_w_ocr/reasoning_type_counting_score', 0.6322843822843822)]\n",
      "Got result for infographics - 3800: [('infographics/anls_total_score', 0.5476046834558332), ('infographics/mllm_evaluation_anls_score', 0.4821801840739889), ('infographics/answer_type_multi_span_score', 0.3521166840270692), ('infographics/answer_type_non_extractive_score', 0.5437561132859506), ('infographics/answer_type_question_span_score', 0.6971015096015096), ('infographics/answer_type_single_span_score', 0.5617916515586124), ('infographics/evidence_type_figure_score', 0.539301028811701), ('infographics/evidence_type_map_score', 0.5149467279385139), ('infographics/evidence_type_table_list_score', 0.5171158666605415), ('infographics/evidence_type_text_score', 0.5911437471175707), ('infographics/evidence_type_visual_layout_score', 0.508492950899289), ('infographics/reasoning_type_arithmetic_score', 0.500104328700219), ('infographics/reasoning_type_comparison_score', 0.4616824365729328), ('infographics/reasoning_type_counting_score', 0.5691142191142191)]\n",
      "Got result for mmbench - 3800: [('mmbench/attribute_comparison', 0.5454545454545454), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9696969696969697), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7445588610045731)]\n",
      "3800\n",
      "Got result for mmmu - 4000: [('mmmu/accounting', 0.43333333333333335), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.9), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.7), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.3), ('mmmu/energy_and_power', 0.5), ('mmmu/finance', 0.4), ('mmmu/geography', 0.43333333333333335), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.4), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.5666666666666667), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.5333333333333333), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5477777777777778), ('mmmu/mllm_eval_accuracy', 0.5588888888888889)]\n",
      "Got result for docvqa - 4000: [('docvqa/anls_total_score', 0.6557621132006228), ('docvqa/mllm_evaluation_anls_score', 0.6560735129820258), ('docvqa/mmllm_fixed_anls_score', 0.695124405435173)]\n",
      "Got result for mathvista - 4000: [('mathvista/accuracy', 0.406)]\n",
      "Got result for ai2d - 4000: [('ai2d/accuracy', 0.7891839378238342)]\n",
      "Got result for chartqa - 4000: [('chartqa/accuracy', 0.5116327829078666)]\n",
      "Got result for vqa - 4000: [('vqa/accuracy', 0.7222039999999758), ('vqa/recall', 0.7545719999999739), ('vqa/bleu', 0.03642560541629791), ('vqa/mllm_evaluation_accuracy', 0.7482519999999753)]\n",
      "Got result for textvqa - 4000: [('textvqa/accuracy', 68.50800000000031), ('textvqa/mllm_eval_accuracy', 73.14200000000034)]\n",
      "Got result for infographics_w_ocr - 4000: [('infographics_w_ocr/anls_total_score', 0.6389975943712692), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5863910260478227), ('infographics_w_ocr/answer_type_multi_span_score', 0.5119515989913251), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6424660029542492), ('infographics_w_ocr/answer_type_question_span_score', 0.7101080180763049), ('infographics_w_ocr/answer_type_single_span_score', 0.6484847301500658), ('infographics_w_ocr/evidence_type_figure_score', 0.613995913071836), ('infographics_w_ocr/evidence_type_map_score', 0.5626269357705002), ('infographics_w_ocr/evidence_type_table_list_score', 0.6414752082186277), ('infographics_w_ocr/evidence_type_text_score', 0.6823618586316191), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5631411658831864), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6300811631291081), ('infographics_w_ocr/reasoning_type_comparison_score', 0.546440538957032), ('infographics_w_ocr/reasoning_type_counting_score', 0.6421911421911422)]\n",
      "Got result for infographics - 4000: [('infographics/anls_total_score', 0.5384296612575069), ('infographics/mllm_evaluation_anls_score', 0.47213753877123976), ('infographics/answer_type_multi_span_score', 0.3679597505782803), ('infographics/answer_type_non_extractive_score', 0.5372776309485173), ('infographics/answer_type_question_span_score', 0.6940607469453622), ('infographics/answer_type_single_span_score', 0.5490329884914343), ('infographics/evidence_type_figure_score', 0.528725017100666), ('infographics/evidence_type_map_score', 0.47801883129489414), ('infographics/evidence_type_table_list_score', 0.5081951649981714), ('infographics/evidence_type_text_score', 0.5774294667146701), ('infographics/evidence_type_visual_layout_score', 0.5039041716110937), ('infographics/reasoning_type_arithmetic_score', 0.48800638098583293), ('infographics/reasoning_type_comparison_score', 0.49954043583597935), ('infographics/reasoning_type_counting_score', 0.5842657342657341)]\n",
      "Got result for mmbench - 4000: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9595959595959596), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.65), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9433962264150944), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7447420851948919)]\n",
      "4000\n",
      "Got result for mmmu - 4200: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5333333333333333), ('mmmu/architecture_and_engineering', 0.23333333333333334), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.8333333333333334), ('mmmu/diagnostics_and_laboratory_medicine', 0.3), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.5333333333333333), ('mmmu/geography', 0.4), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.36666666666666664), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.3), ('mmmu/music', 0.43333333333333335), ('mmmu/pharmacy', 0.6), ('mmmu/physics', 0.5333333333333333), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5422222222222223), ('mmmu/mllm_eval_accuracy', 0.5611111111111111)]\n",
      "Got result for docvqa - 4200: [('docvqa/anls_total_score', 0.6618243087054139), ('docvqa/mllm_evaluation_anls_score', 0.661151300884587), ('docvqa/mmllm_fixed_anls_score', 0.700466679166316)]\n",
      "Got result for mathvista - 4200: [('mathvista/accuracy', 0.383)]\n",
      "Got result for ai2d - 4200: [('ai2d/accuracy', 0.7814119170984456)]\n",
      "Got result for chartqa - 4200: [('chartqa/accuracy', 0.5117902576243175)]\n",
      "Got result for vqa - 4200: [('vqa/accuracy', 0.7200559999999769), ('vqa/recall', 0.7536799999999749), ('vqa/bleu', 0.02330746129155159), ('vqa/mllm_evaluation_accuracy', 0.7474479999999766)]\n",
      "Got result for textvqa - 4200: [('textvqa/accuracy', 68.15400000000034), ('textvqa/mllm_eval_accuracy', 72.83600000000034)]\n",
      "Got result for infographics_w_ocr - 4200: [('infographics_w_ocr/anls_total_score', 0.6455215476090577), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.593631762391077), ('infographics_w_ocr/answer_type_multi_span_score', 0.5406425354185939), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6429346422113151), ('infographics_w_ocr/answer_type_question_span_score', 0.6937793712329522), ('infographics_w_ocr/answer_type_single_span_score', 0.6547822532274337), ('infographics_w_ocr/evidence_type_figure_score', 0.625499822505655), ('infographics_w_ocr/evidence_type_map_score', 0.598435516628586), ('infographics_w_ocr/evidence_type_table_list_score', 0.6322162987204276), ('infographics_w_ocr/evidence_type_text_score', 0.6968024268114222), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5903588646817985), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6294846705805609), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5311788944701075), ('infographics_w_ocr/reasoning_type_counting_score', 0.6368298368298368)]\n",
      "Got result for infographics - 4200: [('infographics/anls_total_score', 0.5445563143333099), ('infographics/mllm_evaluation_anls_score', 0.48358513460367497), ('infographics/answer_type_multi_span_score', 0.38158229778189123), ('infographics/answer_type_non_extractive_score', 0.5402963522312529), ('infographics/answer_type_question_span_score', 0.7169809357309358), ('infographics/answer_type_single_span_score', 0.5541243922961592), ('infographics/evidence_type_figure_score', 0.5353231783478309), ('infographics/evidence_type_map_score', 0.515268015251156), ('infographics/evidence_type_table_list_score', 0.5139482289090517), ('infographics/evidence_type_text_score', 0.580868992683848), ('infographics/evidence_type_visual_layout_score', 0.4886262098664006), ('infographics/reasoning_type_arithmetic_score', 0.49783293190827427), ('infographics/reasoning_type_comparison_score', 0.4740022070390362), ('infographics/reasoning_type_counting_score', 0.576165501165501)]\n",
      "Got result for mmbench - 4200: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.625), ('mmbench/image_quality', 0.6585365853658537), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.5061728395061729), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6103896103896104), ('mmbench/overall', 0.7429999480376221)]\n",
      "4200\n",
      "Got result for mmmu - 4400: [('mmmu/accounting', 0.5333333333333333), ('mmmu/agriculture', 0.5333333333333333), ('mmmu/architecture_and_engineering', 0.2), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.7666666666666667), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.7), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.4), ('mmmu/finance', 0.5), ('mmmu/geography', 0.5666666666666667), ('mmmu/history', 0.8), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.7333333333333333), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5622222222222222), ('mmmu/mllm_eval_accuracy', 0.5766666666666667)]\n",
      "Got result for docvqa - 4400: [('docvqa/anls_total_score', 0.6557633914544971), ('docvqa/mllm_evaluation_anls_score', 0.6573544662986255), ('docvqa/mmllm_fixed_anls_score', 0.6956704681255667)]\n",
      "Got result for mathvista - 4400: [('mathvista/accuracy', 0.387)]\n",
      "Got result for ai2d - 4400: [('ai2d/accuracy', 0.7840025906735751)]\n",
      "Got result for chartqa - 4400: [('chartqa/accuracy', 0.5111227203747679)]\n",
      "Got result for vqa - 4400: [('vqa/accuracy', 0.7231719999999755), ('vqa/recall', 0.7561879999999737), ('vqa/bleu', 0.02786100283265114), ('vqa/mllm_evaluation_accuracy', 0.7498679999999759)]\n",
      "Got result for textvqa - 4400: [('textvqa/accuracy', 68.75000000000031), ('textvqa/mllm_eval_accuracy', 73.40400000000035)]\n",
      "Got result for infographics_w_ocr - 4400: [('infographics_w_ocr/anls_total_score', 0.640869254172259), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5872574524530763), ('infographics_w_ocr/answer_type_multi_span_score', 0.5342826164240475), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6360073922460905), ('infographics_w_ocr/answer_type_question_span_score', 0.6754621084303952), ('infographics_w_ocr/answer_type_single_span_score', 0.6518298617480138), ('infographics_w_ocr/evidence_type_figure_score', 0.622979052981222), ('infographics_w_ocr/evidence_type_map_score', 0.582941645812933), ('infographics_w_ocr/evidence_type_table_list_score', 0.6298614504561774), ('infographics_w_ocr/evidence_type_text_score', 0.6907014119069408), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5795351504666832), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6292651412514425), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5343062520988421), ('infographics_w_ocr/reasoning_type_counting_score', 0.6264568764568764)]\n",
      "Got result for infographics - 4400: [('infographics/anls_total_score', 0.5400922262404273), ('infographics/mllm_evaluation_anls_score', 0.47361738459207764), ('infographics/answer_type_multi_span_score', 0.3702331965672532), ('infographics/answer_type_non_extractive_score', 0.5429525266776625), ('infographics/answer_type_question_span_score', 0.7085865523365523), ('infographics/answer_type_single_span_score', 0.549247504599797), ('infographics/evidence_type_figure_score', 0.5312531164139113), ('infographics/evidence_type_map_score', 0.5240131323165083), ('infographics/evidence_type_table_list_score', 0.5008229100454167), ('infographics/evidence_type_text_score', 0.567843341039409), ('infographics/evidence_type_visual_layout_score', 0.4820589224962835), ('infographics/reasoning_type_arithmetic_score', 0.5045756184454813), ('infographics/reasoning_type_comparison_score', 0.49496596596081155), ('infographics/reasoning_type_counting_score', 0.5758158508158507)]\n",
      "Got result for mmbench - 4400: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9433962264150944), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6266666666666667), ('mmbench/physical_relation', 0.2916666666666667), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7403901385497756)]\n",
      "4400\n",
      "Got result for mmmu - 4600: [('mmmu/accounting', 0.43333333333333335), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7333333333333333), ('mmmu/art_theory', 0.9), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.7), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.36666666666666664), ('mmmu/geography', 0.6333333333333333), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.9), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.43333333333333335), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.3333333333333333), ('mmmu/music', 0.2), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.5), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5544444444444444), ('mmmu/mllm_eval_accuracy', 0.5677777777777778)]\n",
      "Got result for docvqa - 4600: [('docvqa/anls_total_score', 0.6545101486853664), ('docvqa/mllm_evaluation_anls_score', 0.6554901090952917), ('docvqa/mmllm_fixed_anls_score', 0.6946350959096745)]\n",
      "Got result for mathvista - 4600: [('mathvista/accuracy', 0.387)]\n",
      "Got result for ai2d - 4600: [('ai2d/accuracy', 0.7739637305699482)]\n",
      "Got result for chartqa - 4600: [('chartqa/accuracy', 0.5216348272229041)]\n",
      "Got result for vqa - 4600: [('vqa/accuracy', 0.7230239999999759), ('vqa/recall', 0.7547159999999742), ('vqa/bleu', 0.035452645272016525), ('vqa/mllm_evaluation_accuracy', 0.7487519999999752)]\n",
      "Got result for textvqa - 4600: [('textvqa/accuracy', 68.54000000000029), ('textvqa/mllm_eval_accuracy', 72.91400000000033)]\n",
      "Got result for infographics_w_ocr - 4600: [('infographics_w_ocr/anls_total_score', 0.6372967727724873), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5845071785970091), ('infographics_w_ocr/answer_type_multi_span_score', 0.5133089988041432), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6342073538276073), ('infographics_w_ocr/answer_type_question_span_score', 0.688015527233814), ('infographics_w_ocr/answer_type_single_span_score', 0.6485131777633335), ('infographics_w_ocr/evidence_type_figure_score', 0.6138132980157596), ('infographics_w_ocr/evidence_type_map_score', 0.5756396656149132), ('infographics_w_ocr/evidence_type_table_list_score', 0.6336873308582497), ('infographics_w_ocr/evidence_type_text_score', 0.6788250966500194), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5658370750747607), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6304223744292234), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5334067292994951), ('infographics_w_ocr/reasoning_type_counting_score', 0.6263403263403264)]\n",
      "Got result for infographics - 4600: [('infographics/anls_total_score', 0.5390574504385265), ('infographics/mllm_evaluation_anls_score', 0.4779643754502526), ('infographics/answer_type_multi_span_score', 0.3819501817450428), ('infographics/answer_type_non_extractive_score', 0.530899831532743), ('infographics/answer_type_question_span_score', 0.6980554168054168), ('infographics/answer_type_single_span_score', 0.5491342420157099), ('infographics/evidence_type_figure_score', 0.5268103029596938), ('infographics/evidence_type_map_score', 0.48829777655761997), ('infographics/evidence_type_table_list_score', 0.5097240649613587), ('infographics/evidence_type_text_score', 0.5733201277213786), ('infographics/evidence_type_visual_layout_score', 0.47970899044139753), ('infographics/reasoning_type_arithmetic_score', 0.48448952113335664), ('infographics/reasoning_type_comparison_score', 0.46306179883339504), ('infographics/reasoning_type_counting_score', 0.5670745920745921)]\n",
      "Got result for mmbench - 4600: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9433962264150944), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7708333333333334), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7399755199699933)]\n",
      "4600\n",
      "Got result for mmmu - 4800: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.5666666666666667), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.6666666666666666), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.43333333333333335), ('mmmu/design', 0.8333333333333334), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.7), ('mmmu/electronics', 0.3), ('mmmu/energy_and_power', 0.36666666666666664), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.3), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.5477777777777778), ('mmmu/mllm_eval_accuracy', 0.5544444444444444)]\n",
      "Got result for docvqa - 4800: [('docvqa/anls_total_score', 0.658989555206662), ('docvqa/mllm_evaluation_anls_score', 0.6599187076808004), ('docvqa/mmllm_fixed_anls_score', 0.6995491372833866)]\n",
      "Got result for mathvista - 4800: [('mathvista/accuracy', 0.388)]\n",
      "Got result for ai2d - 4800: [('ai2d/accuracy', 0.7914507772020726)]\n",
      "Got result for chartqa - 4800: [('chartqa/accuracy', 0.5083548676666264)]\n",
      "Got result for vqa - 4800: [('vqa/accuracy', 0.7222839999999769), ('vqa/recall', 0.7553879999999753), ('vqa/bleu', 0.02790968306362629), ('vqa/mllm_evaluation_accuracy', 0.7492239999999765)]\n",
      "Got result for textvqa - 4800: [('textvqa/accuracy', 68.54800000000034), ('textvqa/mllm_eval_accuracy', 73.01600000000037)]\n",
      "Got result for infographics_w_ocr - 4800: [('infographics_w_ocr/anls_total_score', 0.6437301576828082), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5877062220362721), ('infographics_w_ocr/answer_type_multi_span_score', 0.5420926642190722), ('infographics_w_ocr/answer_type_non_extractive_score', 0.628466660925974), ('infographics_w_ocr/answer_type_question_span_score', 0.7062542329725198), ('infographics_w_ocr/answer_type_single_span_score', 0.6559372880822653), ('infographics_w_ocr/evidence_type_figure_score', 0.6218048038679517), ('infographics_w_ocr/evidence_type_map_score', 0.5339854229705714), ('infographics_w_ocr/evidence_type_table_list_score', 0.6425519303295671), ('infographics_w_ocr/evidence_type_text_score', 0.6926183671571733), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5855374944971965), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6226897151554683), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5394764741991288), ('infographics_w_ocr/reasoning_type_counting_score', 0.621969696969697)]\n",
      "Got result for infographics - 4800: [('infographics/anls_total_score', 0.5386827314227726), ('infographics/mllm_evaluation_anls_score', 0.4729867452208466), ('infographics/answer_type_multi_span_score', 0.38523672783381535), ('infographics/answer_type_non_extractive_score', 0.5272315287685994), ('infographics/answer_type_question_span_score', 0.7014284326784327), ('infographics/answer_type_single_span_score', 0.5504831452682992), ('infographics/evidence_type_figure_score', 0.5272365884028953), ('infographics/evidence_type_map_score', 0.509050520436659), ('infographics/evidence_type_table_list_score', 0.5061884012277408), ('infographics/evidence_type_text_score', 0.560069064506999), ('infographics/evidence_type_visual_layout_score', 0.5069552577313754), ('infographics/reasoning_type_arithmetic_score', 0.46766792948299796), ('infographics/reasoning_type_comparison_score', 0.4640977606710045), ('infographics/reasoning_type_counting_score', 0.5825174825174825)]\n",
      "Got result for mmbench - 4800: [('mmbench/attribute_comparison', 0.6136363636363636), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8227848101265823), ('mmbench/future_prediction', 0.575), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9433962264150944), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6533333333333333), ('mmbench/physical_relation', 0.375), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.5844155844155844), ('mmbench/overall', 0.7397956610533203)]\n",
      "4800\n",
      "Got result for mmmu - 5000: [('mmmu/accounting', 0.6), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.7666666666666667), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.26666666666666666), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.5333333333333333), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.3), ('mmmu/economics', 0.7333333333333333), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.4), ('mmmu/finance', 0.5), ('mmmu/geography', 0.5), ('mmmu/history', 0.8666666666666667), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.6666666666666666), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.6333333333333333), ('mmmu/mechanical_engineering', 0.3), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5477777777777778), ('mmmu/mllm_eval_accuracy', 0.5611111111111111)]\n",
      "Got result for docvqa - 5000: [('docvqa/anls_total_score', 0.6610317410489557), ('docvqa/mllm_evaluation_anls_score', 0.6627958415479214), ('docvqa/mmllm_fixed_anls_score', 0.7016752137369)]\n",
      "Got result for mathvista - 5000: [('mathvista/accuracy', 0.39)]\n",
      "Got result for ai2d - 5000: [('ai2d/accuracy', 0.7898316062176166)]\n",
      "Got result for chartqa - 5000: [('chartqa/accuracy', 0.5101592981058338)]\n",
      "Got result for vqa - 5000: [('vqa/accuracy', 0.7206159999999766), ('vqa/recall', 0.7538359999999752), ('vqa/bleu', 0.03004704974591732), ('vqa/mllm_evaluation_accuracy', 0.7475639999999762)]\n",
      "Got result for textvqa - 5000: [('textvqa/accuracy', 68.2440000000003), ('textvqa/mllm_eval_accuracy', 72.77600000000032)]\n",
      "Got result for infographics_w_ocr - 5000: [('infographics_w_ocr/anls_total_score', 0.6408745695490592), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5867326674080696), ('infographics_w_ocr/answer_type_multi_span_score', 0.5368646903766523), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6298243347972102), ('infographics_w_ocr/answer_type_question_span_score', 0.6851538056220924), ('infographics_w_ocr/answer_type_single_span_score', 0.6538381325398681), ('infographics_w_ocr/evidence_type_figure_score', 0.6245342820112753), ('infographics_w_ocr/evidence_type_map_score', 0.5964553186087839), ('infographics_w_ocr/evidence_type_table_list_score', 0.6300711025300366), ('infographics_w_ocr/evidence_type_text_score', 0.6913387996179957), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5554432305259865), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6195531637312457), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5486697419076981), ('infographics_w_ocr/reasoning_type_counting_score', 0.6315850815850815)]\n",
      "Got result for infographics - 5000: [('infographics/anls_total_score', 0.5413697516904487), ('infographics/mllm_evaluation_anls_score', 0.4763699278379794), ('infographics/answer_type_multi_span_score', 0.3581538670326961), ('infographics/answer_type_non_extractive_score', 0.5311066068751422), ('infographics/answer_type_question_span_score', 0.7131007347353501), ('infographics/answer_type_single_span_score', 0.5556618175862705), ('infographics/evidence_type_figure_score', 0.5291636987705371), ('infographics/evidence_type_map_score', 0.5080922133015298), ('infographics/evidence_type_table_list_score', 0.5152681249786293), ('infographics/evidence_type_text_score', 0.5756623583169667), ('infographics/evidence_type_visual_layout_score', 0.4714943119802403), ('infographics/reasoning_type_arithmetic_score', 0.4727235853948183), ('infographics/reasoning_type_comparison_score', 0.5056429968347075), ('infographics/reasoning_type_counting_score', 0.5856643356643355)]\n",
      "Got result for mmbench - 5000: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8450704225352113), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9433962264150944), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.48148148148148145), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7416366757373699)]\n",
      "5000\n",
      "Got result for mmmu - 5200: [('mmmu/accounting', 0.4666666666666667), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.43333333333333335), ('mmmu/art', 0.7666666666666667), ('mmmu/art_theory', 0.7333333333333333), ('mmmu/basic_medical_science', 0.7), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.5333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.5), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.23333333333333334), ('mmmu/energy_and_power', 0.36666666666666664), ('mmmu/finance', 0.36666666666666664), ('mmmu/geography', 0.5), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.6333333333333333), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.4666666666666667), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5477777777777778), ('mmmu/mllm_eval_accuracy', 0.56)]\n",
      "Got result for docvqa - 5200: [('docvqa/anls_total_score', 0.652638277816228), ('docvqa/mllm_evaluation_anls_score', 0.6538082045330332), ('docvqa/mmllm_fixed_anls_score', 0.6942694883666434)]\n",
      "Got result for mathvista - 5200: [('mathvista/accuracy', 0.382)]\n",
      "Got result for ai2d - 5200: [('ai2d/accuracy', 0.7840025906735751)]\n",
      "Got result for chartqa - 5200: [('chartqa/accuracy', 0.5049039105589425)]\n",
      "Got result for vqa - 5200: [('vqa/accuracy', 0.7206319999999774), ('vqa/recall', 0.7537959999999749), ('vqa/bleu', 0.027150224894285202), ('vqa/mllm_evaluation_accuracy', 0.7475319999999769)]\n",
      "Got result for textvqa - 5200: [('textvqa/accuracy', 68.48400000000031), ('textvqa/mllm_eval_accuracy', 72.99400000000036)]\n",
      "Got result for infographics_w_ocr - 5200: [('infographics_w_ocr/anls_total_score', 0.63642905626363), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5828794278393601), ('infographics_w_ocr/answer_type_multi_span_score', 0.5033184814803386), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6267674158270905), ('infographics_w_ocr/answer_type_question_span_score', 0.6896966484002294), ('infographics_w_ocr/answer_type_single_span_score', 0.6511121772673722), ('infographics_w_ocr/evidence_type_figure_score', 0.6157750991240359), ('infographics_w_ocr/evidence_type_map_score', 0.5807787509520183), ('infographics_w_ocr/evidence_type_table_list_score', 0.6278200030413302), ('infographics_w_ocr/evidence_type_text_score', 0.6891430909130589), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5598763104872355), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6189579256360076), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5309364302140089), ('infographics_w_ocr/reasoning_type_counting_score', 0.6202214452214452)]\n",
      "Got result for infographics - 5200: [('infographics/anls_total_score', 0.5407560314097579), ('infographics/mllm_evaluation_anls_score', 0.47577455582720424), ('infographics/answer_type_multi_span_score', 0.33359279967225014), ('infographics/answer_type_non_extractive_score', 0.5303086505618152), ('infographics/answer_type_question_span_score', 0.6904241591741592), ('infographics/answer_type_single_span_score', 0.5585733518251951), ('infographics/evidence_type_figure_score', 0.5294030448251783), ('infographics/evidence_type_map_score', 0.5055511999163718), ('infographics/evidence_type_table_list_score', 0.5143543781660223), ('infographics/evidence_type_text_score', 0.5842622971935126), ('infographics/evidence_type_visual_layout_score', 0.47677991532041125), ('infographics/reasoning_type_arithmetic_score', 0.4785754010069079), ('infographics/reasoning_type_comparison_score', 0.4758317144499497), ('infographics/reasoning_type_counting_score', 0.5708041958041957)]\n",
      "Got result for mmbench - 5200: [('mmbench/attribute_comparison', 0.5909090909090909), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9433962264150944), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.64), ('mmbench/physical_relation', 0.4166666666666667), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.5974025974025974), ('mmbench/overall', 0.744505177266927)]\n",
      "5200\n",
      "Got result for mmmu - 5400: [('mmmu/accounting', 0.5333333333333333), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.4), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.8), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.5666666666666667), ('mmmu/computer_science', 0.43333333333333335), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.7), ('mmmu/electronics', 0.4666666666666667), ('mmmu/energy_and_power', 0.4), ('mmmu/finance', 0.5), ('mmmu/geography', 0.5), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.4666666666666667), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.6333333333333333), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.5666666666666667), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5655555555555556), ('mmmu/mllm_eval_accuracy', 0.5722222222222222)]\n",
      "Got result for docvqa - 5400: [('docvqa/anls_total_score', 0.6661388818798234), ('docvqa/mllm_evaluation_anls_score', 0.6660325309024209), ('docvqa/mmllm_fixed_anls_score', 0.7035213442340026)]\n",
      "Got result for mathvista - 5400: [('mathvista/accuracy', 0.386)]\n",
      "Got result for ai2d - 5400: [('ai2d/accuracy', 0.7869170984455959)]\n",
      "Got result for chartqa - 5400: [('chartqa/accuracy', 0.5028102119643632)]\n",
      "Got result for vqa - 5400: [('vqa/accuracy', 0.7217199999999779), ('vqa/recall', 0.7545199999999757), ('vqa/bleu', 0.027963552623987198), ('vqa/mllm_evaluation_accuracy', 0.7485439999999772)]\n",
      "Got result for textvqa - 5400: [('textvqa/accuracy', 68.35600000000031), ('textvqa/mllm_eval_accuracy', 72.82000000000035)]\n",
      "Got result for infographics_w_ocr - 5400: [('infographics_w_ocr/anls_total_score', 0.6382845154040807), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5857492170747914), ('infographics_w_ocr/answer_type_multi_span_score', 0.5279454602181968), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6320433333995722), ('infographics_w_ocr/answer_type_question_span_score', 0.6948477473013283), ('infographics_w_ocr/answer_type_single_span_score', 0.6488859969197899), ('infographics_w_ocr/evidence_type_figure_score', 0.6204661585064071), ('infographics_w_ocr/evidence_type_map_score', 0.6073464077176948), ('infographics_w_ocr/evidence_type_table_list_score', 0.627573513962485), ('infographics_w_ocr/evidence_type_text_score', 0.6885865565841), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5563208647523976), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6186185960158562), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5200610455158744), ('infographics_w_ocr/reasoning_type_counting_score', 0.6295454545454546)]\n",
      "Got result for infographics - 5400: [('infographics/anls_total_score', 0.5526801725132487), ('infographics/mllm_evaluation_anls_score', 0.4876250002616105), ('infographics/answer_type_multi_span_score', 0.36735916274508257), ('infographics/answer_type_non_extractive_score', 0.5461313249740014), ('infographics/answer_type_question_span_score', 0.7146269862616016), ('infographics/answer_type_single_span_score', 0.5660573260000126), ('infographics/evidence_type_figure_score', 0.5446583609378896), ('infographics/evidence_type_map_score', 0.571785707982563), ('infographics/evidence_type_table_list_score', 0.5286171123872022), ('infographics/evidence_type_text_score', 0.5939445537667762), ('infographics/evidence_type_visual_layout_score', 0.4937580677149898), ('infographics/reasoning_type_arithmetic_score', 0.49432861658889043), ('infographics/reasoning_type_comparison_score', 0.4635588119264475), ('infographics/reasoning_type_counting_score', 0.5885780885780885)]\n",
      "Got result for mmbench - 5400: [('mmbench/attribute_comparison', 0.5681818181818182), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8354430379746836), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8125), ('mmbench/object_localization', 0.49382716049382713), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6266666666666667), ('mmbench/physical_relation', 0.4583333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.7438272369363708)]\n",
      "5400\n",
      "Got result for mmmu - 5600: [('mmmu/accounting', 0.6666666666666666), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8333333333333334), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.6333333333333333), ('mmmu/design', 0.7666666666666667), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.7), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5333333333333333), ('mmmu/history', 0.8), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.4), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.43333333333333335), ('mmmu/psychology', 0.7), ('mmmu/public_health', 0.6666666666666666), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.5655555555555556), ('mmmu/mllm_eval_accuracy', 0.5733333333333334)]\n",
      "Got result for docvqa - 5600: [('docvqa/anls_total_score', 0.6656040947493893), ('docvqa/mllm_evaluation_anls_score', 0.665849867487602), ('docvqa/mmllm_fixed_anls_score', 0.7032078699470247)]\n",
      "Got result for mathvista - 5600: [('mathvista/accuracy', 0.385)]\n",
      "Got result for ai2d - 5600: [('ai2d/accuracy', 0.7898316062176166)]\n",
      "Got result for chartqa - 5600: [('chartqa/accuracy', 0.5072405530877618)]\n",
      "Got result for vqa - 5600: [('vqa/accuracy', 0.7203959999999766), ('vqa/recall', 0.7526479999999742), ('vqa/bleu', 0.028335917741060257), ('vqa/mllm_evaluation_accuracy', 0.746399999999976)]\n",
      "Got result for textvqa - 5600: [('textvqa/accuracy', 68.6020000000003), ('textvqa/mllm_eval_accuracy', 73.17600000000033)]\n",
      "Got result for infographics_w_ocr - 5600: [('infographics_w_ocr/anls_total_score', 0.6422024618433236), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5899852365043707), ('infographics_w_ocr/answer_type_multi_span_score', 0.5339917836461908), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6350340136054422), ('infographics_w_ocr/answer_type_question_span_score', 0.6912206554389423), ('infographics_w_ocr/answer_type_single_span_score', 0.6538947092472998), ('infographics_w_ocr/evidence_type_figure_score', 0.6246558902668516), ('infographics_w_ocr/evidence_type_map_score', 0.5751681898959127), ('infographics_w_ocr/evidence_type_table_list_score', 0.6414974025297534), ('infographics_w_ocr/evidence_type_text_score', 0.6827250477032243), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5462087789824172), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6307893020221785), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5464316702006704), ('infographics_w_ocr/reasoning_type_counting_score', 0.6245920745920746)]\n",
      "Got result for infographics - 5600: [('infographics/anls_total_score', 0.5452798777254055), ('infographics/mllm_evaluation_anls_score', 0.4803912785743501), ('infographics/answer_type_multi_span_score', 0.3883635466669175), ('infographics/answer_type_non_extractive_score', 0.5404506885519546), ('infographics/answer_type_question_span_score', 0.6862574925074925), ('infographics/answer_type_single_span_score', 0.5562077372766949), ('infographics/evidence_type_figure_score', 0.5416890767336627), ('infographics/evidence_type_map_score', 0.5032539965118207), ('infographics/evidence_type_table_list_score', 0.4998192466731108), ('infographics/evidence_type_text_score', 0.5913621934271458), ('infographics/evidence_type_visual_layout_score', 0.4991453855264213), ('infographics/reasoning_type_arithmetic_score', 0.4964128907622057), ('infographics/reasoning_type_comparison_score', 0.4491724175345135), ('infographics/reasoning_type_counting_score', 0.5798368298368298)]\n",
      "Got result for mmbench - 5600: [('mmbench/attribute_comparison', 0.5454545454545454), ('mmbench/attribute_recognition', 0.8309859154929577), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8227848101265823), ('mmbench/future_prediction', 0.575), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9519230769230769), ('mmbench/image_style', 0.9433962264150944), ('mmbench/image_topic', 0.8611111111111112), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.4691358024691358), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.6666666666666666), ('mmbench/physical_relation', 0.3333333333333333), ('mmbench/spatial_relationship', 0.4), ('mmbench/structuralized_imagetext_understanding', 0.6363636363636364), ('mmbench/overall', 0.733379746720739)]\n",
      "5600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.5611</td>\n",
       "      <td>0.6496</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7905</td>\n",
       "      <td>0.5061</td>\n",
       "      <td>0.7205</td>\n",
       "      <td>0.67858</td>\n",
       "      <td>0.6418</td>\n",
       "      <td>0.5387</td>\n",
       "      <td>0.7386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.5767</td>\n",
       "      <td>0.5622</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.7976</td>\n",
       "      <td>0.5042</td>\n",
       "      <td>0.7206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6358</td>\n",
       "      <td>0.5427</td>\n",
       "      <td>0.7429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.5611</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.6536</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.7976</td>\n",
       "      <td>0.5135</td>\n",
       "      <td>0.7229</td>\n",
       "      <td>0.68194</td>\n",
       "      <td>0.6392</td>\n",
       "      <td>0.5387</td>\n",
       "      <td>0.7443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.5678</td>\n",
       "      <td>0.5467</td>\n",
       "      <td>0.6537</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.5163</td>\n",
       "      <td>0.7204</td>\n",
       "      <td>0.68472</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.7468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.5622</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.6558</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.7863</td>\n",
       "      <td>0.5142</td>\n",
       "      <td>0.7228</td>\n",
       "      <td>0.68406</td>\n",
       "      <td>0.6395</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.7473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>0.5611</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.7986</td>\n",
       "      <td>0.4978</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>0.68288</td>\n",
       "      <td>0.6425</td>\n",
       "      <td>0.5537</td>\n",
       "      <td>0.7485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>0.5456</td>\n",
       "      <td>0.5456</td>\n",
       "      <td>0.6563</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.5108</td>\n",
       "      <td>0.7243</td>\n",
       "      <td>0.6856</td>\n",
       "      <td>0.6416</td>\n",
       "      <td>0.5457</td>\n",
       "      <td>0.7402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>0.5567</td>\n",
       "      <td>0.5433</td>\n",
       "      <td>0.6644</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.7263</td>\n",
       "      <td>0.6871</td>\n",
       "      <td>0.6455</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>0.7419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.6597</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.7824</td>\n",
       "      <td>0.5116</td>\n",
       "      <td>0.7249</td>\n",
       "      <td>0.68338</td>\n",
       "      <td>0.6448</td>\n",
       "      <td>0.5458</td>\n",
       "      <td>0.7436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.5789</td>\n",
       "      <td>0.5644</td>\n",
       "      <td>0.6627</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.7876</td>\n",
       "      <td>0.5063</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.68392</td>\n",
       "      <td>0.6347</td>\n",
       "      <td>0.5422</td>\n",
       "      <td>0.7526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>0.5789</td>\n",
       "      <td>0.5622</td>\n",
       "      <td>0.6609</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.7908</td>\n",
       "      <td>0.5205</td>\n",
       "      <td>0.7237</td>\n",
       "      <td>0.68882</td>\n",
       "      <td>0.6399</td>\n",
       "      <td>0.5479</td>\n",
       "      <td>0.7488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>0.5633</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.6567</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.7927</td>\n",
       "      <td>0.5261</td>\n",
       "      <td>0.7229</td>\n",
       "      <td>0.6883</td>\n",
       "      <td>0.6382</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.7452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.5778</td>\n",
       "      <td>0.6614</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.7963</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>0.7241</td>\n",
       "      <td>0.68598</td>\n",
       "      <td>0.6478</td>\n",
       "      <td>0.5555</td>\n",
       "      <td>0.7346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.5467</td>\n",
       "      <td>0.6576</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.7931</td>\n",
       "      <td>0.5192</td>\n",
       "      <td>0.7199</td>\n",
       "      <td>0.6863</td>\n",
       "      <td>0.6432</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.7495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>0.5533</td>\n",
       "      <td>0.5378</td>\n",
       "      <td>0.6675</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.8021</td>\n",
       "      <td>0.5112</td>\n",
       "      <td>0.7219</td>\n",
       "      <td>0.6887</td>\n",
       "      <td>0.6446</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.7489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>0.5911</td>\n",
       "      <td>0.5778</td>\n",
       "      <td>0.6668</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.7995</td>\n",
       "      <td>0.5038</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.68174</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.5455</td>\n",
       "      <td>0.7487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>0.5622</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.7205</td>\n",
       "      <td>0.67884</td>\n",
       "      <td>0.6449</td>\n",
       "      <td>0.5471</td>\n",
       "      <td>0.7485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.6546</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.7814</td>\n",
       "      <td>0.5149</td>\n",
       "      <td>0.7237</td>\n",
       "      <td>0.68434</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.7485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>0.5733</td>\n",
       "      <td>0.5589</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.7863</td>\n",
       "      <td>0.4977</td>\n",
       "      <td>0.7204</td>\n",
       "      <td>0.68958</td>\n",
       "      <td>0.6481</td>\n",
       "      <td>0.5476</td>\n",
       "      <td>0.7446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.5589</td>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.6558</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.5116</td>\n",
       "      <td>0.7222</td>\n",
       "      <td>0.68508</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.5384</td>\n",
       "      <td>0.7447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>0.5611</td>\n",
       "      <td>0.5422</td>\n",
       "      <td>0.6618</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.7814</td>\n",
       "      <td>0.5118</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>0.68154</td>\n",
       "      <td>0.6455</td>\n",
       "      <td>0.5446</td>\n",
       "      <td>0.743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>0.5767</td>\n",
       "      <td>0.5622</td>\n",
       "      <td>0.6558</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.5111</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.7404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.5678</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.6545</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.5216</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.6854</td>\n",
       "      <td>0.6373</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.7915</td>\n",
       "      <td>0.5084</td>\n",
       "      <td>0.7223</td>\n",
       "      <td>0.68548</td>\n",
       "      <td>0.6437</td>\n",
       "      <td>0.5387</td>\n",
       "      <td>0.7398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.5611</td>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.7898</td>\n",
       "      <td>0.5102</td>\n",
       "      <td>0.7206</td>\n",
       "      <td>0.68244</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.5414</td>\n",
       "      <td>0.7416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.6526</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.5049</td>\n",
       "      <td>0.7206</td>\n",
       "      <td>0.68484</td>\n",
       "      <td>0.6364</td>\n",
       "      <td>0.5408</td>\n",
       "      <td>0.7445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5400</th>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.6661</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>0.5028</td>\n",
       "      <td>0.7217</td>\n",
       "      <td>0.68356</td>\n",
       "      <td>0.6383</td>\n",
       "      <td>0.5527</td>\n",
       "      <td>0.7438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5600</th>\n",
       "      <td>0.5733</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.7898</td>\n",
       "      <td>0.5072</td>\n",
       "      <td>0.7204</td>\n",
       "      <td>0.68602</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.5453</td>\n",
       "      <td>0.7334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1  docvqa mathvista    ai2d chartqa     vqa  textvqa  \\\n",
       "200     0.57  0.5611  0.6496       0.4  0.7905  0.5061  0.7205  0.67858   \n",
       "400   0.5767  0.5622   0.653     0.399  0.7976  0.5042  0.7206      NaN   \n",
       "600   0.5611  0.5656  0.6536     0.394  0.7976  0.5135  0.7229  0.68194   \n",
       "800   0.5678  0.5467  0.6537     0.385   0.784  0.5163  0.7204  0.68472   \n",
       "1000  0.5622    0.55  0.6558     0.385  0.7863  0.5142  0.7228  0.68406   \n",
       "1200  0.5611  0.5667  0.6667     0.393  0.7986  0.4978  0.7268  0.68288   \n",
       "1400  0.5456  0.5456  0.6563     0.378  0.7889  0.5108  0.7243   0.6856   \n",
       "1600  0.5567  0.5433  0.6644     0.391  0.7889   0.516  0.7263   0.6871   \n",
       "1800    0.57  0.5478  0.6597     0.399  0.7824  0.5116  0.7249  0.68338   \n",
       "2000  0.5789  0.5644  0.6627     0.385  0.7876  0.5063   0.725  0.68392   \n",
       "2200  0.5789  0.5622  0.6609     0.389  0.7908  0.5205  0.7237  0.68882   \n",
       "2400  0.5633  0.5556  0.6567     0.401  0.7927  0.5261  0.7229   0.6883   \n",
       "2600  0.5844  0.5778  0.6614     0.377  0.7963  0.5071  0.7241  0.68598   \n",
       "2800    0.56  0.5467  0.6576     0.379  0.7931  0.5192  0.7199   0.6863   \n",
       "3000  0.5533  0.5378  0.6675     0.396  0.8021  0.5112  0.7219   0.6887   \n",
       "3200  0.5911  0.5778  0.6668     0.401  0.7995  0.5038   0.722  0.68174   \n",
       "3400  0.5622  0.5544  0.6571     0.395  0.7944   0.513  0.7205  0.67884   \n",
       "3600  0.5656  0.5544  0.6546     0.398  0.7814  0.5149  0.7237  0.68434   \n",
       "3800  0.5733  0.5589   0.668     0.391  0.7863  0.4977  0.7204  0.68958   \n",
       "4000  0.5589  0.5478  0.6558     0.406  0.7892  0.5116  0.7222  0.68508   \n",
       "4200  0.5611  0.5422  0.6618     0.383  0.7814  0.5118  0.7201  0.68154   \n",
       "4400  0.5767  0.5622  0.6558     0.387   0.784  0.5111  0.7232   0.6875   \n",
       "4600  0.5678  0.5544  0.6545     0.387   0.774  0.5216   0.723   0.6854   \n",
       "4800  0.5544  0.5478   0.659     0.388  0.7915  0.5084  0.7223  0.68548   \n",
       "5000  0.5611  0.5478   0.661      0.39  0.7898  0.5102  0.7206  0.68244   \n",
       "5200    0.56  0.5478  0.6526     0.382   0.784  0.5049  0.7206  0.68484   \n",
       "5400  0.5722  0.5656  0.6661     0.386  0.7869  0.5028  0.7217  0.68356   \n",
       "5600  0.5733  0.5656  0.6656     0.385  0.7898  0.5072  0.7204  0.68602   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "200              0.6418       0.5387  0.7386  \n",
       "400              0.6358       0.5427  0.7429  \n",
       "600              0.6392       0.5387  0.7443  \n",
       "800               0.644        0.537  0.7468  \n",
       "1000             0.6395        0.541  0.7473  \n",
       "1200             0.6425       0.5537  0.7485  \n",
       "1400             0.6416       0.5457  0.7402  \n",
       "1600             0.6455       0.5486  0.7419  \n",
       "1800             0.6448       0.5458  0.7436  \n",
       "2000             0.6347       0.5422  0.7526  \n",
       "2200             0.6399       0.5479  0.7488  \n",
       "2400             0.6382         0.55  0.7452  \n",
       "2600             0.6478       0.5555  0.7346  \n",
       "2800             0.6432       0.5407  0.7495  \n",
       "3000             0.6446       0.5518  0.7489  \n",
       "3200               0.64       0.5455  0.7487  \n",
       "3400             0.6449       0.5471  0.7485  \n",
       "3600             0.6422       0.5415  0.7485  \n",
       "3800             0.6481       0.5476  0.7446  \n",
       "4000              0.639       0.5384  0.7447  \n",
       "4200             0.6455       0.5446   0.743  \n",
       "4400             0.6409       0.5401  0.7404  \n",
       "4600             0.6373       0.5391    0.74  \n",
       "4800             0.6437       0.5387  0.7398  \n",
       "5000             0.6409       0.5414  0.7416  \n",
       "5200             0.6364       0.5408  0.7445  \n",
       "5400             0.6383       0.5527  0.7438  \n",
       "5600             0.6422       0.5453  0.7334  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores_all(\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_504px_64nodes_exp29\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 336px i18n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New checkpoints: []\n"
     ]
    }
   ],
   "source": [
    "eval_helper.run_eval_sweep(\n",
    "    output_dir=f\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_64nodes_i18n\",\n",
    "    eval_sbatch=EVAL_SBATCH,\n",
    "    eval_config_dir=EVAL_CONFIG_DIR,\n",
    "    aligner_parent_dir=ALIGNER_CODE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000, 5200, 5400, 5600, 5800, 6000, 6200, 6400]\n",
      "Got result for mmmu - 5000: [('mmmu/accounting', 0.6), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.3), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8666666666666667), ('mmmu/basic_medical_science', 0.6333333333333333), ('mmmu/biology', 0.3333333333333333), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.4666666666666667), ('mmmu/design', 0.6), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.7), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.6333333333333333), ('mmmu/finance', 0.4), ('mmmu/geography', 0.6), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5333333333333333), ('mmmu/marketing', 0.43333333333333335), ('mmmu/materials', 0.3), ('mmmu/math', 0.5), ('mmmu/mechanical_engineering', 0.4), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.3333333333333333), ('mmmu/psychology', 0.5333333333333333), ('mmmu/public_health', 0.7), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.5433333333333333), ('mmmu/mllm_eval_accuracy', 0.5444444444444444)]\n",
      "Got result for docvqa - 5000: [('docvqa/anls_total_score', 0.670637585261586), ('docvqa/mllm_evaluation_anls_score', 0.6717115681299881), ('docvqa/mmllm_fixed_anls_score', 0.7098482827714544)]\n",
      "Got result for mathvista - 5000: [('mathvista/accuracy', 0.476)]\n",
      "Got result for ai2d - 5000: [('ai2d/accuracy', 0.8487694300518135)]\n",
      "Got result for chartqa - 5000: [('chartqa/accuracy', 0.7013713040155702)]\n",
      "Got result for vqa - 5000: [('vqa/accuracy', 0.7255639999999781), ('vqa/recall', 0.7703199999999752), ('vqa/bleu', 0.02905668318271637), ('vqa/mllm_evaluation_accuracy', 0.7620959999999765)]\n",
      "Got result for textvqa - 5000: [('textvqa/accuracy', 66.91600000000031), ('textvqa/mllm_eval_accuracy', 72.05800000000038)]\n",
      "Got result for infographics_w_ocr - 5000: [('infographics_w_ocr/anls_total_score', 0.6587297113673974), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6052329270539901), ('infographics_w_ocr/answer_type_multi_span_score', 0.49944804635198403), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6579324232127127), ('infographics_w_ocr/answer_type_question_span_score', 0.7174033765379919), ('infographics_w_ocr/answer_type_single_span_score', 0.6722565143940358), ('infographics_w_ocr/evidence_type_figure_score', 0.6386578491151), ('infographics_w_ocr/evidence_type_map_score', 0.6146071337903021), ('infographics_w_ocr/evidence_type_table_list_score', 0.652155825003489), ('infographics_w_ocr/evidence_type_text_score', 0.7148331041823205), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5843910088653583), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.662796678207637), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5495278857502663), ('infographics_w_ocr/reasoning_type_counting_score', 0.6432400932400932)]\n",
      "Got result for infographics - 5000: [('infographics/anls_total_score', 0.5542418821514459), ('infographics/mllm_evaluation_anls_score', 0.49538160071682635), ('infographics/answer_type_multi_span_score', 0.3964590614371482), ('infographics/answer_type_non_extractive_score', 0.5370127310904886), ('infographics/answer_type_question_span_score', 0.7127330468676623), ('infographics/answer_type_single_span_score', 0.5690404304472785), ('infographics/evidence_type_figure_score', 0.5369959821881908), ('infographics/evidence_type_map_score', 0.5463945974429375), ('infographics/evidence_type_table_list_score', 0.5292795570991102), ('infographics/evidence_type_text_score', 0.6047008255767977), ('infographics/evidence_type_visual_layout_score', 0.497281120300204), ('infographics/reasoning_type_arithmetic_score', 0.4779156631040195), ('infographics/reasoning_type_comparison_score', 0.4874533088981116), ('infographics/reasoning_type_counting_score', 0.5795454545454546)]\n",
      "Got result for mmbench - 5000: [('mmbench/attribute_comparison', 0.7272727272727273), ('mmbench/attribute_recognition', 0.9154929577464789), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.55), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.8490566037735849), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8333333333333334), ('mmbench/object_localization', 0.5802469135802469), ('mmbench/ocr', 0.8717948717948718), ('mmbench/physical_property_reasoning', 0.6933333333333334), ('mmbench/physical_relation', 0.75), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.7142857142857143), ('mmbench/overall', 0.7887324153485087)]\n",
      "5000\n",
      "Got result for mmmu - 5200: [('mmmu/accounting', 0.5666666666666667), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.7), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.5666666666666667), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.36666666666666664), ('mmmu/clinical_medicine', 0.5333333333333333), ('mmmu/computer_science', 0.5), ('mmmu/design', 0.6333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.5333333333333333), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.6), ('mmmu/finance', 0.3333333333333333), ('mmmu/geography', 0.5), ('mmmu/history', 0.8), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.43333333333333335), ('mmmu/materials', 0.26666666666666666), ('mmmu/math', 0.4666666666666667), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.3333333333333333), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.26666666666666666), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.5333333333333333), ('mmmu/sociology', 0.6666666666666666), ('mmmu/accuracy', 0.5277777777777778), ('mmmu/mllm_eval_accuracy', 0.5422222222222223)]\n",
      "Got result for docvqa - 5200: [('docvqa/anls_total_score', 0.6631147285194107), ('docvqa/mllm_evaluation_anls_score', 0.6651859112559485), ('docvqa/mmllm_fixed_anls_score', 0.705311167713988)]\n",
      "Got result for mathvista - 5200: [('mathvista/accuracy', 0.524)]\n",
      "Got result for ai2d - 5200: [('ai2d/accuracy', 0.8724093264248705)]\n",
      "Got result for chartqa - 5200: [('chartqa/accuracy', 0.77878383530482)]\n",
      "Got result for vqa - 5200: [('vqa/accuracy', 0.7279519999999774), ('vqa/recall', 0.7583879999999757), ('vqa/bleu', 0.021016282960772514), ('vqa/mllm_evaluation_accuracy', 0.7521079999999764)]\n",
      "Got result for textvqa - 5200: [('textvqa/accuracy', 66.48400000000035), ('textvqa/mllm_eval_accuracy', 71.19600000000038)]\n",
      "Got result for infographics_w_ocr - 5200: [('infographics_w_ocr/anls_total_score', 0.6527497136634058), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6003825017246154), ('infographics_w_ocr/answer_type_multi_span_score', 0.49536616919547577), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6454729668563306), ('infographics_w_ocr/answer_type_question_span_score', 0.6954899267399267), ('infographics_w_ocr/answer_type_single_span_score', 0.6678365477310179), ('infographics_w_ocr/evidence_type_figure_score', 0.6343397723280322), ('infographics_w_ocr/evidence_type_map_score', 0.6203827113480578), ('infographics_w_ocr/evidence_type_table_list_score', 0.6489245524968859), ('infographics_w_ocr/evidence_type_text_score', 0.6988071501948709), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.574185477431139), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6383443516148992), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5522644390014025), ('infographics_w_ocr/reasoning_type_counting_score', 0.6384032634032634)]\n",
      "Got result for infographics - 5200: [('infographics/anls_total_score', 0.5202706772978193), ('infographics/mllm_evaluation_anls_score', 0.47142314915796146), ('infographics/answer_type_multi_span_score', 0.35172581764910577), ('infographics/answer_type_non_extractive_score', 0.4846594333936105), ('infographics/answer_type_question_span_score', 0.707409951159951), ('infographics/answer_type_single_span_score', 0.5384356957578428), ('infographics/evidence_type_figure_score', 0.5015040520669284), ('infographics/evidence_type_map_score', 0.5147984441301272), ('infographics/evidence_type_table_list_score', 0.49225625090675046), ('infographics/evidence_type_text_score', 0.5589801698423115), ('infographics/evidence_type_visual_layout_score', 0.46309517071891576), ('infographics/reasoning_type_arithmetic_score', 0.37500000000000006), ('infographics/reasoning_type_comparison_score', 0.46012078685604085), ('infographics/reasoning_type_counting_score', 0.5795454545454545)]\n",
      "Got result for mmbench - 5200: [('mmbench/attribute_comparison', 0.7272727272727273), ('mmbench/attribute_recognition', 0.9436619718309859), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.55), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.8867924528301887), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7708333333333334), ('mmbench/object_localization', 0.5679012345679012), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.7466666666666667), ('mmbench/physical_relation', 0.7916666666666666), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.7272727272727273), ('mmbench/overall', 0.7946849384057302)]\n",
      "5200\n",
      "Got result for mmmu - 5400: [('mmmu/accounting', 0.43333333333333335), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.5333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.43333333333333335), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5), ('mmmu/design', 0.7), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.3333333333333333), ('mmmu/energy_and_power', 0.7), ('mmmu/finance', 0.4), ('mmmu/geography', 0.6666666666666666), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.8333333333333334), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.26666666666666666), ('mmmu/math', 0.36666666666666664), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.6666666666666666), ('mmmu/physics', 0.5666666666666667), ('mmmu/psychology', 0.5666666666666667), ('mmmu/public_health', 0.5333333333333333), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5355555555555556), ('mmmu/mllm_eval_accuracy', 0.5444444444444444)]\n",
      "Got result for docvqa - 5400: [('docvqa/anls_total_score', 0.641838160825989), ('docvqa/mllm_evaluation_anls_score', 0.6449672436995196), ('docvqa/mmllm_fixed_anls_score', 0.6834503558936926)]\n",
      "Got result for mathvista - 5400: [('mathvista/accuracy', 0.521)]\n",
      "Got result for ai2d - 5400: [('ai2d/accuracy', 0.881800518134715)]\n",
      "Got result for chartqa - 5400: [('chartqa/accuracy', 0.7923155418680999)]\n",
      "Got result for vqa - 5400: [('vqa/accuracy', 0.7171559999999774), ('vqa/recall', 0.7514839999999755), ('vqa/bleu', 0.017669549211859703), ('vqa/mllm_evaluation_accuracy', 0.7444439999999758)]\n",
      "Got result for textvqa - 5400: [('textvqa/accuracy', 65.89800000000032), ('textvqa/mllm_eval_accuracy', 71.17800000000035)]\n",
      "Got result for infographics_w_ocr - 5400: [('infographics_w_ocr/anls_total_score', 0.6534752684865679), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.6021788516045447), ('infographics_w_ocr/answer_type_multi_span_score', 0.46453368142480655), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6523244838344298), ('infographics_w_ocr/answer_type_question_span_score', 0.7082458673804828), ('infographics_w_ocr/answer_type_single_span_score', 0.6700545234450036), ('infographics_w_ocr/evidence_type_figure_score', 0.6353385068897828), ('infographics_w_ocr/evidence_type_map_score', 0.6222150272302772), ('infographics_w_ocr/evidence_type_table_list_score', 0.6473518115950101), ('infographics_w_ocr/evidence_type_text_score', 0.7060117909118188), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5728771112381088), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.670669313563149), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5553838676095528), ('infographics_w_ocr/reasoning_type_counting_score', 0.6194638694638694)]\n",
      "Got result for infographics - 5400: [('infographics/anls_total_score', 0.5298240142229065), ('infographics/mllm_evaluation_anls_score', 0.4791340211128352), ('infographics/answer_type_multi_span_score', 0.3592175234621677), ('infographics/answer_type_non_extractive_score', 0.5309911306294671), ('infographics/answer_type_question_span_score', 0.7102029914529914), ('infographics/answer_type_single_span_score', 0.5380994474916069), ('infographics/evidence_type_figure_score', 0.5153769918507295), ('infographics/evidence_type_map_score', 0.4659031460421583), ('infographics/evidence_type_table_list_score', 0.5035818660867833), ('infographics/evidence_type_text_score', 0.5754177098005614), ('infographics/evidence_type_visual_layout_score', 0.4902879714863689), ('infographics/reasoning_type_arithmetic_score', 0.4564660795825179), ('infographics/reasoning_type_comparison_score', 0.46085607299315806), ('infographics/reasoning_type_counting_score', 0.587995337995338)]\n",
      "Got result for mmbench - 5400: [('mmbench/attribute_comparison', 0.7272727272727273), ('mmbench/attribute_recognition', 0.9436619718309859), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.55), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.75), ('mmbench/object_localization', 0.5185185185185185), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.7733333333333333), ('mmbench/physical_relation', 0.7916666666666666), ('mmbench/spatial_relationship', 0.5), ('mmbench/structuralized_imagetext_understanding', 0.6753246753246753), ('mmbench/overall', 0.7902476313269925)]\n",
      "5400\n",
      "Got result for mmmu - 5600: [('mmmu/accounting', 0.4), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.26666666666666666), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.8), ('mmmu/basic_medical_science', 0.6), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.5333333333333333), ('mmmu/design', 0.6333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.43333333333333335), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.6666666666666666), ('mmmu/finance', 0.3333333333333333), ('mmmu/geography', 0.6), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8), ('mmmu/manage', 0.5666666666666667), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.16666666666666666), ('mmmu/math', 0.4), ('mmmu/mechanical_engineering', 0.43333333333333335), ('mmmu/music', 0.3), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.3333333333333333), ('mmmu/psychology', 0.5666666666666667), ('mmmu/public_health', 0.5333333333333333), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5211111111111111), ('mmmu/mllm_eval_accuracy', 0.5277777777777778)]\n",
      "Got result for docvqa - 5600: [('docvqa/anls_total_score', 0.648769674145729), ('docvqa/mllm_evaluation_anls_score', 0.6516133037863439), ('docvqa/mmllm_fixed_anls_score', 0.6893111146627985)]\n",
      "Got result for ai2d - 5600: [('ai2d/accuracy', 0.8950777202072538)]\n",
      "Got result for chartqa - 5600: [('chartqa/accuracy', 0.7982149435649808)]\n",
      "Got result for vqa - 5600: [('vqa/accuracy', 0.712295999999978), ('vqa/recall', 0.7495919999999758), ('vqa/bleu', 0.016216762363910675), ('vqa/mllm_evaluation_accuracy', 0.7411279999999764)]\n",
      "Got result for textvqa - 5600: [('textvqa/accuracy', 65.26600000000028), ('textvqa/mllm_eval_accuracy', 70.79200000000029)]\n",
      "Got result for infographics - 5600: [('infographics/anls_total_score', 0.5400153120712504), ('infographics/mllm_evaluation_anls_score', 0.4823149412166404), ('infographics/answer_type_multi_span_score', 0.3665127042660745), ('infographics/answer_type_non_extractive_score', 0.5309609919917335), ('infographics/answer_type_question_span_score', 0.7219516060862213), ('infographics/answer_type_single_span_score', 0.5520740334422093), ('infographics/evidence_type_figure_score', 0.5254525829818045), ('infographics/evidence_type_map_score', 0.541431198847439), ('infographics/evidence_type_table_list_score', 0.4973839635416816), ('infographics/evidence_type_text_score', 0.5912562903858297), ('infographics/evidence_type_visual_layout_score', 0.5122317063256603), ('infographics/reasoning_type_arithmetic_score', 0.4698793215916503), ('infographics/reasoning_type_comparison_score', 0.4466229466937155), ('infographics/reasoning_type_counting_score', 0.5772144522144522)]\n",
      "Got result for mmbench - 5600: [('mmbench/attribute_comparison', 0.6590909090909091), ('mmbench/attribute_recognition', 0.9295774647887324), ('mmbench/celebrity_recognition', 0.9494949494949495), ('mmbench/function_reasoning', 0.8734177215189873), ('mmbench/future_prediction', 0.525), ('mmbench/image_quality', 0.5609756097560976), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.8541666666666666), ('mmbench/object_localization', 0.5555555555555556), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.7466666666666667), ('mmbench/physical_relation', 0.7083333333333334), ('mmbench/spatial_relationship', 0.5), ('mmbench/structuralized_imagetext_understanding', 0.7012987012987013), ('mmbench/overall', 0.7878702280782688)]\n",
      "5600\n",
      "Got result for mmmu - 5800: [('mmmu/accounting', 0.6), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.36666666666666664), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.7), ('mmmu/basic_medical_science', 0.6333333333333333), ('mmmu/biology', 0.36666666666666664), ('mmmu/chemistry', 0.3), ('mmmu/clinical_medicine', 0.7), ('mmmu/computer_science', 0.5333333333333333), ('mmmu/design', 0.7), ('mmmu/diagnostics_and_laboratory_medicine', 0.4666666666666667), ('mmmu/economics', 0.5666666666666667), ('mmmu/electronics', 0.3), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.43333333333333335), ('mmmu/history', 0.8333333333333334), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.6333333333333333), ('mmmu/materials', 0.36666666666666664), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.23333333333333334), ('mmmu/pharmacy', 0.7333333333333333), ('mmmu/physics', 0.4), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.6), ('mmmu/sociology', 0.6), ('mmmu/accuracy', 0.5388888888888889), ('mmmu/mllm_eval_accuracy', 0.55)]\n",
      "Got result for docvqa - 5800: [('docvqa/anls_total_score', 0.6481590973298108), ('docvqa/mllm_evaluation_anls_score', 0.6517799632443199), ('docvqa/mmllm_fixed_anls_score', 0.6950030568201728)]\n",
      "Got result for mathvista - 5800: [('mathvista/accuracy', 0.565)]\n",
      "Got result for ai2d - 5800: [('ai2d/accuracy', 0.8908678756476683)]\n",
      "Got result for chartqa - 5800: [('chartqa/accuracy', 0.800995224418768)]\n",
      "Got result for vqa - 5800: [('vqa/accuracy', 0.7168999999999779), ('vqa/recall', 0.7557119999999755), ('vqa/bleu', 0.0201967004686594), ('vqa/mllm_evaluation_accuracy', 0.7471439999999756)]\n",
      "Got result for textvqa - 5800: [('textvqa/accuracy', 65.32000000000028), ('textvqa/mllm_eval_accuracy', 71.02000000000031)]\n",
      "Got result for infographics_w_ocr - 5800: [('infographics_w_ocr/anls_total_score', 0.6400898567522709), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5890014464636287), ('infographics_w_ocr/answer_type_multi_span_score', 0.47299291825094913), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6396301558598124), ('infographics_w_ocr/answer_type_question_span_score', 0.699496336996337), ('infographics_w_ocr/answer_type_single_span_score', 0.6544817443493565), ('infographics_w_ocr/evidence_type_figure_score', 0.6234140801306912), ('infographics_w_ocr/evidence_type_map_score', 0.5936985723459224), ('infographics_w_ocr/evidence_type_table_list_score', 0.6338705344895262), ('infographics_w_ocr/evidence_type_text_score', 0.6928315164375424), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5748734071875359), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.644059849967384), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5279096383180244), ('infographics_w_ocr/reasoning_type_counting_score', 0.6241796664873588)]\n",
      "Got result for infographics - 5800: [('infographics/anls_total_score', 0.522499501173765), ('infographics/mllm_evaluation_anls_score', 0.4721290742230477), ('infographics/answer_type_multi_span_score', 0.3720889846584698), ('infographics/answer_type_non_extractive_score', 0.4864955719386098), ('infographics/answer_type_question_span_score', 0.7229894571240725), ('infographics/answer_type_single_span_score', 0.539040904972798), ('infographics/evidence_type_figure_score', 0.5052817739858165), ('infographics/evidence_type_map_score', 0.4599203673131576), ('infographics/evidence_type_table_list_score', 0.49032609281058714), ('infographics/evidence_type_text_score', 0.5713060250104918), ('infographics/evidence_type_visual_layout_score', 0.4803694002151992), ('infographics/reasoning_type_arithmetic_score', 0.38252985598876027), ('infographics/reasoning_type_comparison_score', 0.44829671664233195), ('infographics/reasoning_type_counting_score', 0.5746503496503497)]\n",
      "Got result for mmbench - 5800: [('mmbench/attribute_comparison', 0.7045454545454546), ('mmbench/attribute_recognition', 0.9436619718309859), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.575), ('mmbench/image_quality', 0.5853658536585366), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9245283018867925), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.5679012345679012), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.8), ('mmbench/physical_relation', 0.625), ('mmbench/spatial_relationship', 0.45), ('mmbench/structuralized_imagetext_understanding', 0.6883116883116883), ('mmbench/overall', 0.7843912396018498)]\n",
      "5800\n",
      "Got result for mmmu - 6000: [('mmmu/accounting', 0.43333333333333335), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.4666666666666667), ('mmmu/art', 0.6), ('mmmu/art_theory', 0.6333333333333333), ('mmmu/basic_medical_science', 0.6333333333333333), ('mmmu/biology', 0.43333333333333335), ('mmmu/chemistry', 0.4666666666666667), ('mmmu/clinical_medicine', 0.7), ('mmmu/computer_science', 0.5666666666666667), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.4), ('mmmu/economics', 0.6), ('mmmu/electronics', 0.36666666666666664), ('mmmu/energy_and_power', 0.6333333333333333), ('mmmu/finance', 0.5), ('mmmu/geography', 0.5), ('mmmu/history', 0.8), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.5), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.5666666666666667), ('mmmu/music', 0.26666666666666666), ('mmmu/pharmacy', 0.6333333333333333), ('mmmu/physics', 0.5), ('mmmu/psychology', 0.6), ('mmmu/public_health', 0.7), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5522222222222222), ('mmmu/mllm_eval_accuracy', 0.5544444444444444)]\n",
      "Got result for docvqa - 6000: [('docvqa/anls_total_score', 0.6225563382726894), ('docvqa/mllm_evaluation_anls_score', 0.624743853312618), ('docvqa/mmllm_fixed_anls_score', 0.6647037973554452)]\n",
      "Got result for mathvista - 6000: [('mathvista/accuracy', 0.55)]\n",
      "Got result for ai2d - 6000: [('ai2d/accuracy', 0.9028497409326425)]\n",
      "Got result for chartqa - 6000: [('chartqa/accuracy', 0.8121336186938924)]\n",
      "Got result for vqa - 6000: [('vqa/accuracy', 0.709799999999979), ('vqa/recall', 0.7481919999999755), ('vqa/bleu', 0.01966903917491436), ('vqa/mllm_evaluation_accuracy', 0.7396079999999767)]\n",
      "Got result for textvqa - 6000: [('textvqa/accuracy', 65.2180000000003), ('textvqa/mllm_eval_accuracy', 70.89000000000036)]\n",
      "Got result for infographics_w_ocr - 6000: [('infographics_w_ocr/anls_total_score', 0.6484606018739255), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5936063423998835), ('infographics_w_ocr/answer_type_multi_span_score', 0.4962421882450204), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6318134848876263), ('infographics_w_ocr/answer_type_question_span_score', 0.7085892739738894), ('infographics_w_ocr/answer_type_single_span_score', 0.6662293108259586), ('infographics_w_ocr/evidence_type_figure_score', 0.6304430156216781), ('infographics_w_ocr/evidence_type_map_score', 0.5967402278628253), ('infographics_w_ocr/evidence_type_table_list_score', 0.6467023727015322), ('infographics_w_ocr/evidence_type_text_score', 0.7070964945132682), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5511326870043679), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6450097847358117), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5230213169716028), ('infographics_w_ocr/reasoning_type_counting_score', 0.6058275058275057)]\n",
      "Got result for infographics - 6000: [('infographics/anls_total_score', 0.5235667422686492), ('infographics/mllm_evaluation_anls_score', 0.4750291600083593), ('infographics/answer_type_multi_span_score', 0.3903511578328981), ('infographics/answer_type_non_extractive_score', 0.4902410428631043), ('infographics/answer_type_question_span_score', 0.7260760073260073), ('infographics/answer_type_single_span_score', 0.5375230592087112), ('infographics/evidence_type_figure_score', 0.5082912169155553), ('infographics/evidence_type_map_score', 0.5104128711914946), ('infographics/evidence_type_table_list_score', 0.5032189065294689), ('infographics/evidence_type_text_score', 0.5659207124343892), ('infographics/evidence_type_visual_layout_score', 0.4882887255570725), ('infographics/reasoning_type_arithmetic_score', 0.4050341211300117), ('infographics/reasoning_type_comparison_score', 0.4472756110076091), ('infographics/reasoning_type_counting_score', 0.5608974358974359)]\n",
      "Got result for mmbench - 6000: [('mmbench/attribute_comparison', 0.7045454545454546), ('mmbench/attribute_recognition', 0.9436619718309859), ('mmbench/celebrity_recognition', 0.9292929292929293), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.475), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8888888888888888), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.5308641975308642), ('mmbench/ocr', 0.8461538461538461), ('mmbench/physical_property_reasoning', 0.7733333333333333), ('mmbench/physical_relation', 0.7083333333333334), ('mmbench/spatial_relationship', 0.5), ('mmbench/structuralized_imagetext_understanding', 0.6883116883116883), ('mmbench/overall', 0.786427401306212)]\n",
      "6000\n",
      "Got result for mmmu - 6200: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.6), ('mmmu/architecture_and_engineering', 0.3333333333333333), ('mmmu/art', 0.6333333333333333), ('mmmu/art_theory', 0.7333333333333333), ('mmmu/basic_medical_science', 0.7333333333333333), ('mmmu/biology', 0.4), ('mmmu/chemistry', 0.3333333333333333), ('mmmu/clinical_medicine', 0.6333333333333333), ('mmmu/computer_science', 0.5), ('mmmu/design', 0.8), ('mmmu/diagnostics_and_laboratory_medicine', 0.5), ('mmmu/economics', 0.5333333333333333), ('mmmu/electronics', 0.43333333333333335), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.43333333333333335), ('mmmu/geography', 0.4666666666666667), ('mmmu/history', 0.7666666666666667), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.5), ('mmmu/marketing', 0.5666666666666667), ('mmmu/materials', 0.23333333333333334), ('mmmu/math', 0.43333333333333335), ('mmmu/mechanical_engineering', 0.4666666666666667), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.36666666666666664), ('mmmu/psychology', 0.6333333333333333), ('mmmu/public_health', 0.5666666666666667), ('mmmu/sociology', 0.7), ('mmmu/accuracy', 0.5433333333333333), ('mmmu/mllm_eval_accuracy', 0.5477777777777778)]\n",
      "Got result for docvqa - 6200: [('docvqa/anls_total_score', 0.6320537685629557), ('docvqa/mllm_evaluation_anls_score', 0.6348844596118374), ('docvqa/mmllm_fixed_anls_score', 0.6794462520691361)]\n",
      "Got result for mathvista - 6200: [('mathvista/accuracy', 0.564)]\n",
      "Got result for ai2d - 6200: [('ai2d/accuracy', 0.8957253886010362)]\n",
      "Got result for chartqa - 6200: [('chartqa/accuracy', 0.8031163518326795)]\n",
      "Got result for vqa - 6200: [('vqa/accuracy', 0.7085679999999783), ('vqa/recall', 0.7410039999999772), ('vqa/bleu', 0.0), ('vqa/mllm_evaluation_accuracy', 0.7325559999999772)]\n",
      "Got result for textvqa - 6200: [('textvqa/accuracy', 65.12400000000031), ('textvqa/mllm_eval_accuracy', 70.84400000000032)]\n",
      "Got result for infographics_w_ocr - 6200: [('infographics_w_ocr/anls_total_score', 0.643966629360914), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5957240252661248), ('infographics_w_ocr/answer_type_multi_span_score', 0.46594457020149255), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6504413157668132), ('infographics_w_ocr/answer_type_question_span_score', 0.6997751714097868), ('infographics_w_ocr/answer_type_single_span_score', 0.6568290867399764), ('infographics_w_ocr/evidence_type_figure_score', 0.6295221460858975), ('infographics_w_ocr/evidence_type_map_score', 0.5977059699897115), ('infographics_w_ocr/evidence_type_table_list_score', 0.6337661265614828), ('infographics_w_ocr/evidence_type_text_score', 0.6937912821607473), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5479495531133022), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6414179712981082), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5392213666987133), ('infographics_w_ocr/reasoning_type_counting_score', 0.6407925407925408)]\n",
      "Got result for infographics - 6200: [('infographics/anls_total_score', 0.5277842205193978), ('infographics/mllm_evaluation_anls_score', 0.4753568248826356), ('infographics/answer_type_multi_span_score', 0.36128719944509285), ('infographics/answer_type_non_extractive_score', 0.510042459047884), ('infographics/answer_type_question_span_score', 0.6908959096459096), ('infographics/answer_type_single_span_score', 0.5424217935048695), ('infographics/evidence_type_figure_score', 0.5158807171960773), ('infographics/evidence_type_map_score', 0.5100828381881943), ('infographics/evidence_type_table_list_score', 0.5015272827284045), ('infographics/evidence_type_text_score', 0.5716506873281096), ('infographics/evidence_type_visual_layout_score', 0.49125836494781006), ('infographics/reasoning_type_arithmetic_score', 0.42712378945255675), ('infographics/reasoning_type_comparison_score', 0.4549009088925855), ('infographics/reasoning_type_counting_score', 0.5769230769230769)]\n",
      "Got result for mmbench - 6200: [('mmbench/attribute_comparison', 0.6818181818181818), ('mmbench/attribute_recognition', 0.9436619718309859), ('mmbench/celebrity_recognition', 0.9292929292929293), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6341463414634146), ('mmbench/image_scene', 0.9615384615384616), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.8611111111111112), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.5679012345679012), ('mmbench/ocr', 0.7948717948717948), ('mmbench/physical_property_reasoning', 0.7466666666666667), ('mmbench/physical_relation', 0.7083333333333334), ('mmbench/spatial_relationship', 0.55), ('mmbench/structuralized_imagetext_understanding', 0.6493506493506493), ('mmbench/overall', 0.7885844387132492)]\n",
      "6200\n",
      "Got result for mmmu - 6400: [('mmmu/accounting', 0.5), ('mmmu/agriculture', 0.5666666666666667), ('mmmu/architecture_and_engineering', 0.43333333333333335), ('mmmu/art', 0.6666666666666666), ('mmmu/art_theory', 0.7333333333333333), ('mmmu/basic_medical_science', 0.6), ('mmmu/biology', 0.26666666666666666), ('mmmu/chemistry', 0.3), ('mmmu/clinical_medicine', 0.6), ('mmmu/computer_science', 0.6), ('mmmu/design', 0.7333333333333333), ('mmmu/diagnostics_and_laboratory_medicine', 0.36666666666666664), ('mmmu/economics', 0.6333333333333333), ('mmmu/electronics', 0.4), ('mmmu/energy_and_power', 0.5333333333333333), ('mmmu/finance', 0.4666666666666667), ('mmmu/geography', 0.5), ('mmmu/history', 0.7333333333333333), ('mmmu/literature', 0.8666666666666667), ('mmmu/manage', 0.4666666666666667), ('mmmu/marketing', 0.5333333333333333), ('mmmu/materials', 0.3333333333333333), ('mmmu/math', 0.5333333333333333), ('mmmu/mechanical_engineering', 0.36666666666666664), ('mmmu/music', 0.4), ('mmmu/pharmacy', 0.7), ('mmmu/physics', 0.5666666666666667), ('mmmu/psychology', 0.6666666666666666), ('mmmu/public_health', 0.6333333333333333), ('mmmu/sociology', 0.6333333333333333), ('mmmu/accuracy', 0.5444444444444444), ('mmmu/mllm_eval_accuracy', 0.5488888888888889)]\n",
      "Got result for docvqa - 6400: [('docvqa/anls_total_score', 0.6456780104551609), ('docvqa/mllm_evaluation_anls_score', 0.6472735744733693), ('docvqa/mmllm_fixed_anls_score', 0.6880354760223297)]\n",
      "Got result for mathvista - 6400: [('mathvista/accuracy', 0.581)]\n",
      "Got result for ai2d - 6400: [('ai2d/accuracy', 0.9034974093264249)]\n",
      "Got result for chartqa - 6400: [('chartqa/accuracy', 0.8083823765033065)]\n",
      "Got result for vqa - 6400: [('vqa/accuracy', 0.7016279999999785), ('vqa/recall', 0.736695999999977), ('vqa/bleu', 0.013117595575749874), ('vqa/mllm_evaluation_accuracy', 0.7285679999999773)]\n",
      "Got result for textvqa - 6400: [('textvqa/accuracy', 64.49800000000033), ('textvqa/mllm_eval_accuracy', 70.66200000000035)]\n",
      "Got result for infographics_w_ocr - 6400: [('infographics_w_ocr/anls_total_score', 0.6480874088053854), ('infographics_w_ocr/mllm_evaluation_anls_score', 0.5970853112819781), ('infographics_w_ocr/answer_type_multi_span_score', 0.47117033310368694), ('infographics_w_ocr/answer_type_non_extractive_score', 0.6514423490915355), ('infographics_w_ocr/answer_type_question_span_score', 0.731547619047619), ('infographics_w_ocr/answer_type_single_span_score', 0.6596650324288442), ('infographics_w_ocr/evidence_type_figure_score', 0.6357192647034131), ('infographics_w_ocr/evidence_type_map_score', 0.6041416135540679), ('infographics_w_ocr/evidence_type_table_list_score', 0.6414376624419228), ('infographics_w_ocr/evidence_type_text_score', 0.6957928450353565), ('infographics_w_ocr/evidence_type_visual_layout_score', 0.5537735320747905), ('infographics_w_ocr/reasoning_type_arithmetic_score', 0.6584964122635353), ('infographics_w_ocr/reasoning_type_comparison_score', 0.5362850732163409), ('infographics_w_ocr/reasoning_type_counting_score', 0.6287878787878787)]\n",
      "Got result for infographics - 6400: [('infographics/anls_total_score', 0.5346366444805102), ('infographics/mllm_evaluation_anls_score', 0.48217456418720644), ('infographics/answer_type_multi_span_score', 0.4160775137549833), ('infographics/answer_type_non_extractive_score', 0.49788151872238673), ('infographics/answer_type_question_span_score', 0.7063227904574059), ('infographics/answer_type_single_span_score', 0.5485397858341805), ('infographics/evidence_type_figure_score', 0.5215268135623273), ('infographics/evidence_type_map_score', 0.5211389437987555), ('infographics/evidence_type_table_list_score', 0.5077253737290466), ('infographics/evidence_type_text_score', 0.5712222300962516), ('infographics/evidence_type_visual_layout_score', 0.5074194691454754), ('infographics/reasoning_type_arithmetic_score', 0.4040358899091776), ('infographics/reasoning_type_comparison_score', 0.4635989494635539), ('infographics/reasoning_type_counting_score', 0.581002331002331)]\n",
      "Got result for mmbench - 6400: [('mmbench/attribute_comparison', 0.7045454545454546), ('mmbench/attribute_recognition', 0.9436619718309859), ('mmbench/celebrity_recognition', 0.9393939393939394), ('mmbench/function_reasoning', 0.8607594936708861), ('mmbench/future_prediction', 0.6), ('mmbench/image_quality', 0.6097560975609756), ('mmbench/image_scene', 0.9711538461538461), ('mmbench/image_style', 0.9056603773584906), ('mmbench/image_topic', 0.9166666666666666), ('mmbench/nature_relation', 0.7916666666666666), ('mmbench/object_localization', 0.5432098765432098), ('mmbench/ocr', 0.8205128205128205), ('mmbench/physical_property_reasoning', 0.76), ('mmbench/physical_relation', 0.75), ('mmbench/spatial_relationship', 0.5), ('mmbench/structuralized_imagetext_understanding', 0.7142857142857143), ('mmbench/overall', 0.7977350502215844)]\n",
      "6400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmmu_v2</th>\n",
       "      <th>mmmu_v1</th>\n",
       "      <th>docvqa</th>\n",
       "      <th>mathvista</th>\n",
       "      <th>ai2d</th>\n",
       "      <th>chartqa</th>\n",
       "      <th>vqa</th>\n",
       "      <th>textvqa</th>\n",
       "      <th>infographics_w_ocr</th>\n",
       "      <th>infographics</th>\n",
       "      <th>mmbench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.5444</td>\n",
       "      <td>0.5433</td>\n",
       "      <td>0.6706</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.8488</td>\n",
       "      <td>0.7014</td>\n",
       "      <td>0.7256</td>\n",
       "      <td>0.66916</td>\n",
       "      <td>0.6587</td>\n",
       "      <td>0.5542</td>\n",
       "      <td>0.7887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>0.5422</td>\n",
       "      <td>0.5278</td>\n",
       "      <td>0.6631</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.8724</td>\n",
       "      <td>0.7788</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.66484</td>\n",
       "      <td>0.6527</td>\n",
       "      <td>0.5203</td>\n",
       "      <td>0.7947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5400</th>\n",
       "      <td>0.5444</td>\n",
       "      <td>0.5356</td>\n",
       "      <td>0.6418</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.7923</td>\n",
       "      <td>0.7172</td>\n",
       "      <td>0.65898</td>\n",
       "      <td>0.6535</td>\n",
       "      <td>0.5298</td>\n",
       "      <td>0.7902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5600</th>\n",
       "      <td>0.5278</td>\n",
       "      <td>0.5211</td>\n",
       "      <td>0.6488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8951</td>\n",
       "      <td>0.7982</td>\n",
       "      <td>0.7123</td>\n",
       "      <td>0.65266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.7879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5800</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5389</td>\n",
       "      <td>0.6482</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.7169</td>\n",
       "      <td>0.6532</td>\n",
       "      <td>0.6401</td>\n",
       "      <td>0.5225</td>\n",
       "      <td>0.7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.5522</td>\n",
       "      <td>0.6226</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.9028</td>\n",
       "      <td>0.8121</td>\n",
       "      <td>0.7098</td>\n",
       "      <td>0.65218</td>\n",
       "      <td>0.6485</td>\n",
       "      <td>0.5236</td>\n",
       "      <td>0.7864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6200</th>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.5433</td>\n",
       "      <td>0.6321</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.8957</td>\n",
       "      <td>0.8031</td>\n",
       "      <td>0.7086</td>\n",
       "      <td>0.65124</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.5278</td>\n",
       "      <td>0.7886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6400</th>\n",
       "      <td>0.5489</td>\n",
       "      <td>0.5444</td>\n",
       "      <td>0.6457</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.9035</td>\n",
       "      <td>0.8084</td>\n",
       "      <td>0.7016</td>\n",
       "      <td>0.64498</td>\n",
       "      <td>0.6481</td>\n",
       "      <td>0.5346</td>\n",
       "      <td>0.7977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mmmu_v2 mmmu_v1  docvqa mathvista    ai2d chartqa     vqa  textvqa  \\\n",
       "5000  0.5444  0.5433  0.6706     0.476  0.8488  0.7014  0.7256  0.66916   \n",
       "5200  0.5422  0.5278  0.6631     0.524  0.8724  0.7788   0.728  0.66484   \n",
       "5400  0.5444  0.5356  0.6418     0.521  0.8818  0.7923  0.7172  0.65898   \n",
       "5600  0.5278  0.5211  0.6488       NaN  0.8951  0.7982  0.7123  0.65266   \n",
       "5800    0.55  0.5389  0.6482     0.565  0.8909   0.801  0.7169   0.6532   \n",
       "6000  0.5544  0.5522  0.6226      0.55  0.9028  0.8121  0.7098  0.65218   \n",
       "6200  0.5478  0.5433  0.6321     0.564  0.8957  0.8031  0.7086  0.65124   \n",
       "6400  0.5489  0.5444  0.6457     0.581  0.9035  0.8084  0.7016  0.64498   \n",
       "\n",
       "     infographics_w_ocr infographics mmbench  \n",
       "5000             0.6587       0.5542  0.7887  \n",
       "5200             0.6527       0.5203  0.7947  \n",
       "5400             0.6535       0.5298  0.7902  \n",
       "5600                NaN         0.54  0.7879  \n",
       "5800             0.6401       0.5225  0.7844  \n",
       "6000             0.6485       0.5236  0.7864  \n",
       "6200              0.644       0.5278  0.7886  \n",
       "6400             0.6481       0.5346  0.7977  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_helper.get_eval_scores_all(\"/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_64nodes_i18n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
