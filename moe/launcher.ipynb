{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host name: submit-1\n",
      "Number of CPUs: 32\n",
      "Total memory (GB): 247.74\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import socket\n",
    "import psutil\n",
    "import sys \n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "lib_path = ['/fsx_0/user/tranx/experiments']\n",
    "import_paths = [\n",
    "    \"/fsx_0/user/tranx/rsync\",\n",
    "    # \"/fsx_0/user/tranx/experiments/lib\"\n",
    "]\n",
    "for path in import_paths:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "from llm_mm_aligner.experiments.aws import launch_job\n",
    "# import utils\n",
    "    \n",
    "hostname = socket.gethostname()\n",
    "print(\"Host name:\", hostname)\n",
    "num_cpus = psutil.cpu_count()\n",
    "print(\"Number of CPUs:\", num_cpus)\n",
    "total_memory = psutil.virtual_memory().total / (1024 ** 3)\n",
    "print(\"Total memory (GB):\", round(total_memory, 2))\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "    config: str,\n",
    "    overrides_dict: None,\n",
    "    nodes: int = 1,\n",
    "    config_base: str = \"/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws\",\n",
    "    qos: str = \"ar-ai-hipri\",\n",
    "    conda_env: str = \"/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822\"\n",
    "):\n",
    "    config_file = os.path.join(config_base, config)\n",
    "    \n",
    "    overrides = [[\n",
    "        (\"slurm_args.qos\", qos),\n",
    "        (\"slurm_args.account\", qos),\n",
    "        (\"slurm_args.nodes\", nodes)\n",
    "    ]]\n",
    "    \n",
    "    if overrides_dict is not None:\n",
    "        for k, v in overrides_dict.items():\n",
    "            overrides[0].append((\n",
    "                k, v\n",
    "            ))\n",
    "    \n",
    "    launch_job.run_job(\n",
    "        config_file = config_file,\n",
    "        conda_env = conda_env,\n",
    "        overrides = overrides\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Test limit on batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1/241027_03_45_58_614409/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1/241027_03_45_58_614409/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1/241027_03_45_58_614409/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1/241027_03_45_58_614409/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1/241027_03_45_58_614409/config.json\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1/241027_03_45_58_614409/launch_script.sh\n"
     ]
    }
   ],
   "source": [
    "run(\n",
    "    config=\"mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_8x1.json\",\n",
    "    nodes=8,\n",
    "    overrides_dict={\n",
    "        \"trainer_args.output_dir\": \"/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = \"mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_8x1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/config.json\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/launch_script.sh\n"
     ]
    }
   ],
   "source": [
    "run(\n",
    "    config=base_config,\n",
    "    nodes=8,\n",
    "    overrides_dict={\n",
    "        \"trainer_args.gradient_accumulation_steps\": 2,\n",
    "        \"trainer_args.output_dir\": \"/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bz in [16, 32]:\n",
    "    run(\n",
    "        config=f\"mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_{bz}x1.json\",\n",
    "        nodes=8,\n",
    "        overrides_dict={\n",
    "            \"trainer_args.output_dir\": \"/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1\"\n",
    "        }\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
