{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host name: submit-1\n",
      "Number of CPUs: 32\n",
      "Total memory (GB): 247.74\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import socket\n",
    "import psutil\n",
    "import sys \n",
    "# lib_path = '/fsx_0/user/tranx/oss/moe'\n",
    "# if lib_path not in sys.path:\n",
    "#     sys.path.append(lib_path)\n",
    "    \n",
    "hostname = socket.gethostname()\n",
    "print(\"Host name:\", hostname)\n",
    "num_cpus = psutil.cpu_count()\n",
    "print(\"Number of CPUs:\", num_cpus)\n",
    "total_memory = psutil.virtual_memory().total / (1024 ** 3)\n",
    "print(\"Total memory (GB):\", round(total_memory, 2))\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "def initialize_moe(moe, candidates, num_layers: int = 4, num_experts: int = 4):\n",
    "    \"\"\"\n",
    "    Initialize the MoE (Mixture of Experts) model from existing experts.\n",
    "\n",
    "    It will assume the experts are already `packed`, i.e. they will have\n",
    "    two tensors, `w12` and `w3`. The `w12` tensor will be used for the first two layers,'\n",
    "    and the `w3` tensor will be used for the last two layers.\n",
    "\n",
    "    Args:\n",
    "        moe (pytorch state dictionary): The MoE model to be initialized.\n",
    "        candidates (list): A list of candidate `statedicts` to be used for initialization.\n",
    "                    len(candidates) = num_experts\n",
    "        num_layers (int, optional): The number of layers in the MoE model. Defaults to 4.\n",
    "        num_experts (int, optional): The number of experts in the MoE model. Defaults to 4.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    discarded_tensors = set()\n",
    "    expert = candidates[0]\n",
    "\n",
    "    moe_num_params = sum(p.numel() for p in moe.values())\n",
    "    candidate_num_params = sum(p.numel() for p in expert.values())\n",
    "    print(\n",
    "        f\"Number of parameters in MoE: {moe_num_params} ({moe_num_params//1e9} B), in candidate: {candidate_num_params} ({candidate_num_params//1e9} B)\"\n",
    "    )\n",
    "\n",
    "    for key in expert.keys():\n",
    "        if key in moe.keys() and expert[key].shape == moe[key].shape:\n",
    "            print(\"matched keys:\", key, expert[key].shape)\n",
    "            moe[key] = expert[key].clone().detach()\n",
    "        else:\n",
    "            discarded_tensors.add(key)\n",
    "\n",
    "    print(\"Update weights for each layer and expert\")\n",
    "    for layer_id in range(num_layers):\n",
    "        print(f\"Layer {layer_id}\")\n",
    "        for expert_id in range(num_experts):\n",
    "            print(f\"    expert {expert_id}\")\n",
    "            for weight_suffix in [\"w12.weight\", \"w3.weight\"]:\n",
    "                tensor_name = (\n",
    "                    f\"module.layers.{layer_id}.ff.experts.{expert_id}.{weight_suffix}\"\n",
    "                )\n",
    "                candidate_weight = f\"module.layers.{layer_id}.ff.{weight_suffix}\"\n",
    "                moe[tensor_name] = (\n",
    "                    candidates[expert_id][candidate_weight].clone().detach()\n",
    "                )\n",
    "\n",
    "    print(\"Update position embeddings and image end of input token\") \n",
    "    num_image_tokens = moe[\"module.pos_embs\"].shape[0]\n",
    "    for key in [\"module.pos_embs\", \"module.image_eoi\"]:\n",
    "        moe[key] = candidates[0][key][:num_image_tokens, ...].clone().detach()\n",
    "\n",
    "    print(f\"Discarded tensors: {discarded_tensors}\")\n",
    "\n",
    "\n",
    "def initialize_perceiver_model(\n",
    "    moe_checkpoint_path: str,\n",
    "    candidate_checkpoints_path: list[str],\n",
    "    output_checkpoint_path: str,\n",
    "    num_experts: int = 4,\n",
    "    num_layers: int = 22,\n",
    ") -> None:\n",
    "    \"\"\"Initialize the Perceiver model.\"\"\"\n",
    "\n",
    "    print(f\"Loading MoE template: {moe_checkpoint_path}\")\n",
    "    moe = torch.load(moe_checkpoint_path)\n",
    "    \n",
    "    candidates = []\n",
    "    for i, candidate_checkpoint in enumerate(candidate_checkpoints_path):\n",
    "        print(f\"Loading candidate for expert #{i}\")\n",
    "        candidate = torch.load(candidate_checkpoint)\n",
    "        candidates.append(candidate)\n",
    "\n",
    "    print(\"Initializing MoE\")\n",
    "    initialize_moe(moe, candidates, num_layers=num_layers, num_experts=num_experts)\n",
    "    \n",
    "    print(f\"Saving initialized MoE to {output_checkpoint_path}\")\n",
    "    initialization_log = {\n",
    "        \"moe_checkpoint_path\": moe_checkpoint_path,\n",
    "        \"candidate_checkpoints_path\": candidate_checkpoints_path,\n",
    "        \"num_experts\": num_experts,\n",
    "        \"num_layers\": num_layers\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(output_checkpoint_path)\n",
    "    except FileExistsError:\n",
    "        raise FileExistsError(f\"Directory '{output_checkpoint_path}' already exists.\")\n",
    "    \n",
    "    torch.save(moe, f\"{output_checkpoint_path}/perception_tokenizer.pt\")\n",
    "    \n",
    "    with open(f\"{output_checkpoint_path}/initialization_log.json\", \"w\") as f:\n",
    "        json.dump(initialization_log, f, indent=4)\n",
    "    \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perception_tokenizer_22x8x2.pt\t    perception_tokenizer_moe_22x4x2.pt\n",
      "perception_tokenizer_moe_22x2x1.pt\n"
     ]
    }
   ],
   "source": [
    "! ls /fsx_3/bucket/tranx/moe/templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_checkpoint=\"/fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1_idl/checkpoint-17800/perception_tokenizer.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MoE template: /fsx_3/bucket/tranx/moe/templates/perception_tokenizer_moe_22x4x2.pt\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_layers = 22\n",
    "num_experts = 4\n",
    "initialize_perceiver_model(\n",
    "    moe_checkpoint_path=f\"/fsx_3/bucket/tranx/moe/templates/perception_tokenizer_moe_22x{num_experts}x2.pt\",\n",
    "    candidate_checkpoints_path=[stage1_checkpoint]*num_experts,\n",
    "    output_checkpoint_path=f\"/fsx_3/bucket/tranx/moe/initialized/perception_tokenizer_mm10.1_17800_22x{num_experts}\",\n",
    "    num_experts=num_experts,\n",
    "    num_layers=num_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px/checkpoint-10000/perception_tokenizer.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.latents : torch.Size([1, 64, 4096])\n",
      "module.pos_embs : torch.Size([10, 1, 4096])\n",
      "module.image_eoi : torch.Size([1, 10, 1, 4096])\n",
      "module.chunk_gate : torch.Size([1])\n",
      "module.average_tok_rms : torch.Size([])\n",
      "module.in_projector.norm.weight : torch.Size([1408])\n",
      "module.in_projector.proj.weight : torch.Size([4096, 1408])\n",
      "module.layers.0.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.0.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.0.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.0.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.0.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.0.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.0.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.0.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.0.media_norm.weight : torch.Size([4096])\n",
      "module.layers.0.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.0.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.1.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.1.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.1.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.1.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.1.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.1.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.1.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.1.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.1.media_norm.weight : torch.Size([4096])\n",
      "module.layers.1.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.1.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.2.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.2.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.2.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.2.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.2.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.2.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.2.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.2.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.2.media_norm.weight : torch.Size([4096])\n",
      "module.layers.2.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.2.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.3.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.3.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.3.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.3.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.3.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.3.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.3.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.3.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.3.media_norm.weight : torch.Size([4096])\n",
      "module.layers.3.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.3.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.4.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.4.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.4.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.4.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.4.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.4.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.4.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.4.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.4.media_norm.weight : torch.Size([4096])\n",
      "module.layers.4.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.4.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.5.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.5.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.5.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.5.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.5.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.5.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.5.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.5.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.5.media_norm.weight : torch.Size([4096])\n",
      "module.layers.5.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.5.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.6.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.6.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.6.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.6.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.6.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.6.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.6.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.6.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.6.media_norm.weight : torch.Size([4096])\n",
      "module.layers.6.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.6.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.7.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.7.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.7.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.7.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.7.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.7.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.7.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.7.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.7.media_norm.weight : torch.Size([4096])\n",
      "module.layers.7.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.7.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.8.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.8.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.8.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.8.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.8.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.8.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.8.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.8.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.8.media_norm.weight : torch.Size([4096])\n",
      "module.layers.8.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.8.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.9.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.9.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.9.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.9.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.9.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.9.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.9.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.9.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.9.media_norm.weight : torch.Size([4096])\n",
      "module.layers.9.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.9.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.10.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.10.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.10.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.10.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.10.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.10.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.10.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.10.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.10.media_norm.weight : torch.Size([4096])\n",
      "module.layers.10.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.10.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.11.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.11.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.11.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.11.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.11.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.11.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.11.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.11.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.11.media_norm.weight : torch.Size([4096])\n",
      "module.layers.11.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.11.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.12.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.12.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.12.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.12.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.12.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.12.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.12.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.12.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.12.media_norm.weight : torch.Size([4096])\n",
      "module.layers.12.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.12.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.13.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.13.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.13.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.13.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.13.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.13.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.13.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.13.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.13.media_norm.weight : torch.Size([4096])\n",
      "module.layers.13.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.13.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.14.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.14.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.14.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.14.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.14.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.14.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.14.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.14.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.14.media_norm.weight : torch.Size([4096])\n",
      "module.layers.14.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.14.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.15.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.15.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.15.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.15.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.15.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.15.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.15.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.15.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.15.media_norm.weight : torch.Size([4096])\n",
      "module.layers.15.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.15.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.16.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.16.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.16.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.16.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.16.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.16.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.16.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.16.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.16.media_norm.weight : torch.Size([4096])\n",
      "module.layers.16.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.16.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.17.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.17.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.17.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.17.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.17.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.17.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.17.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.17.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.17.media_norm.weight : torch.Size([4096])\n",
      "module.layers.17.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.17.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.18.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.18.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.18.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.18.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.18.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.18.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.18.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.18.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.18.media_norm.weight : torch.Size([4096])\n",
      "module.layers.18.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.18.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.19.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.19.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.19.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.19.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.19.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.19.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.19.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.19.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.19.media_norm.weight : torch.Size([4096])\n",
      "module.layers.19.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.19.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.20.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.20.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.20.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.20.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.20.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.20.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.20.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.20.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.20.media_norm.weight : torch.Size([4096])\n",
      "module.layers.20.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.20.ffn_norm.weight : torch.Size([4096])\n",
      "module.layers.21.attn.to_q.weight : torch.Size([4096, 4096])\n",
      "module.layers.21.attn.to_k.weight : torch.Size([1024, 4096])\n",
      "module.layers.21.attn.to_v.weight : torch.Size([1024, 4096])\n",
      "module.layers.21.attn.norm_q.weight : torch.Size([128])\n",
      "module.layers.21.attn.norm_k.weight : torch.Size([128])\n",
      "module.layers.21.attn.to_o.weight : torch.Size([4096, 4096])\n",
      "module.layers.21.ff.w12.weight : torch.Size([24576, 4096])\n",
      "module.layers.21.ff.w3.weight : torch.Size([4096, 12288])\n",
      "module.layers.21.media_norm.weight : torch.Size([4096])\n",
      "module.layers.21.latent_norm.weight : torch.Size([4096])\n",
      "module.layers.21.ffn_norm.weight : torch.Size([4096])\n",
      "Total number of params: 4.0 B\n"
     ]
    }
   ],
   "source": [
    "num_params = 0\n",
    "for k, v in model.items():\n",
    "    print(k, \":\", v.shape)\n",
    "    num_params += v.numel()\n",
    "\n",
    "print(f\"Total number of params: {num_params // 1e9} B\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
