{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward x.size()=torch.Size([4, 16, 64])\n",
      "flat_square_2x2 x.size()=torch.Size([4, 4, 4, 64])\n",
      "Original shape: torch.Size([4, 16, 64])\n",
      "Downsampled shape: torch.Size([4, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Define the DownSample2x2BlockFix class\n",
    "class DownSample2x2BlockFix(nn.Module):\n",
    "    def forward(self, x):\n",
    "        print(f\"forward {x.size()=}\")\n",
    "        vit_embeds = x\n",
    "        h = w = int(vit_embeds.shape[1] ** 0.5)\n",
    "        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], h, w, -1)\n",
    "        vit_embeds = flat_square_2x2(vit_embeds)\n",
    "        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], -1, vit_embeds.shape[-1])\n",
    "        return vit_embeds\n",
    "\n",
    "# Define the flat_square_2x2 function\n",
    "def flat_square_2x2(x):\n",
    "    print(f\"flat_square_2x2 {x.size()=}\")\n",
    "    n, w, h, c = x.size()\n",
    "    if w % 2 == 1:\n",
    "        x = torch.concat([x, torch.zeros((n, 1, h, c), dtype=x.dtype).to(x.device)], dim=1).contiguous()\n",
    "        n, w, h, c = x.size()\n",
    "    x = x.contiguous()\n",
    "    if h % 2 == 1:\n",
    "        x = torch.concat([x, torch.zeros((n, w, 1, c), dtype=x.dtype).to(x.device)], dim=2).contiguous()\n",
    "        n, w, h, c = x.size()\n",
    "    x = x.view(n, w, int(h / 2), int(c * 2))\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "    x = x.view(n, int(h / 2), int(w / 2), int(c * 4))\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "    return x\n",
    "\n",
    "batch_size = 4\n",
    "num_patches = 16  # This should be a perfect square for simplicity\n",
    "embedding_dim = 64\n",
    "\n",
    "# Simulate a batch of ViT embeddings\n",
    "vit_embeds = torch.randn(batch_size, num_patches, embedding_dim)\n",
    "\n",
    "# Initialize the downsampling block\n",
    "downsample_block = DownSample2x2BlockFix()\n",
    "\n",
    "# Forward pass through the downsampling block\n",
    "downsampled_embeds = downsample_block(vit_embeds)\n",
    "\n",
    "# Print the shape of the output\n",
    "print(\"Original shape:\", vit_embeds.shape)\n",
    "print(\"Downsampled shape:\", downsampled_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16 * int(1/0.5)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num patches: 256.0\n",
      "Number of image tokens: 64\n"
     ]
    }
   ],
   "source": [
    "class ImageProcessor:\n",
    "    def __init__(self, image_size, patch_size, config):\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.config = config\n",
    "        self.num_image_token = int((image_size // patch_size) ** 2 * (config.downsample_ratio ** 2))\n",
    "\n",
    "# Example configuration class\n",
    "class Config:\n",
    "    def __init__(self, downsample_ratio):\n",
    "        self.downsample_ratio = downsample_ratio\n",
    "\n",
    "\n",
    "# Define image size, patch size, and downsample ratio\n",
    "image_size = 256  # e.g., 256x256 image\n",
    "patch_size = 16   # e.g., 16x16 patches\n",
    "downsample_ratio = 0.5  # e.g., downsample by a factor of 0.5\n",
    "\n",
    "print(f\"Num patches:\", (image_size/patch_size)**2)\n",
    "\n",
    "# Create a config object\n",
    "config = Config(downsample_ratio)\n",
    "\n",
    "# Initialize the ImageProcessor\n",
    "processor = ImageProcessor(image_size, patch_size, config)\n",
    "\n",
    "# Print the number of image tokens\n",
    "print(\"Number of image tokens:\", processor.num_image_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InternVL - pixel_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_square(x, kernel_size):\n",
    "    \"\"\"\n",
    "    Reshapes and rearranges the input tensor to prepare it for downsampling.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Input tensor.\n",
    "        kernel_size (int): Size of the downsampling kernel.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Reshaped and rearranged tensor.\n",
    "    \"\"\"\n",
    "    n, w, h, c = x.size()\n",
    "\n",
    "    # Reshape and rearrange\n",
    "    x = x.view(n, w, int(h / kernel_size), int(c * kernel_size))\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "    x = x.view(n, int(h / kernel_size), int(w / kernel_size), int(c * kernel_size ** 2))\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "    return x\n",
    "\n",
    "def pixel_shuffle(x, scale_factor=0.5):\n",
    "    n, w, h, c = x.size()\n",
    "    # N, W, H, C --> N, W, H * scale, C // scale\n",
    "    x = x.view(n, w, int(h * scale_factor), int(c / scale_factor))\n",
    "    # N, W, H * scale, C // scale --> N, H * scale, W, C // scale\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "    # N, H * scale, W, C // scale --> N, H * scale, W * scale, C // (scale ** 2)\n",
    "    x = x.view(n, int(h * scale_factor), int(w * scale_factor),\n",
    "                int(c / (scale_factor * scale_factor)))\n",
    "\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "    \n",
    "    return x\n",
    "\n",
    "def extract_feature(pixel_values):\n",
    "    if self.select_layer == -1:\n",
    "        vit_embeds = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True).last_hidden_state\n",
    "    else:\n",
    "        vit_embeds = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True).hidden_states[self.select_layer]\n",
    "    vit_embeds = vit_embeds[:, 1:, :]\n",
    "\n",
    "    h = w = int(vit_embeds.shape[1] ** 0.5)\n",
    "    vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], h, w, -1)\n",
    "    vit_embeds = self.pixel_shuffle(vit_embeds, scale_factor=self.downsample_ratio)\n",
    "    vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], -1, vit_embeds.shape[-1])\n",
    "    vit_embeds = self.mlp1(vit_embeds)\n",
    "    return vit_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the class that contains the pixel_shuffle and extract_feature methods\n",
    "model = YourModel()\n",
    "\n",
    "# Example usage of pixel_shuffle\n",
    "input_tensor = torch.randn(1, 16, 16, 3)  # batch_size, height, width, channels\n",
    "output_tensor = model.pixel_shuffle(input_tensor, scale_factor=0.5)\n",
    "print(output_tensor.shape)\n",
    "\n",
    "# Example usage of extract_feature\n",
    "input_tensor = torch.randn(1, 3, 224, 224)  # batch_size, channels, height, width\n",
    "output_tensor = model.extract_feature(input_tensor)\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVILA - flat square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample2x2BlockFix(nn.Module):\n",
    "    def forward(self, x):\n",
    "        vit_embeds = x\n",
    "        h = w = int(vit_embeds.shape[1] ** 0.5)\n",
    "        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], h, w, -1)\n",
    "        vit_embeds = flat_square_2x2(vit_embeds)\n",
    "        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], -1, vit_embeds.shape[-1])\n",
    "        return vit_embeds\n",
    "\n",
    "\n",
    "def flat_square_2x2(x):\n",
    "    n, w, h, c = x.size()\n",
    "    if w % 2 == 1:\n",
    "        x = torch.concat([x, torch.zeros((n, 1, h, c), dtype=x.dtype).to(x.device)], dim=1).contiguous()\n",
    "        n, w, h, c = x.size()\n",
    "    x = x.contiguous()\n",
    "    if h % 2 == 1:\n",
    "        x = torch.concat([x, torch.zeros((n, w, 1, c), dtype=x.dtype).to(x.device)], dim=2).contiguous()\n",
    "        n, w, h, c = x.size()\n",
    "    x = x.view(n, w, int(h / 2), int(c * 2))\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "    x = x.view(n, int(h / 2), int(w / 2), int(c * 4))\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "    return x\n",
    "\n",
    "def flat_square_downsample(x: torch.Tensor, down_sample_ratio: int):\n",
    "    \"\"\"\n",
    "    Reshapes and rearranges the input tensor to prepare it for downsampling.\n",
    "    Args:\n",
    "        x (Tensor): Input tensor.\n",
    "    Returns:\n",
    "        Tensor: Reshaped and rearranged tensor.\n",
    "    \"\"\"\n",
    "    h = w = int(x.shape[1] ** 0.5)\n",
    "    x = x.reshape(x.shape[0], h, w, -1)\n",
    "    \n",
    "    n, w, h, c = x.size()\n",
    "    # Pad width and height if necessary\n",
    "    if w % down_sample_ratio != 0:\n",
    "        padding_w = down_sample_ratio - (w % down_sample_ratio)\n",
    "        x = torch.concat([x, torch.zeros((n, padding_w, h, c), dtype=x.dtype).to(x.device)], dim=1).contiguous()\n",
    "        n, w, h, c = x.size()\n",
    "    if h % down_sample_ratio != 0:\n",
    "        padding_h = down_sample_ratio - (h % down_sample_ratio)\n",
    "        x = torch.concat([x, torch.zeros((n, w, padding_h, c), dtype=x.dtype).to(x.device)], dim=2).contiguous()\n",
    "        n, w, h, c = x.size()\n",
    "    # Reshape and rearrange\n",
    "    x = x.view(n, w, int(h / down_sample_ratio), int(c * down_sample_ratio))\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "    x = x.view(n, int(h / down_sample_ratio), int(w / down_sample_ratio), int(c * down_sample_ratio ** 2))\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "    \n",
    "    return x.reshape(x.shape[0], -1, x.shape[-1])\n",
    "\n",
    "class DownSample3x3BlockFix(nn.Module):\n",
    "    def forward(self, x):\n",
    "        vit_embeds = x\n",
    "        h = w = int(vit_embeds.shape[1] ** 0.5)\n",
    "        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], h, w, -1)\n",
    "        vit_embeds = flat_square_3x3(vit_embeds)\n",
    "        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], -1, vit_embeds.shape[-1])\n",
    "        return vit_embeds\n",
    "\n",
    "\n",
    "def flat_square_3x3(x):\n",
    "    n, w, h, c = x.size()\n",
    "    if w % 3 != 0:\n",
    "        x = torch.concat([x, torch.zeros((n, 3 - (w % 3), h, c), dtype=x.dtype).to(x.device)], dim=1).contiguous()\n",
    "        n, w, h, c = x.size()\n",
    "    x = x.contiguous()\n",
    "    if h % 3 != 0:\n",
    "        x = torch.concat([x, torch.zeros((n, w, 3 - (h % 3), c), dtype=x.dtype).to(x.device)], dim=2).contiguous()\n",
    "        n, w, h, c = x.size()\n",
    "    x = x.view(n, w, int(h / 3), int(c * 3))\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "    x = x.view(n, int(h / 3), int(w / 3), int(c * 9))\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "    return x\n",
    "\n",
    "def flat_square(x, kernel_size):\n",
    "    \"\"\"\n",
    "    Reshapes and rearranges the input tensor to prepare it for downsampling.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Input tensor.\n",
    "        kernel_size (int): Size of the downsampling kernel.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Reshaped and rearranged tensor.\n",
    "    \"\"\"\n",
    "    n, w, h, c = x.size()\n",
    "\n",
    "    # Pad width and height if necessary\n",
    "    if w % kernel_size != 0:\n",
    "        padding_w = kernel_size - (w % kernel_size)\n",
    "        x = torch.concat([x, torch.zeros((n, padding_w, h, c), dtype=x.dtype).to(x.device)], dim=1).contiguous()\n",
    "        n, w, h, c = x.size()\n",
    "\n",
    "    if h % kernel_size != 0:\n",
    "        padding_h = kernel_size - (h % kernel_size)\n",
    "        x = torch.concat([x, torch.zeros((n, w, padding_h, c), dtype=x.dtype).to(x.device)], dim=2).contiguous()\n",
    "        n, w, h, c = x.size()\n",
    "\n",
    "    # Reshape and rearrange\n",
    "    x = x.view(n, w, int(h / kernel_size), int(c * kernel_size))\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "    x = x.view(n, int(h / kernel_size), int(w / kernel_size), int(c * kernel_size ** 2))\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "    return x\n",
    "\n",
    "class DownSampleBlock(nn.Module):\n",
    "    def __init__(self, down_sample_ratio: int):\n",
    "        self.kernel_size = down_sample_ratio\n",
    "        \n",
    "    def _flat_square(self, x):\n",
    "        \"\"\"\n",
    "        Reshapes and rearranges the input tensor to prepare it for downsampling.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "        Returns:\n",
    "            Tensor: Reshaped and rearranged tensor.\n",
    "        \"\"\"\n",
    "        n, w, h, c = x.size()\n",
    "\n",
    "        # Pad width and height if necessary\n",
    "        if w % self.kernel_size != 0:\n",
    "            padding_w = self.kernel_size - (w % self.kernel_size)\n",
    "            x = torch.concat([x, torch.zeros((n, padding_w, h, c), dtype=x.dtype).to(x.device)], dim=1).contiguous()\n",
    "            n, w, h, c = x.size()\n",
    "\n",
    "        if h % self.kernel_size != 0:\n",
    "            padding_h = self.kernel_size - (h % self.kernel_size)\n",
    "            x = torch.concat([x, torch.zeros((n, w, padding_h, c), dtype=x.dtype).to(x.device)], dim=2).contiguous()\n",
    "            n, w, h, c = x.size()\n",
    "\n",
    "        # Reshape and rearrange\n",
    "        x = x.view(n, w, int(h / self.kernel_size), int(c * self.kernel_size))\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(n, int(h / self.kernel_size), int(w / self.kernel_size), int(c * self.kernel_size ** 2))\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        vit_embeds = x\n",
    "        h = w = int(vit_embeds.shape[1] ** 0.5)\n",
    "        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], h, w, -1)\n",
    "        \n",
    "        vit_embeds = self._flat_square(vit_embeds)\n",
    "        \n",
    "        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], -1, vit_embeds.shape[-1])\n",
    "        return vit_embeds\n",
    "    \n",
    "\n",
    "def flat_square_downsample(x: torch.Tensor, down_sample_ratio: int):\n",
    "    \"\"\"\n",
    "    Reshapes and rearranges the input tensor to prepare it for downsampling.\n",
    "    Args:\n",
    "        x (Tensor): Input tensor.\n",
    "    Returns:\n",
    "        Tensor: Reshaped and rearranged tensor.\n",
    "    \"\"\"\n",
    "    h = w = int(x.shape[1] ** 0.5)\n",
    "    x = x.reshape(x.shape[0], h, w, -1)\n",
    "    n, w, h, c = x.size()\n",
    "    # Pad width and height if necessary\n",
    "    if w % down_sample_ratio != 0:\n",
    "        padding_w = down_sample_ratio - (w % down_sample_ratio)\n",
    "        x = torch.concat([x, torch.zeros((n, padding_w, h, c), dtype=x.dtype).to(x.device)], dim=1).contiguous()\n",
    "        n, w, h, c = x.size()\n",
    "    if h % down_sample_ratio != 0:\n",
    "        padding_h = down_sample_ratio - (h % down_sample_ratio)\n",
    "        x = torch.concat([x, torch.zeros((n, w, padding_h, c), dtype=x.dtype).to(x.device)], dim=2).contiguous()\n",
    "        n, w, h, c = x.size()\n",
    "    # Reshape and rearrange\n",
    "    x = x.view(n, w, int(h / down_sample_ratio), int(c * down_sample_ratio))\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "    x = x.view(n, int(h / down_sample_ratio), int(w / down_sample_ratio), int(c * down_sample_ratio ** 2))\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "    return x.reshape(x.shape[0], -1, x.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_block.shape=torch.Size([8, 64, 4096])\n",
      "output_function.shape=torch.Size([8, 64, 4096])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create a random input tensor\n",
    "x = torch.randn(8, 256, 1024)\n",
    "# Create an instance of DownSample2x2BlockFix\n",
    "downsample_block = DownSample2x2BlockFix()\n",
    "# Run the input through the block\n",
    "output_block = downsample_block(x)\n",
    "# Run the input through the flat_square_downsample function\n",
    "output_function = flat_square_downsample(x, 2)\n",
    "# Check if the outputs are equal\n",
    "print(f\"{output_block.shape=}\")\n",
    "print(f\"{output_function.shape=}\")\n",
    "print(torch.allclose(output_block, output_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def test_flat_square_2x2():\n",
    "    x = torch.randn(1, 10, 10, 3)\n",
    "    expected_output = flat_square_2x2(x)\n",
    "    \n",
    "    downsampler = DownSampleBlock(down_sample_ratio=2)\n",
    "    actual_output = downsampler._flat_square(x)\n",
    "    # actual_output = downsampler.forward(x)\n",
    "    # actual_output = flat_square(x, 2)\n",
    "    print(torch.allclose(expected_output, actual_output))\n",
    "    # self.assertTrue()\n",
    "    \n",
    "def test_flat_square_3x3():\n",
    "    x = torch.randn(1, 12, 12, 3)\n",
    "    expected_output = flat_square_3x3(x)\n",
    "    actual_output = flat_square(x, 3)\n",
    "    print(torch.allclose(expected_output, actual_output))\n",
    "    # self.assertTrue(torch.allclose(expected_output, actual_output))\n",
    "    \n",
    "test_flat_square_2x2()\n",
    "test_flat_square_3x3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPResampler(nn.Module):\n",
    "    # enforce_uniform_emb_variance = True \n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        feature_dim: int, \n",
    "        embedding_size: int,\n",
    "        scale_factor: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "        # To-do: add pos_emb\n",
    "        self.pos_emb = None\n",
    "        \n",
    "        # To-do: add pos_emb resize\n",
    "        \n",
    "        # To-do: add eoi_emb\n",
    "        \n",
    "        # To-do: add eoi_emb resize\n",
    "        \n",
    "        # To-do: add out-norm\n",
    "        \n",
    "        \n",
    "        # self.register_buffer('average_tok_rms', torch.tensor(1.0))\n",
    "        # if self.enforce_uniform_emb_variance:\n",
    "        #     self.average_tok_rms = None\n",
    "                \n",
    "         # with scale_factor=0.5 --> feature_dim x 4\n",
    "        \n",
    "        # h = int(feature_dim ** 0.5)\n",
    "        # assert h % self.scale_factor == 0\n",
    "        \n",
    "        inner_dim = feature_dim * int(self.scale_factor ** 2)\n",
    "        \n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(inner_dim),\n",
    "            nn.Linear(inner_dim, self.embedding_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.embedding_size, self.embedding_size)\n",
    "        )\n",
    "        \n",
    "        # EOI\n",
    "        init_scale = 1 / torch.sqrt(\n",
    "            torch.tensor(embedding_size, dtype=torch.float32, requires_grad=False)\n",
    "        )\n",
    "        self.eoi = nn.Parameter(torch.randn(1, 1, embedding_size) * init_scale)\n",
    "        \n",
    "    def _channel_reshuffle(self, x):\n",
    "        \"\"\"\n",
    "        Applies spatial-to-channel reshuffling to the input tensor.\n",
    "        This function rearranges the dimensions of the input tensor to transform\n",
    "        spatial dimensions into channel dimensions. The transformation is as follows:\n",
    "        bsz, w, h, dim --> bsz, w * scale, h * scale, dim // (scale ** 2)\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor with shape (N, W, H, C)\n",
    "        Returns:\n",
    "            torch.Tensor: Reshuffled tensor with shape (N, W * scale, H * scale, C // (scale ** 2))\n",
    "        \"\"\"\n",
    "    \n",
    "        # n, w, h, c = x.size()\n",
    "        \n",
    "        # # N, W, H, C --> N, W, H * scale, C // scale\n",
    "        # x = x.view(n, w, int(h * self.scale_factor), int(c / self.scale_factor))\n",
    "        \n",
    "        # # N, W, H * scale, C // scale --> N, H * scale, W, C // scale\n",
    "        # x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        # # N, H * scale, W, C // scale --> N, H * scale, W * scale, C // (scale ** 2)\n",
    "        # x = x.view(n, int(h * self.scale_factor), int(w * self.scale_factor),\n",
    "        #             int(c / (self.scale_factor * self.scale_factor)))\n",
    "\n",
    "        # # N, W*scale, H*scale, C//(scale ** 2)\n",
    "        # x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        # return x\n",
    "        \n",
    "        bsz, w, h, dim = x.size()\n",
    "        \n",
    "        # bsz, w, h, dim --> bsz, w, h * scale, dim // scale\n",
    "        x = x.view(bsz, w, int(h // self.scale_factor), dim * self.scale_factor)\n",
    "        \n",
    "        # bsz, w, h * scale, dim // scale --> bsz, h * scale, w, dim // scale\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        # bsz, h * scale, w, dim // scale --> bsz, h * scale, w * scale, dim // (scale ** 2)\n",
    "        # x = x.view(bsz, int(h * self.scale_factor), int(w * self.scale_factor),\n",
    "        #             int(dim / (self.scale_factor * self.scale_factor)))\n",
    "        x = x.view(bsz, int(h // self.scale_factor), int(w // self.scale_factor),\n",
    "                dim * self.scale_factor * self.scale_factor)\n",
    "\n",
    "        # bsz, w * scale, h * scale, dim // (scale ** 2)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def forward(self, modality_embs: torch.Tensor):\n",
    "        bsz, seqlen, dim = modality_embs.shape\n",
    "        \n",
    "        print(f\"{modality_embs.shape = }\")\n",
    "        \n",
    "        h = w = int(seqlen ** 0.5)\n",
    "        \n",
    "        modality_embs = modality_embs.reshape(modality_embs.shape[0], h, w, -1) # (bz, h, w, -1)\n",
    "        print(f\"1. {modality_embs.shape = }\")\n",
    "        \n",
    "        modality_embs = self._channel_reshuffle(modality_embs) # bz, w*scale, h*scale, dim /(scale **2 )\n",
    "        print(f\"2. {modality_embs.shape = }\")\n",
    "        \n",
    "        modality_embs = modality_embs.reshape(modality_embs.shape[0], -1, modality_embs.shape[-1]) # bz, w*h*scale^2, dim / scale ^ 2\n",
    "        print(f\"3. {modality_embs.shape = }\")\n",
    "        \n",
    "        modality_embs = self.mlp(modality_embs) # bz, w*h*scale^2, emd\n",
    "        print(f\"4. {modality_embs.shape = }\")\n",
    "        \n",
    "        # To-do: add EOI token \n",
    "        modality_embs = torch.cat([modality_embs, self.eoi.repeat(bsz, 1, 1)], dim=1)\n",
    "        \n",
    "        # # To-do: add out norm\n",
    "        # if self.out_norm is not None:\n",
    "        #     modality_embs = self.out_norm(modality_embs)\n",
    "            \n",
    "        # # To-do: add average_tok_rms\n",
    "        # if self.enforce_uniform_emb_variance:\n",
    "        #     modality_embs = modality_embs * self.average_tok_rms\n",
    "        \n",
    "        return modality_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = torch.Size([2, 16, 128])\n",
      "modality_embs.shape = torch.Size([2, 16, 128])\n",
      "1. modality_embs.shape = torch.Size([2, 4, 4, 128])\n",
      "2. modality_embs.shape = torch.Size([2, 2, 2, 512])\n",
      "3. modality_embs.shape = torch.Size([2, 4, 512])\n",
      "4. modality_embs.shape = torch.Size([2, 4, 4096])\n",
      "output.shape = torch.Size([2, 5, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Create a test input tensor\n",
    "batch_size = 2\n",
    "patch_sequence_length = 16\n",
    "embedding_dim = 128\n",
    "x = torch.randn(batch_size, patch_sequence_length, embedding_dim)\n",
    "# Create an instance of the MLPResampler module\n",
    "print(f\"{x.shape = }\")\n",
    "# resampler = MLPResampler(feature_dim=embedding_dim, embedding_size=4096, scale_factor=0.5)\n",
    "resampler = MLPResampler(feature_dim=embedding_dim, embedding_size=4096, scale_factor=2)\n",
    "# Run the forward pass\n",
    "output = resampler(x)\n",
    "# Print the output shape\n",
    "print(f\"{output.shape = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VE pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original pos_embedding.shape=torch.Size([256, 1024])\n",
      "Interpolating pos embedding from n patches: 16 to 24. hidden dim: 1024\n",
      "1. pos_embedding.shape=torch.Size([16, 16, 1024])\n",
      "2. pos_embedding.shape=torch.Size([1, 1024, 16, 16])\n",
      "3. pos_embedding.shape=torch.Size([1, 1024, 24, 24])\n",
      "4. pos_embedding.shape=torch.Size([576, 1024])\n",
      "5. pos_embedding.shape=torch.Size([576, 1024])\n",
      "interpolated_pos_embedding.shape=torch.Size([576, 1024]),  expected_shape=(576, 1024)\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional \n",
    "\n",
    "def interpolate_position_embedding(\n",
    "    pos_embedding: torch.Tensor,\n",
    "    target_image_size: int,\n",
    "    patch_size: int\n",
    "):\n",
    "    hidden_dim = pos_embedding.shape[-1]\n",
    "    original_n_patches = int((pos_embedding.shape[0]) ** (1 / 2))\n",
    "    target_n_patches = target_image_size // patch_size\n",
    "    print(\n",
    "        f\"Interpolating pos embedding from n patches: {original_n_patches} to {target_n_patches}. hidden dim: {hidden_dim}\"\n",
    "    )\n",
    "    # reshape to (original_n_patches, original_n_patches, hidden_dim)\n",
    "    pos_embedding = pos_embedding.reshape(\n",
    "        original_n_patches, original_n_patches, -1\n",
    "    )\n",
    "    print(f\"1. {pos_embedding.shape=}\")\n",
    "    \n",
    "    pos_embedding = pos_embedding.unsqueeze(dim=0).permute(0, 3, 1, 2)\n",
    "    print(f\"2. {pos_embedding.shape=}\")\n",
    "    \n",
    "    # interpolate\n",
    "    pos_embedding = torch.nn.functional.interpolate(\n",
    "        pos_embedding, size=(target_n_patches, target_n_patches), mode=\"bilinear\"\n",
    "    )\n",
    "    print(f\"3. {pos_embedding.shape=}\")\n",
    "    \n",
    "    pos_embedding = (\n",
    "        pos_embedding[0]\n",
    "        .permute(1, 2, 0)\n",
    "        .reshape(target_n_patches * target_n_patches, hidden_dim)\n",
    "    )\n",
    "    print(f\"4. {pos_embedding.shape=}\")\n",
    "    \n",
    "    print(f\"5. {pos_embedding.shape=}\")\n",
    "    \n",
    "    return pos_embedding\n",
    "\n",
    "pos_embedding = torch.randn(256, 1024)  # original_n_patches x hidden_dim\n",
    "print(f\"original {pos_embedding.shape=}\")\n",
    "# Create a sample target image size and patch size\n",
    "target_image_size = 336\n",
    "patch_size = 14\n",
    "\n",
    "# Call the interpolate_position_embedding method\n",
    "interpolated_pos_embedding = interpolate_position_embedding(\n",
    "    pos_embedding, target_image_size, patch_size\n",
    ")\n",
    "# Check that the output shape is correct\n",
    "expected_shape = (target_image_size // patch_size) ** 2, pos_embedding.shape[-1]\n",
    "print(f\"{interpolated_pos_embedding.shape=},  {expected_shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating pos embedding from n patches: 4 to 8. hidden dim: 128\n",
      "interpolated_pos_embedding.shape=torch.Size([64, 128]),  expected_shape=(64, 128)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([576, 1024])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated_pos_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 576, 1024])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = interpolated_pos_embedding.unsqueeze(0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"a\": {\"b\": 1}}'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 1}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 10]), torch.Size([1, 16, 10]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(16, 10).shape, torch.randn(1, 16, 10).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
