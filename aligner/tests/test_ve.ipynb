{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use aligner_v7 kernel\n",
    "\n",
    "import sys\n",
    "add_paths = [\n",
    "    \"/fsx_0/user/tranx/rsync\", # ALIGNER_PARENT_DIR\n",
    "    \"/fsx_0/user/tranx/rsync/llm_mm_aligner/replicated\", # ALIGNER_PARENT_DIR/llm_mm_aligner/replicated\n",
    "    # \"/data/home/tranx/conda/envs/aligner_20240822_v2/python-packages\", #CONDA_PREFIX/python-packages\n",
    "    # \"/data/home/kapilk/.conda/envs/aligner_20240822_v2/python-packages\"\n",
    "    \"/fsx_0/shared/conda/aligner_20241030/python-packages\"\n",
    "]\n",
    "\n",
    "for p in add_paths:\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n",
    "        \n",
    "device = \"cuda:2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_H = \"/fsx_0/checkpoints/clip/vev0/vit_h14_336_fair_vev0/fair_vev01.pt\"\n",
    "MODEL_G = \"/fsx_0/user/marcomonteiro/experiments/G/VEv0p2_G14_aws_final_256x8x98_next/checkpoints/epoch_450.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_h = torch.load(MODEL_H, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sd_g = torch.load(MODEL_G, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llm_mm_aligner.lib.encoders.collator import PACKED\n",
    "from llm_mm_aligner.lib.encoders.metaclip_fixed_resolution import MetaCLIP\n",
    "# from llm_mm_aligner.lib.encoders.metaclip_native_resolution import MetaCLIP_NAVIT\n",
    "from llm_mm_aligner.lib.encoders.metaclip_navit import MetaCLIP_NAVIT\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = \"/fsx_0/checkpoints/clip/vev0/vit_h14_336_fair_vev0/fair_vev01.pt\"\n",
    "TEST_IMAGE = \"/fsx_0/dataset01/coco/val2014/COCO_val2014_000000485307.jpg\"\n",
    "\n",
    "metaclip_config = {\n",
    "    \"ckpt_path\": MODEL_CHECKPOINT,\n",
    "    \"image_size\": 336,  # image_size can also be 448, since the position embedding is rope_2d\n",
    "    \"patch_size\": 14,\n",
    "    \"width\": 1408,\n",
    "    \"mlp_ratio\": 4.0,\n",
    "    \"layers\": 34,\n",
    "    \"heads\": 16,\n",
    "    \"load_ckpt\": True,\n",
    "    \"relative_pos_embed_type\": \"rope_2d\",\n",
    "}\n",
    "\n",
    "CLIP_MEAN = (0.4814546, 0.4578275, 0.40821073)\n",
    "CLIP_STD = (0.2686295, 0.2613025, 0.2757711)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected keys: ['positional_embedding']\n",
      "Loaded MetaCLIP fixed resolution model.\n"
     ]
    }
   ],
   "source": [
    "base_model =  MetaCLIP(**metaclip_config)\n",
    "base_model.cuda().eval()\n",
    "print(\"Loaded MetaCLIP fixed resolution model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaCLIP(\n",
       "  (conv1): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "  (patch_dropout): Identity()\n",
       "  (ln_pre): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-33): 34 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): AttentionXformer4x(\n",
       "          (out_proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_post): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected keys: []\n",
      "Loaded MetaCLIP native resolution model.\n"
     ]
    }
   ],
   "source": [
    "navit_model = MetaCLIP_NAVIT(**metaclip_config)\n",
    "navit_model.cuda().eval()\n",
    "print(\"Loaded MetaCLIP native resolution model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(metaclip_config[\"image_size\"]),\n",
    "            transforms.CenterCrop(metaclip_config[\"image_size\"]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=CLIP_MEAN, std=CLIP_STD),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return preprocess(image).unsqueeze(0)\n",
    "\n",
    "def pack_window(img):\n",
    "    patch_h = patch_w = metaclip_config[\"patch_size\"]\n",
    "    _, channel, image_h, image_w = img.shape\n",
    "    img = img.unfold(-2, patch_h, patch_h).unfold(-2, patch_w, patch_w)\n",
    "\n",
    "    idx_h, idx_w = image_h // patch_h, image_w // patch_w\n",
    "    img_idx = torch.arange(\n",
    "        image_h * image_w // (patch_h * patch_w), dtype=torch.int32\n",
    "    )\n",
    "    img_idx = img_idx.reshape(idx_h * idx_w, 1)\n",
    "\n",
    "    img = img.reshape(channel, -1, 1, patch_h, patch_w)\n",
    "    img_idx = img_idx.reshape(-1, 1)\n",
    "\n",
    "    img = torch.cat([img, img[:, :1]], dim=1)\n",
    "    img_idx = torch.cat([img_idx, img_idx[:1]], dim=0)\n",
    "    img_idx[-1, -1] = PACKED.ID_CLS_TOKEN\n",
    "\n",
    "    packed_img_idx = torch.empty(\n",
    "        img_idx.shape[0],\n",
    "        img_idx.shape[1],\n",
    "        PACKED.NUM_METADATA - 1,\n",
    "        dtype=torch.int32,\n",
    "    )\n",
    "\n",
    "    packed_img_idx[:, :, PACKED.Y] = img_idx // idx_w\n",
    "    packed_img_idx[:, :, PACKED.X] = img_idx % idx_w\n",
    "    packed_img_idx[:, :, PACKED.IDX] = img_idx\n",
    "\n",
    "    return img, packed_img_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_tensor_base: torch.Size([8, 3, 336, 336])\n",
      "base_features.shape: torch.Size([8, 576, 1408])\n",
      "image_tensor_navit: torch.Size([8, 3, 336, 336])\n",
      "0\n",
      "image.shape=torch.Size([1, 3, 336, 336])\n",
      "packed_img.shape=torch.Size([3, 577, 1, 14, 14])\n",
      "1\n",
      "image.shape=torch.Size([1, 3, 336, 336])\n",
      "packed_img.shape=torch.Size([3, 577, 1, 14, 14])\n",
      "2\n",
      "image.shape=torch.Size([1, 3, 336, 336])\n",
      "packed_img.shape=torch.Size([3, 577, 1, 14, 14])\n",
      "3\n",
      "image.shape=torch.Size([1, 3, 336, 336])\n",
      "packed_img.shape=torch.Size([3, 577, 1, 14, 14])\n",
      "4\n",
      "image.shape=torch.Size([1, 3, 336, 336])\n",
      "packed_img.shape=torch.Size([3, 577, 1, 14, 14])\n",
      "5\n",
      "image.shape=torch.Size([1, 3, 336, 336])\n",
      "packed_img.shape=torch.Size([3, 577, 1, 14, 14])\n",
      "6\n",
      "image.shape=torch.Size([1, 3, 336, 336])\n",
      "packed_img.shape=torch.Size([3, 577, 1, 14, 14])\n",
      "7\n",
      "image.shape=torch.Size([1, 3, 336, 336])\n",
      "packed_img.shape=torch.Size([3, 577, 1, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "# test forward parity\n",
    "image_tensor_base = (\n",
    "    preprocess_image(TEST_IMAGE).cuda().repeat(8, 1, 1, 1)\n",
    ")  # [8, 3, 336, 336]\n",
    "print(\"image_tensor_base:\", image_tensor_base.shape)\n",
    "with torch.no_grad():\n",
    "    base_features = base_model(image_tensor_base)\n",
    "print(\"base_features.shape:\", base_features.shape)\n",
    "# print(base_features)\n",
    "\n",
    "image_tensor_navit = preprocess_image(TEST_IMAGE).cuda().repeat(8, 1, 1, 1)\n",
    "print(\"image_tensor_navit:\", image_tensor_navit.shape)\n",
    "batch_size = image_tensor_navit.shape[0]\n",
    "\n",
    "images = []\n",
    "packed_img_idxs = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    print(i)\n",
    "    image = image_tensor_navit[i].unsqueeze(0)  # [8, 3, 336, 336]\n",
    "    print(f\"image.shape={image.shape}\")\n",
    "    packed_img, packed_img_idx_i = pack_window(image)\n",
    "    print(f\"packed_img.shape={packed_img.shape}\")\n",
    "    images.append(packed_img)\n",
    "    packed_img_idxs.append(packed_img_idx_i)\n",
    "    \n",
    "tokens_per_img = torch.tensor(\n",
    "    [packed_img.shape[1] for packed_img in images]\n",
    ").long()\n",
    "\n",
    "packed_img_idxs = torch.cat(packed_img_idxs, dim=0)\n",
    "images = torch.cat(images, dim=1).permute(1, 0, 2, 3, 4).contiguous()\n",
    "images_package = [images.cuda(), packed_img_idxs.cuda(), tokens_per_img.cuda()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimages_package\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "images_package.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_navit.py:287: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  condition = torch.tensor(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     navit_features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnavit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor_navit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    navit_features = torch.stack(navit_model(image_tensor_navit), dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
