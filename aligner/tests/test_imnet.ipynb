{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use aligner_v7 kernel\n",
    "\n",
    "import sys\n",
    "add_paths = [\n",
    "    \"/fsx_0/user/tranx/rsync\", # ALIGNER_PARENT_DIR\n",
    "    \"/fsx_0/user/tranx/rsync/llm_mm_aligner/replicated\", # ALIGNER_PARENT_DIR/llm_mm_aligner/replicated\n",
    "    \"/fsx_0/shared/conda/aligner_20241030/python-packages\"\n",
    "]\n",
    "\n",
    "for p in add_paths:\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n",
    "        \n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from llm_mm_aligner.lib.encoders.metaclip_text_encoder import MetaCLIPTextTransformer\n",
    "from llm_mm_aligner.lib.encoders.metaclip_vev01 import MetaCLIPVEv01\n",
    "from llm_mm_aligner.lib.encoders.metaclip_vev02 import MetaCLIPVEv02\n",
    "\n",
    "from transformers import CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metaclip_model(model_path:str):\n",
    "    if \"vev0p1\" in model_path or model_path.split(\"/\")[-1] in [\n",
    "        \"MetaCLIP-hp14_336_fair_vev0_ckpt_epoch205\"\n",
    "    ]:\n",
    "        return MetaCLIPVEv01\n",
    "    \n",
    "    if model_path.split(\"/\")[-1] in [\n",
    "        \"MetaCLIP-Gs_224_fair_vev02_ckpt_epoch_360\"\n",
    "    ]:\n",
    "        return MetaCLIPVEv02\n",
    "    \n",
    "    raise ValueError(f\"Unknown metaclip model from path: {model_path}\")\n",
    "    \n",
    "class OpenCLIPModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, vision_model: torch.nn.Module, text_model: torch.nn.Module):\n",
    "        super().__init__()\n",
    "        self.vision_model = vision_model\n",
    "        self.visual_projection = self.vision_model.proj\n",
    "        self.text_model = text_model\n",
    "        self.text_projection = self.text_model.text_projection\n",
    "        self.logit_scale = torch.nn.Parameter(torch.tensor(4.605))\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_path: str, **kwargs):\n",
    "        metaclip_model = get_metaclip_model(pretrained_path)\n",
    "\n",
    "        vision_model = metaclip_model.from_pretrained(\n",
    "            pretrained_path=pretrained_path\n",
    "        )\n",
    "                \n",
    "        # vision_model = metaclip_model.from_pretrained(\n",
    "        #     pretrained_path=pretrained_path,\n",
    "        #     return_intermediate=False,\n",
    "        # )\n",
    "        text_model = MetaCLIPTextTransformer.from_pretrained(pretrained_path)\n",
    "        return cls(vision_model, text_model)\n",
    "\n",
    "    def get_text_features(\n",
    "        self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        return self.text_model(input_ids)\n",
    "\n",
    "    def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        return self.vision_model(pixel_values)\n",
    "    \n",
    "def load_clip_model(clip_model_path: str):\n",
    "\n",
    "    clip_model = OpenCLIPModelWrapper.from_pretrained(clip_model_path)\n",
    "    clip_processor = CLIPProcessor.from_pretrained(clip_model_path)\n",
    "    print(f\"Successfully loaded clip model and preprocessor from {clip_model_path}\")\n",
    "    \n",
    "    return clip_model, clip_processor\n",
    "\n",
    "def load_label_templates(filename: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load the ImageNet label templates or other custom templates in txt format.\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        templates = f.readlines()\n",
    "    return [item.strip(\"\\n\") for item in templates]\n",
    "\n",
    "def load_label_names(filename: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load the ImageNet label names / categories or other custom label names in txt format.\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "        labels = [\",\".join(x.split(\",\")[1:]).strip(\"\\n\") for x in content]\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_label_embeddings(\n",
    "    clip_model,\n",
    "    processor: CLIPProcessor,\n",
    "    labels: List[str],\n",
    "    templates: List[str],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    For ImageNet Evaluation, the first step is to get the averaged embedding of each category / label.\n",
    "    \"\"\"\n",
    "    # pyre-fixme[16]: `CLIPModel` has no attribute `text_model`.\n",
    "    clip_model.text_model = clip_model.text_model.cuda()\n",
    "    # pyre-fixme[16]: `CLIPModel` has no attribute `text_projection`.\n",
    "    clip_model.text_projection = clip_model.text_projection.cuda()\n",
    "    with torch.no_grad():\n",
    "        all_text_embeds = []\n",
    "        \n",
    "        for label in labels:\n",
    "            # put one label / category in the templates\n",
    "            texts = [template.format(label) for template in templates]\n",
    "            # text to token ids in CLIP way\n",
    "            tokenized_outputs = processor(text=texts, return_tensors=\"pt\", padding=True)\n",
    "            text_input_ids, text_attn_masks = (\n",
    "                tokenized_outputs.input_ids,\n",
    "                tokenized_outputs.attention_mask,\n",
    "            )\n",
    "            text_input_ids = text_input_ids.cuda()\n",
    "            text_attn_masks = text_attn_masks.cuda()\n",
    "            # pyre-fixme[16]: `CLIPModel` has no attribute `get_text_features`.\n",
    "            text_features = clip_model.get_text_features(\n",
    "                input_ids=text_input_ids,\n",
    "                attention_mask=text_attn_masks,\n",
    "            )\n",
    "            text_embeds = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "            # averaged embedding of a category\n",
    "            text_embeds = text_embeds.mean(dim=0)\n",
    "            text_embeds /= text_embeds.norm()\n",
    "\n",
    "            all_text_embeds.append(text_embeds)\n",
    "\n",
    "    return torch.stack(all_text_embeds)\n",
    "\n",
    "\n",
    "class ImageFolderWithNames(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, target, path\n",
    "    \n",
    "class ImageNetCollator():\n",
    "    def __init__(self, f_transform):\n",
    "        # Assuming f_transform is CLIPProcessor\n",
    "        assert isinstance(f_transform, CLIPProcessor)\n",
    "        self.f_transform = f_transform\n",
    "    \n",
    "    def __call__(self, batch_data: list[dict[str, Any]])-> Optional[dict[str, Any]]:\n",
    "        images = [item[0] for item in batch_data]\n",
    "        images = self.f_transform(images=images, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        \n",
    "        labels = [item[1] for item in batch_data]\n",
    "        image_names = [item[2] for item in batch_data]\n",
    "        \n",
    "        return {\"images\": images, \"labels\": labels, \"image_names\": image_names}\n",
    "\n",
    "\n",
    "def create_imagenet_dataloader(data_path: str, f_transform, batch_size=8, num_workers=0, sampler=None):\n",
    "\n",
    "    dataset = ImageFolderWithNames(data_path)\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        sampler=sampler,\n",
    "        collate_fn=ImageNetCollator(f_transform=f_transform),\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_config: {'output_dim': 1280, 'global_layers': -1, 'relative_pos_embed_type': 'rope_2d', 'pool_type': 'attn', 'embed_cls_token': False, 'mim_loss': True, 'mim_ratio': 0.0625, 'pos_embed_type': 'learnable', 'image_size': 392, 'patch_size': 14, 'layers': 50, 'width': 1536, 'heads': 16, 'mlp_ratio': 5.833333334, 'ckpt_path': '/fsx_0/checkpoints/clip/MetaCLIP-Gs_224_fair_vev02_ckpt_epoch_360/epoch_360.pt'}\n",
      "img_idx.shape=torch.Size([784, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MetaCLIPVEv02 Visual Pretrained Model Missing keys: []\n",
      "MetaCLIPVEv02 Visual Pretrained Model Unexpected keys: []\n",
      "MetaCLIPTextTransformer Pretrained Model Missing keys: []\n",
      "MetaCLIPTextTransformer Pretrained Model Unexpected keys: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded clip model and preprocessor from /fsx_0/checkpoints/clip/MetaCLIP-Gs_224_fair_vev02_ckpt_epoch_360\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"/fsx_0/checkpoints/clip/MetaCLIP-Gs_224_fair_vev02_ckpt_epoch_360\"\n",
    "clip_model, clip_processor = load_clip_model(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each sample will compare against 1000 labels\n"
     ]
    }
   ],
   "source": [
    "templates = load_label_templates(\n",
    "    \"/fsx_0/user/tranx/github/openCLIPMeta/scripts/tranx/clip_imagenet_templates.txt\"\n",
    ")\n",
    "\n",
    "label_names = load_label_names(\n",
    "    \"/fsx_0/user/tranx/github/openCLIPMeta/scripts/tranx/clip_modified_full_size_labels.txt\"\n",
    ")\n",
    "print(f\"Each sample will compare against {len(label_names)} labels\")\n",
    "\n",
    "\n",
    "\n",
    "label_embeds = get_label_embeddings(\n",
    "    clip_model, clip_processor, label_names, templates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "dataloader = create_imagenet_dataloader(\n",
    "    data_path=\"/fsx_0/dataset01/imagenet/val\",\n",
    "    f_transform=clip_processor,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=24\n",
    ")\n",
    "# batch = next(iter(dataloader))\n",
    "# batch\n",
    "\n",
    "total_samples = len(dataloader.dataset)\n",
    "num_batches = (total_samples // batch_size) + (total_samples % batch_size != 0)\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPProcessor:\n",
       "- image_processor: CLIPImageProcessor {\n",
       "  \"crop_size\": {\n",
       "    \"height\": 392,\n",
       "    \"width\": 392\n",
       "  },\n",
       "  \"do_center_crop\": true,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 392\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: CLIPTokenizerFast(name_or_path='/fsx_0/checkpoints/clip/MetaCLIP-Gs_224_fair_vev02_ckpt_epoch_360', vocab_size=49408, model_max_length=32, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"CLIPProcessor\"\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tranx] type(x): <class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/391 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "packed_img_idx is required for RoPE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, total\u001b[38;5;241m=\u001b[39mnum_batches):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# for i, batch in enumerate(dataloader):\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# for batch in tqdm(dataloader):\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     images \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 12\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[43mclip_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m image_features \u001b[38;5;241m/\u001b[39m image_features\u001b[38;5;241m.\u001b[39mnorm(\n\u001b[1;32m     15\u001b[0m             p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         )\n\u001b[1;32m     18\u001b[0m     logits_per_text \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(label_embeds, image_embeds\u001b[38;5;241m.\u001b[39mt())\n",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m, in \u001b[0;36mOpenCLIPModelWrapper.get_image_features\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_image_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:1413\u001b[0m, in \u001b[0;36mMetaCLIPVEv02.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1406\u001b[0m     x, packed_img_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m   1407\u001b[0m         x,\n\u001b[1;32m   1408\u001b[0m         packed_img_idx\u001b[38;5;241m=\u001b[39mpacked_img_idx,\n\u001b[1;32m   1409\u001b[0m         windows_per_img\u001b[38;5;241m=\u001b[39mwindows_per_img,  \u001b[38;5;66;03m# pyre-ignore[61]\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m         packing_boundaries\u001b[38;5;241m=\u001b[39mpacking_boundaries,  \u001b[38;5;66;03m# pyre-ignore[61]\u001b[39;00m\n\u001b[1;32m   1411\u001b[0m     )\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1413\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;66;03m# if do_mim_loss:\u001b[39;00m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;66;03m#     # Undo the MIM concat\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;66;03m#     x, mim_x = x[:-num_mim_tokens], x[-num_mim_tokens:]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m#     batch_size = batch_size - num_mim_samples\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;66;03m#     windows_per_img = windows_per_img[:batch_size]\u001b[39;00m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[tranx] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpacked_input\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:583\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, packed_img_idx, windows_per_img, packing_boundaries)\u001b[0m\n\u001b[1;32m    557\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[1;32m    558\u001b[0m         torch\u001b[38;5;241m.\u001b[39meq(\n\u001b[1;32m    559\u001b[0m             packed_img_idx[:, :, PACKED\u001b[38;5;241m.\u001b[39mIDX, \u001b[38;5;28;01mNone\u001b[39;00m], PACKED\u001b[38;5;241m.\u001b[39mID_PAD_TOKEN\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m         x,\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# if (\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m#     self.grad_checkpointing\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m#     and not torch.jit.is_scripting()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_img_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacked_img_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindows_per_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindows_per_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacking_boundaries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacking_boundaries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle_windows:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshuffler_windows not implemented yet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:446\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, q_x, k_x, v_x, attn_mask, need_weights, packed_img_idx, windows_per_img, packing_boundaries)\u001b[0m\n\u001b[1;32m    437\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpacked_global_attn(\n\u001b[1;32m    438\u001b[0m         q_x_normalized,\n\u001b[1;32m    439\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m         packing_boundaries\u001b[38;5;241m=\u001b[39mpacking_boundaries,\n\u001b[1;32m    443\u001b[0m     )\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# We want to do window attention / our input is not packed (no need to unpack)\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     attn_output, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_x_normalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpacked_img_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacked_img_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m x \u001b[38;5;241m=\u001b[39m q_x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls_1(attn_output)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# PaLM parallel transfomer formulation with PreNorm https://arxiv.org/pdf/2204.02311.pdf\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# y = x + MLP(LayerNorm(x)) + Attention(LayerNorm(x))\u001b[39;00m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:333\u001b[0m, in \u001b[0;36mResidualAttentionBlock._call_attention\u001b[0;34m(self, q_x, k_x, v_x, attn_mask, need_weights, packed_img_idx, tokens_per_img)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_pos_embed_type:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;66;03m# xformers-based attn\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m need_weights\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpacked_img_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacked_img_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokens_per_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens_per_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    341\u001b[0m         q_x, k_x, v_x, need_weights\u001b[38;5;241m=\u001b[39mneed_weights, attn_mask\u001b[38;5;241m=\u001b[39mattn_mask\n\u001b[1;32m    342\u001b[0m     )\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:142\u001b[0m, in \u001b[0;36mAttentionXformer4x.forward\u001b[0;34m(self, query, attn_mask, packed_img_idx, tokens_per_img)\u001b[0m\n\u001b[1;32m    139\u001b[0m v_ \u001b[38;5;241m=\u001b[39m rearrange(v_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb s (h d) -> b s h d\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_pos_embed_type:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m packed_img_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpacked_img_idx is required for RoPE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m     q_ \u001b[38;5;241m=\u001b[39m rearrange(q_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb s h d -> b h s d\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m     k_ \u001b[38;5;241m=\u001b[39m rearrange(k_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb s h d -> b h s d\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: packed_img_idx is required for RoPE"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "clip_model.vision_model = clip_model.vision_model.cuda()\n",
    "clip_model.visual_projection = clip_model.visual_projection.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, total=num_batches):\n",
    "    # for i, batch in enumerate(dataloader):\n",
    "    # for batch in tqdm(dataloader):\n",
    "        images = batch[\"images\"].cuda()\n",
    "        image_features = clip_model.get_image_features(images)\n",
    "        \n",
    "        image_embeds = image_features / image_features.norm(\n",
    "                p=2, dim=-1, keepdim=True\n",
    "            )\n",
    "        \n",
    "        logits_per_text = torch.matmul(label_embeds, image_embeds.t())\n",
    "        # pyre-fixme[16]: `CLIPModel` has no attribute `logit_scale`.\n",
    "        logits_per_image = logits_per_text.t() * clip_model.logit_scale.exp()\n",
    "        # confidences of each sample over the classes\n",
    "        probs = logits_per_image.softmax(dim=1)\n",
    "        # get the top1 prediction\n",
    "        preds = probs.argmax(dim=1)\n",
    "        # add to all_preds => List[int]\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(batch[\"labels\"])\n",
    "    \n",
    "accuracy = 0.0\n",
    "if len(all_labels) == len(all_preds):\n",
    "    accuracy_metrics = Accuracy(num_classes=1000, task='multiclass')\n",
    "    accuracy = accuracy_metrics(\n",
    "        torch.Tensor(all_preds).long(), torch.Tensor(all_labels).long()\n",
    "    ).item()\n",
    "print(f\"Top-1 Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_config = {\n",
    "    \"output_dim\": 1280,\n",
    "    \"global_layers\": -1,\n",
    "    \"relative_pos_embed_type\": \"rope_2d\",\n",
    "    \"pool_type\": \"attn\",\n",
    "    \"embed_cls_token\": False,\n",
    "\n",
    "    \"mim_loss\": True,\n",
    "    \"mim_ratio\": 0.0625,\n",
    "    \n",
    "    \"pos_embed_type\": \"learnable\",\n",
    "    \"image_size\": 392,\n",
    "    \n",
    "    \"patch_size\": 14,\n",
    "\n",
    "    \"layers\": 50,\n",
    "    \"width\": 1536,\n",
    "    \"heads\": 16,\n",
    "    \"mlp_ratio\": 5.833333334\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    CLIPProcessor:\n",
       "- image_processor: CLIPImageProcessor {\n",
       "  \"crop_size\": {\n",
       "    \"height\": 392,\n",
       "    \"width\": 392\n",
       "  },\n",
       "  \"do_center_crop\": true,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 392\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: CLIPTokenizerFast(name_or_path='/fsx_0/checkpoints/clip/MetaCLIP-Gs_224_fair_vev02_ckpt_epoch_360', vocab_size=49408, model_max_length=32, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"CLIPProcessor\"\n",
       "}\n",
       "\n",
       "    PackWindowsAndPad(window_size=1, patch_size=(14, 14), window_shape=none)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform2 = Compose([clip_processor, PackWindowsAndPad(\n",
    "    window_size=1, patch_size=14, insert_cls_token=False\n",
    ")])\n",
    "\n",
    "transform2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\"/fsx_0/dataset01/imagenet/val\", transform=transform2)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    sampler=None,\n",
    "    collate_fn=CollatePackedWindows,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolderWithNames(\"/fsx_0/dataset01/imagenet/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    sampler=None,\n",
    "    collate_fn=CollatePackedWindows,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 247, in __getitem__\n    sample = self.transform(sample)\n  File \"/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n  File \"/home/tranx/.local/lib/python3.10/site-packages/transformers/models/clip/processing_clip.py\", line 106, in __call__\n    encoding = self.tokenizer(text, return_tensors=return_tensors, **tokenizer_kwargs)\n  File \"/home/tranx/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3016, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n  File \"/home/tranx/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3076, in _call_one\n    raise ValueError(\nValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 247, in __getitem__\n    sample = self.transform(sample)\n  File \"/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n  File \"/home/tranx/.local/lib/python3.10/site-packages/transformers/models/clip/processing_clip.py\", line 106, in __call__\n    encoding = self.tokenizer(text, return_tensors=return_tensors, **tokenizer_kwargs)\n  File \"/home/tranx/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3016, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n  File \"/home/tranx/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3076, in _call_one\n    raise ValueError(\nValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        sampler=sampler,\n",
    "        collate_fn=ImageNetCollator(f_transform=f_transform),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12498"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "dataloader = create_imagenet_dataloader(\n",
    "    data_path=\"/fsx_0/dataset01/imagenet/val\",\n",
    "    f_transform=clip_processor,\n",
    "    # f_transform=transform2,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=24\n",
    ")\n",
    "# batch = next(iter(dataloader))\n",
    "# batch\n",
    "\n",
    "total_samples = len(dataloader.dataset)\n",
    "num_batches = (total_samples // batch_size) + (total_samples % batch_size != 0)\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "images = batch[\"images\"].cuda(device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_idx.shape=torch.Size([784, 1])\n"
     ]
    }
   ],
   "source": [
    "vision_model = MetaCLIPVEv02(**vision_config) \n",
    "vision_model = vision_model.cuda(device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_model.vision_select_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([4, 784, 1536])\n",
      "x.shape=torch.Size([4, 784, 1536])\n",
      "self.positional_embedding.shape=torch.Size([784, 1536])\n",
      "packed_img_idx.shape=torch.Size([4, 784, 7])\n",
      "x.shape=torch.Size([4, 784, 1536])\n",
      "int_x.shape=torch.Size([4, 784, 7])\n",
      "return_x_batch=tensor([[      1136,          0,          0,  ...,         28,         28,\n",
      "                  0],\n",
      "        [     22069,          0,          1,  ...,         28,         28,\n",
      "                  1],\n",
      "        [1694805312,          0,          2,  ...,         28,         28,\n",
      "                  2],\n",
      "        ...,\n",
      "        [     32684,         27,         25,  ...,         28,         28,\n",
      "                781],\n",
      "        [ 325064608,         27,         26,  ...,         28,         28,\n",
      "                782],\n",
      "        [     32684,         27,         27,  ...,         28,         28,\n",
      "                783]], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvision_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_vev01\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:1685\u001b[0m, in \u001b[0;36mMetaCLIPVEv02.forward_vev01\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_x_batch\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_select_feature \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1685\u001b[0m     return_x \u001b[38;5;241m=\u001b[39m \u001b[43mreturn_x_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1687\u001b[0m     return_x \u001b[38;5;241m=\u001b[39m return_x\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "vision_model.forward_vev01(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class PACKED:\n",
    "    Z         = 0  # Z (time) coordinate of the token in the original sample\n",
    "    Y         = 1  # Y (height) coordinate of the token in the original sample\n",
    "    X         = 2  # X (width) coordinate of the token in the original sample\n",
    "    TIME      = 3  # Total number of time units (frames) in the original sample\n",
    "    HEIGHT    = 4  # Height of the original sample\n",
    "    WIDTH     = 5  # Width of the original sample\n",
    "    # USE INDEX TO CHECK THE TYPE OF THE TOKEN (see ID fields below)\n",
    "    IDX       = 6  # Full index of the token in the original sample (x + y * w + z * w * h)\n",
    "    BATCH_IDX = 7  # Which batch element this token belongs to.\n",
    "\n",
    "    # Total size of the enum, remember to update this!\n",
    "    NUM_METADATA = 8\n",
    "\n",
    "    # Note: For padding tokens IDX = -1\n",
    "    #       For cls tokens,    IDX = -2\n",
    "    ID_CLS_TOKEN = -2\n",
    "    ID_PAD_TOKEN = -1\n",
    "    \n",
    "class PackWindowsAndPad:\n",
    "    \"\"\" Takes a torch image, chunks it into windows, and pads to the nearest window. \"\"\"\n",
    "    def __init__(self, window_size, patch_size, insert_cls_token):\n",
    "        self.window_size = window_size  # can be (h, w) or #tokens, if #tokens use hilbert, if (h, w) use z-order\n",
    "        self.patch_size  = patch_size if isinstance(patch_size, (list, tuple)) else (patch_size, patch_size)\n",
    "        self.insert_cls_token = insert_cls_token\n",
    "\n",
    "        if isinstance(window_size, (list, tuple)):\n",
    "            self.window_shape = \"quad\"\n",
    "        elif window_size == 1:\n",
    "            self.window_shape = \"none\"\n",
    "        else:\n",
    "            self.window_shape = \"hilbert\"\n",
    "\n",
    "    def __call__(self, img: torch.Tensor):\n",
    "        c, ih, iw = img.shape\n",
    "\n",
    "        # Quad padding requires the image to be a multiple of window size, while hilbert is just patch size\n",
    "        patch_h, patch_w = self.patch_size\n",
    "        \n",
    "        h = int(math.ceil(ih / patch_h)) * patch_h\n",
    "        w = int(math.ceil(iw / patch_w)) * patch_w\n",
    "\n",
    "        pt = (h - ih) // 2\n",
    "        pb = (h - ih) - pt\n",
    "        pl = (w - iw) // 2\n",
    "        pr = (w - iw) - pl\n",
    "\n",
    "        # Check if we need to pad at all because idk if pytorch internally checks\n",
    "        if not (pt == pb and pl == pr and pr == 0):\n",
    "            img = F.pad(img, [pl, pt, pr, pb], fill=0)\n",
    "\n",
    "        img = img.unfold(-2, patch_h, patch_h).unfold(-2, patch_w, patch_w)\n",
    "        _, grid_h, grid_w, _, _ = img.shape\n",
    "\n",
    "        img_idx = torch.arange(h*w // (patch_h * patch_w), dtype=torch.int32)\n",
    "        img_idx = img_idx.reshape(h // patch_h, w // patch_w)\n",
    "\n",
    "        # Then rearrange the patches into windows and pad to the nearest window (in the case of hilbert)\n",
    "        if self.window_shape == \"quad\":\n",
    "            new_grid_h = int(math.ceil(grid_h / self.window_size[0])) * self.window_size[0]\n",
    "            new_grid_w = int(math.ceil(grid_w / self.window_size[1])) * self.window_size[1]\n",
    "\n",
    "            pb = (new_grid_h - grid_h)\n",
    "            pr = (new_grid_w - grid_w)\n",
    "\n",
    "            img = img.reshape(c, grid_h, -1)\n",
    "            img = F.pad(img, [0, 0, pr * patch_h * patch_w, pb], fill=0)\n",
    "\n",
    "            img = img.reshape(c, new_grid_h, new_grid_w, patch_h, patch_w)\n",
    "            img = img.unfold(1, self.window_size[0], self.window_size[0]).unfold(2, self.window_size[1], self.window_size[1])\n",
    "            img = img.permute(0, 1, 2, 5, 6, 3, 4)  # oof\n",
    "            img = img.reshape(c, -1, self.window_size[0] * self.window_size[1], *self.patch_size)\n",
    "\n",
    "            img_idx = F.pad(img_idx[None, ...], [0, 0, pr, pb], fill=PACKED.ID_PAD_TOKEN)[0]\n",
    "            img_idx = img_idx.unfold(-2, self.window_size[0], self.window_size[0]).unfold(-2, self.window_size[1], self.window_size[1])\n",
    "            img_idx = img_idx.reshape(img.shape[1], img.shape[2])\n",
    "\n",
    "            if self.insert_cls_token:\n",
    "                # To be honest, idk where to put the cls token with quad packing so I'll just put it at the end, which is probably a padding token\n",
    "                img_idx[-1, -1] = PACKED.ID_CLS_TOKEN\n",
    "            \n",
    "        elif self.window_shape == \"hilbert\":\n",
    "            # Rearrange the patches according to a hilbert curve sequence then pad.\n",
    "                from .gilbert import gilbert2d\n",
    "                idx = torch.Tensor(list(gilbert2d(grid_w, grid_h))).long().to(device=img.device, dtype=torch.long)\n",
    "                img = img[:, idx[:, 1], idx[:, 0], :, :]\n",
    "                img = img.reshape(c, -1, self.patch_size[0] * self.patch_size[1])\n",
    "\n",
    "                extra_tokens = 1 if self.insert_cls_token else 0\n",
    "\n",
    "                # Add padding tokens up to the nearest window\n",
    "                pad_tokens = int(math.ceil((grid_h * grid_w + extra_tokens) / self.window_size)) * self.window_size - grid_h * grid_w\n",
    "                img = F.pad(img, [0, 0, 0, pad_tokens], fill=0)\n",
    "                img = img.reshape(c, -1, self.window_size, *self.patch_size)\n",
    "\n",
    "                img_idx = img_idx[idx[:, 1], idx[:, 0]]\n",
    "                img_idx = F.pad(img_idx.view(1, -1, 1), [0, 0, 0, pad_tokens], fill=PACKED.ID_PAD_TOKEN)  # use -1 idx for posemb padding tokens\n",
    "                img_idx = img_idx.reshape(-1, self.window_size)\n",
    "\n",
    "                if self.insert_cls_token:\n",
    "                    # Not true anymore: We guaranteed above that there is at least one padding token at the end for the cls token\n",
    "                    # Add a cls token to every window, we're gonna avg the cls token logits at the end\n",
    "                    img_idx[:, -1] = PACKED.ID_CLS_TOKEN\n",
    "\n",
    "        elif self.window_shape == \"none\":\n",
    "            # Don't do any reordering, just immediately reshape into windows\n",
    "            # TODO: Add padding if we want to use this with window size > 1\n",
    "            img = img.reshape(c, -1, self.window_size, *self.patch_size)\n",
    "            img_idx = img_idx.reshape(-1, self.window_size)\n",
    "\n",
    "            # This currently assumes window_size = 1\n",
    "            if self.insert_cls_token:\n",
    "                assert self.window_size == 1\n",
    "                img = torch.cat([img, img[:, :1]], dim=1)\n",
    "                img_idx = torch.cat([img_idx, img_idx[:1]], dim=0)\n",
    "                img_idx[-1, -1] = PACKED.ID_CLS_TOKEN\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown window shape type: ({self.window_shape}) in {repr(self)}.\")\n",
    "\n",
    "        # Save the padded image size (after patchification, e.g., for position embeddings)\n",
    "        idx_h = h // self.patch_size[0]\n",
    "        idx_w = w // self.patch_size[1]\n",
    "\n",
    "        # Last metadata is added by collate\n",
    "        packed_img_idx = torch.empty(img_idx.shape[0], img_idx.shape[1], PACKED.NUM_METADATA - 1, dtype=torch.int32)\n",
    "\n",
    "        packed_img_idx[:, :, PACKED.Z].fill_(0)\n",
    "        packed_img_idx[:, :, PACKED.Y] = img_idx // idx_w\n",
    "        packed_img_idx[:, :, PACKED.X] = img_idx % idx_w\n",
    "        packed_img_idx[:, :, PACKED.TIME].fill_(1)\n",
    "        packed_img_idx[:, :, PACKED.HEIGHT].fill_(idx_h)\n",
    "        packed_img_idx[:, :, PACKED.WIDTH].fill_(idx_w)\n",
    "        packed_img_idx[:, :, PACKED.IDX] = img_idx\n",
    "\n",
    "        # Return shape: [c, #windows, window_size, patch_height, patch_width]\n",
    "        return img, packed_img_idx\n",
    "    \n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + f'(window_size={self.window_size}'\n",
    "        format_string += f', patch_size={self.patch_size}'\n",
    "        format_string += f', window_shape={self.window_shape})'\n",
    "        return format_string\n",
    "\n",
    "\n",
    "pp = PackWindowsAndPad(\n",
    "    window_size=1, patch_size=14, insert_cls_token=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "batch_pp = [pp(x) for x in images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 128, 14, 14])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [(packed_img, packed_img_idx, num_windows\u001b[38;5;241m.\u001b[39mtolist(), packing_boundaries), target]\n\u001b[1;32m     37\u001b[0m pp_collator \u001b[38;5;241m=\u001b[39m CollatePackedWindows()\n\u001b[0;32m---> 39\u001b[0m batch_packed \u001b[38;5;241m=\u001b[39m \u001b[43mpp_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_pp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 26\u001b[0m, in \u001b[0;36mCollatePackedWindows.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     24\u001b[0m packed_img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(packed_img\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 26\u001b[0m packed_image \u001b[38;5;241m=\u001b[39m \u001b[43mpacked_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m     28\u001b[0m packed_img_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     30\u001b[0m element_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(\u001b[38;5;28msum\u001b[39m([[i]\u001b[38;5;241m*\u001b[39mn \u001b[38;5;28;01mfor\u001b[39;00m i, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(num_windows)], []))\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint32)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 5"
     ]
    }
   ],
   "source": [
    "class CollatePackedWindows:\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = [x for x in batch if x is not None]\n",
    "\n",
    "        # Sort the batch by number of windows\n",
    "        batch.sort(key=lambda x: x[0][0].shape[1])\n",
    "\n",
    "        data = [item[0] for item in batch]\n",
    "        target = [item[1] for item in batch]\n",
    "        \n",
    "        # To handle both plain integer targets and captions\n",
    "        if isinstance(target, (list, tuple)) and isinstance(target[0], torch.Tensor):\n",
    "            target = torch.stack(target, dim=0)\n",
    "        else:\n",
    "            target = torch.Tensor(target)\n",
    "\n",
    "        num_windows = torch.Tensor([x[0].shape[1] for x in data]).long()\n",
    "\n",
    "        packed_num_windows, packed_counts = torch.unique(num_windows, return_counts=True)\n",
    "        packed_end_idx = (packed_counts * packed_num_windows).cumsum(dim=0)\n",
    "        packing_boundaries = [packed_num_windows.tolist(), packed_end_idx.tolist()]\n",
    "        \n",
    "        packed_img = torch.cat([x[0] for x in data], dim=1)\n",
    "        print(packed_img.shape)\n",
    "        packed_image = packed_img.permute(1, 0, 2, 3, 4).contiguous()\n",
    "        \n",
    "        packed_img_idx = torch.cat([x[1] for x in data], dim=0)\n",
    "\n",
    "        element_idx = torch.Tensor(sum([[i]*n for i, n in enumerate(num_windows)], [])).to(torch.int32)\n",
    "        element_idx = element_idx.view(-1, 1).tile((1, packed_img_idx.shape[1]))\n",
    "\n",
    "        packed_img_idx = torch.cat([packed_img_idx, element_idx[..., None]], dim=-1)\n",
    "\n",
    "        return [(packed_img, packed_img_idx, num_windows.tolist(), packing_boundaries), target]\n",
    "    \n",
    "pp_collator = CollatePackedWindows()\n",
    "\n",
    "batch_packed = pp_collator(batch_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_pp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tranx] type(x): <class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "packed_img_idx is required for RoPE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvision_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_metaclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:1413\u001b[0m, in \u001b[0;36mMetaCLIPVEv02.forward_metaclip\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1406\u001b[0m     x, packed_img_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m   1407\u001b[0m         x,\n\u001b[1;32m   1408\u001b[0m         packed_img_idx\u001b[38;5;241m=\u001b[39mpacked_img_idx,\n\u001b[1;32m   1409\u001b[0m         windows_per_img\u001b[38;5;241m=\u001b[39mwindows_per_img,  \u001b[38;5;66;03m# pyre-ignore[61]\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m         packing_boundaries\u001b[38;5;241m=\u001b[39mpacking_boundaries,  \u001b[38;5;66;03m# pyre-ignore[61]\u001b[39;00m\n\u001b[1;32m   1411\u001b[0m     )\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1413\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;66;03m# if do_mim_loss:\u001b[39;00m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;66;03m#     # Undo the MIM concat\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;66;03m#     x, mim_x = x[:-num_mim_tokens], x[-num_mim_tokens:]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m#     batch_size = batch_size - num_mim_samples\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;66;03m#     windows_per_img = windows_per_img[:batch_size]\u001b[39;00m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[tranx] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpacked_input\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:583\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, packed_img_idx, windows_per_img, packing_boundaries)\u001b[0m\n\u001b[1;32m    557\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[1;32m    558\u001b[0m         torch\u001b[38;5;241m.\u001b[39meq(\n\u001b[1;32m    559\u001b[0m             packed_img_idx[:, :, PACKED\u001b[38;5;241m.\u001b[39mIDX, \u001b[38;5;28;01mNone\u001b[39;00m], PACKED\u001b[38;5;241m.\u001b[39mID_PAD_TOKEN\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m         x,\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# if (\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m#     self.grad_checkpointing\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m#     and not torch.jit.is_scripting()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_img_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacked_img_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindows_per_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindows_per_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacking_boundaries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacking_boundaries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle_windows:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshuffler_windows not implemented yet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:446\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, q_x, k_x, v_x, attn_mask, need_weights, packed_img_idx, windows_per_img, packing_boundaries)\u001b[0m\n\u001b[1;32m    437\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpacked_global_attn(\n\u001b[1;32m    438\u001b[0m         q_x_normalized,\n\u001b[1;32m    439\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m         packing_boundaries\u001b[38;5;241m=\u001b[39mpacking_boundaries,\n\u001b[1;32m    443\u001b[0m     )\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# We want to do window attention / our input is not packed (no need to unpack)\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     attn_output, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_x_normalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpacked_img_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacked_img_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m x \u001b[38;5;241m=\u001b[39m q_x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls_1(attn_output)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# PaLM parallel transfomer formulation with PreNorm https://arxiv.org/pdf/2204.02311.pdf\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# y = x + MLP(LayerNorm(x)) + Attention(LayerNorm(x))\u001b[39;00m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:333\u001b[0m, in \u001b[0;36mResidualAttentionBlock._call_attention\u001b[0;34m(self, q_x, k_x, v_x, attn_mask, need_weights, packed_img_idx, tokens_per_img)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_pos_embed_type:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;66;03m# xformers-based attn\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m need_weights\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpacked_img_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacked_img_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokens_per_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens_per_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    341\u001b[0m         q_x, k_x, v_x, need_weights\u001b[38;5;241m=\u001b[39mneed_weights, attn_mask\u001b[38;5;241m=\u001b[39mattn_mask\n\u001b[1;32m    342\u001b[0m     )\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:142\u001b[0m, in \u001b[0;36mAttentionXformer4x.forward\u001b[0;34m(self, query, attn_mask, packed_img_idx, tokens_per_img)\u001b[0m\n\u001b[1;32m    139\u001b[0m v_ \u001b[38;5;241m=\u001b[39m rearrange(v_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb s (h d) -> b s h d\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_pos_embed_type:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m packed_img_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpacked_img_idx is required for RoPE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m     q_ \u001b[38;5;241m=\u001b[39m rearrange(q_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb s h d -> b h s d\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m     k_ \u001b[38;5;241m=\u001b[39m rearrange(k_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb s h d -> b h s d\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: packed_img_idx is required for RoPE"
     ]
    }
   ],
   "source": [
    "vision_model.forward_metaclip(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/functional.py:4343: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid configuration argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:1588\u001b[0m, in \u001b[0;36mMetaCLIPVEv02.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m   1585\u001b[0m images_package \u001b[38;5;241m=\u001b[39m [images, packed_img_idxs, tokens_per_img]\n\u001b[1;32m   1586\u001b[0m \u001b[38;5;66;03m# image_features = self.forward_vith(images_package, None)\u001b[39;00m\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;66;03m# image_features = self.forward_metaclip(images_package)\u001b[39;00m\n\u001b[0;32m-> 1588\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_vitg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_package\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m reshaped_tensor \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mview(bz, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth)\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reshaped_tensor\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:1531\u001b[0m, in \u001b[0;36mMetaCLIPVEv02.forward_vitg\u001b[0;34m(self, images, ar)\u001b[0m\n\u001b[1;32m   1528\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_pre(x)\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;66;03m# assert self.return_intermediate, \"return_intermediate must be True\"\u001b[39;00m\n\u001b[0;32m-> 1531\u001b[0m x, int_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_img_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacked_img_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindows_per_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens_per_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# return_intermediate=self.return_intermediate,\u001b[39;49;00m\n\u001b[1;32m   1536\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1538\u001b[0m return_x_batch \u001b[38;5;241m=\u001b[39m int_x[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_select_layer]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1539\u001b[0m return_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(\n\u001b[1;32m   1540\u001b[0m     return_x_batch, [\u001b[38;5;28mint\u001b[39m(num\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;28;01mfor\u001b[39;00m num \u001b[38;5;129;01min\u001b[39;00m tokens_per_img]\n\u001b[1;32m   1541\u001b[0m )\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:583\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, packed_img_idx, windows_per_img, packing_boundaries)\u001b[0m\n\u001b[1;32m    557\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[1;32m    558\u001b[0m         torch\u001b[38;5;241m.\u001b[39meq(\n\u001b[1;32m    559\u001b[0m             packed_img_idx[:, :, PACKED\u001b[38;5;241m.\u001b[39mIDX, \u001b[38;5;28;01mNone\u001b[39;00m], PACKED\u001b[38;5;241m.\u001b[39mID_PAD_TOKEN\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m         x,\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# if (\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m#     self.grad_checkpointing\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m#     and not torch.jit.is_scripting()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_img_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacked_img_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindows_per_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindows_per_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacking_boundaries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacking_boundaries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle_windows:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshuffler_windows not implemented yet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:446\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, q_x, k_x, v_x, attn_mask, need_weights, packed_img_idx, windows_per_img, packing_boundaries)\u001b[0m\n\u001b[1;32m    437\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpacked_global_attn(\n\u001b[1;32m    438\u001b[0m         q_x_normalized,\n\u001b[1;32m    439\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m         packing_boundaries\u001b[38;5;241m=\u001b[39mpacking_boundaries,\n\u001b[1;32m    443\u001b[0m     )\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# We want to do window attention / our input is not packed (no need to unpack)\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     attn_output, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_x_normalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpacked_img_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacked_img_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m x \u001b[38;5;241m=\u001b[39m q_x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls_1(attn_output)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# PaLM parallel transfomer formulation with PreNorm https://arxiv.org/pdf/2204.02311.pdf\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# y = x + MLP(LayerNorm(x)) + Attention(LayerNorm(x))\u001b[39;00m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:333\u001b[0m, in \u001b[0;36mResidualAttentionBlock._call_attention\u001b[0;34m(self, q_x, k_x, v_x, attn_mask, need_weights, packed_img_idx, tokens_per_img)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_pos_embed_type:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;66;03m# xformers-based attn\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m need_weights\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpacked_img_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacked_img_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokens_per_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens_per_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    341\u001b[0m         q_x, k_x, v_x, need_weights\u001b[38;5;241m=\u001b[39mneed_weights, attn_mask\u001b[38;5;241m=\u001b[39mattn_mask\n\u001b[1;32m    342\u001b[0m     )\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_v7/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/encoders/metaclip_vev02.py:206\u001b[0m, in \u001b[0;36mAttentionXformer4x.forward\u001b[0;34m(self, query, attn_mask, packed_img_idx, tokens_per_img)\u001b[0m\n\u001b[1;32m    201\u001b[0m             attn_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogical_not(pad_tokens[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :])\n\u001b[1;32m    203\u001b[0m     flip_hs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: rearrange(\n\u001b[1;32m    204\u001b[0m         x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb s h d -> b h s d\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     )  \u001b[38;5;66;03m# sdpa requires head seq to be second to last dim\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflip_hs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflip_hs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflip_hs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     attn \u001b[38;5;241m=\u001b[39m rearrange(attn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb h s d -> b s (h d)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mlinear(attn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid configuration argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "vision_model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height=336, width=336, scale=0.5833333333333334, downscaled_width=336, downscaled_height=336, wasted_resolution=0\n",
      "effective_resolution=112896, scaled_resolution=112896, wasted_resolution=0\n",
      "height=336, width=672, scale=0.5833333333333334, downscaled_width=336, downscaled_height=336, wasted_resolution=112896\n",
      "effective_resolution=112896, scaled_resolution=225792, wasted_resolution=112896\n",
      "height=336, width=1008, scale=0.5833333333333334, downscaled_width=336, downscaled_height=336, wasted_resolution=225792\n",
      "effective_resolution=112896, scaled_resolution=338688, wasted_resolution=225792\n",
      "height=672, width=336, scale=0.5833333333333334, downscaled_width=336, downscaled_height=336, wasted_resolution=112896\n",
      "effective_resolution=112896, scaled_resolution=225792, wasted_resolution=112896\n",
      "height=672, width=672, scale=1.1666666666666667, downscaled_width=672, downscaled_height=672, wasted_resolution=119808\n",
      "effective_resolution=331776, scaled_resolution=451584, wasted_resolution=119808\n",
      "height=672, width=1008, scale=1.1666666666666667, downscaled_width=672, downscaled_height=672, wasted_resolution=345600\n",
      "effective_resolution=331776, scaled_resolution=677376, wasted_resolution=345600\n",
      "height=1008, width=336, scale=0.5833333333333334, downscaled_width=336, downscaled_height=336, wasted_resolution=225792\n",
      "effective_resolution=112896, scaled_resolution=338688, wasted_resolution=225792\n",
      "height=1008, width=672, scale=1.1666666666666667, downscaled_width=672, downscaled_height=672, wasted_resolution=345600\n",
      "effective_resolution=331776, scaled_resolution=677376, wasted_resolution=345600\n",
      "height=1008, width=1008, scale=1.75, downscaled_width=1008, downscaled_height=1008, wasted_resolution=684288\n",
      "effective_resolution=331776, scaled_resolution=1016064, wasted_resolution=684288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(672, 672)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def select_best_resolution(original_size: tuple, possible_resolutions: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Selects the best resolution from a list of possible resolutions based on the original size.\n",
    "\n",
    "    This is done by calculating the effective and wasted resolution for each possible resolution.\n",
    "\n",
    "    The best fit resolution is the one that maximizes the effective resolution and minimizes the wasted resolution.\n",
    "\n",
    "    Args:\n",
    "        original_size (tuple):\n",
    "            The original size of the image in the format (height, width).\n",
    "        possible_resolutions (list):\n",
    "            A list of possible resolutions in the format [(height1, width1), (height2, width2), ...].\n",
    "\n",
    "    Returns:\n",
    "        tuple: The best fit resolution in the format (height, width).\n",
    "    \"\"\"\n",
    "    original_height, original_width = original_size\n",
    "    best_fit = None\n",
    "    max_effective_resolution = 0\n",
    "    min_wasted_resolution = float(\"inf\")\n",
    "\n",
    "    for height, width in possible_resolutions:\n",
    "        \n",
    "        scale = min(width / original_width, height / original_height)\n",
    "        \n",
    "        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)\n",
    "        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)\n",
    "        wasted_resolution = (width * height) - effective_resolution\n",
    "\n",
    "        print(f\"{height=}, {width=}, {scale=}, {downscaled_width=}, {downscaled_height=}, {wasted_resolution=}\")\n",
    "        print(f\"{effective_resolution=}, scaled_resolution={width * height}, {wasted_resolution=}\")\n",
    "        \n",
    "        if effective_resolution > max_effective_resolution or (\n",
    "            effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution\n",
    "        ):\n",
    "            max_effective_resolution = effective_resolution\n",
    "            min_wasted_resolution = wasted_resolution\n",
    "            best_fit = (height, width)\n",
    "\n",
    "    return best_fit\n",
    "\n",
    "possible_resolutions = []\n",
    "chunk_size = 336\n",
    "for x in range(1,4):\n",
    "    for y in range(1,4):\n",
    "        possible_resolutions.append((x*chunk_size, y*chunk_size))\n",
    "   \n",
    "# original_size = (432, 576)\n",
    "original_size = (576, 576)\n",
    "select_best_resolution(original_size, possible_resolutions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
