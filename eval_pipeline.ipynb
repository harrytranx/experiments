{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\t\n",
    "import sys\n",
    "add_paths = [\n",
    "    \"/fsx_0/user/tranx/rsync\", # ALIGNER_PARENT_DIR\n",
    "    \"/fsx_0/user/tranx/rsync/llm_mm_aligner/replicated\", # ALIGNER_PARENT_DIR/llm_mm_aligner/replicated\n",
    "    # \"/data/home/tranx/conda/envs/aligner_20240822_v2/python-packages\", #CONDA_PREFIX/python-packages\n",
    "    \"/data/home/kapilk/.conda/envs/aligner_20240822_v2/python-packages\"\n",
    "]\n",
    "\n",
    "for p in add_paths:\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n",
    "        \n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llm_mm_aligner.evaluate import main as llm_mm_aligner_evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency\n",
    "from transformers.trainer import TRAINING_ARGS_NAME\n",
    "from typing import Any, Dict, List, Optional, Sequence, Union, cast\n",
    "\n",
    "# fbcode/fblearner/flow/projects/assistant/multimodal/llm_mm_aligner/fblearner_utils.py\n",
    "from itertools import chain\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class JobRunnerArgs:\n",
    "    trainer_args: Optional[Dict[str, Any]] = None\n",
    "    eval_args: Optional[Dict[str, Any]] = None\n",
    "    fsdp_config: Optional[Dict[str, Any]] = None\n",
    "    log_dir: Optional[str] = None\n",
    "    is_mast: Optional[bool] = False\n",
    "    eval_output_path: Optional[str] = None\n",
    "\n",
    "    def get_args_list(self, args: Dict[str, Any]) -> Sequence[str]:\n",
    "        \"\"\"Convert a dict of args to a list of strings for passing to the binary\"\"\"\n",
    "        # TODO: This may introduce some confusion when v is set to false\n",
    "        return list(\n",
    "            chain.from_iterable(\n",
    "                [f\"--{k}\", str(v)] if v is not None else [f\"--{k}\"]\n",
    "                for k, v in args.items()\n",
    "            )\n",
    "        )\n",
    "\n",
    "# fbcode/fblearner/flow/projects/assistant/multimodal/llm_mm_aligner/fblearner.py\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers.trainer import TRAINING_ARGS_NAME\n",
    "from llm_mm_aligner.lib.file_utils import get_pathmgr\n",
    "from llm_mm_aligner.lib.configs import DataTrainingArguments\n",
    "\n",
    "from llm_mm_aligner.lib.file_utils import (\n",
    "    get_pathmgr,\n",
    "    LM_EMBEDDING_NAME,\n",
    "    LM_HEAD_NAME,\n",
    "    PERCEPTION_NAME,\n",
    "    PERCEPTION_TOKENIZER_NAME,\n",
    "    TC_OUTPUT_FILE_PATH,\n",
    ")\n",
    "\n",
    "def manifold_file_exists(str):\n",
    "    pass \n",
    "\n",
    "def extract_data_args(data_args: DataTrainingArguments) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Only keep the ones needed to align the image preprocessing setting with the training for offline evaluation.\n",
    "    \"\"\"\n",
    "    processing_related_args = [\n",
    "        \"resize_longest\",\n",
    "        \"add_resized_image\",\n",
    "        \"chunk_size\",\n",
    "        \"num_image_chunks\",\n",
    "        \"activated_num_image_chunks\",\n",
    "        \"max_total_image_chunks\",\n",
    "        \"instruction_model_type\",\n",
    "        \"frames_per_clip\",\n",
    "        \"clips_per_video\",\n",
    "        \"frame_dilation\",\n",
    "        \"treat_frames_as_images\",\n",
    "    ]\n",
    "\n",
    "    key_data_args = {k: getattr(data_args, k) for k in processing_related_args}\n",
    "    return key_data_args\n",
    "\n",
    "\n",
    "def is_in_sanitized_list(key):\n",
    "    for keyword in [\n",
    "        \"fsdp\",\n",
    "        \"accelerator_config\",\n",
    "        \"lr_scheduler_kwargs\",\n",
    "    ]:\n",
    "        if keyword in key:\n",
    "            print(\"SANITIZE: \", key)\n",
    "            return True\n",
    "    exact_match = [\"debug\"]\n",
    "    if key in exact_match:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def sanitize_eval_args_old(eval_args: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Removes any arguments passed from training/model args which are not\n",
    "    needed for evaluation.\n",
    "    \"\"\"\n",
    "    eval_args = {\n",
    "        k: v\n",
    "        for k, v in eval_args.items()\n",
    "        if v is not None and not is_in_sanitized_list(k)\n",
    "    }\n",
    "    return eval_args\n",
    "\n",
    "def sanitize_eval_args(eval_args: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Removes any arguments passed from training/model args which are not\n",
    "    needed for evaluation.\n",
    "    SANITIZE:  lr_scheduler_kwargs\n",
    "SANITIZE:  fsdp\n",
    "SANITIZE:  fsdp_min_num_params\n",
    "SANITIZE:  fsdp_config\n",
    "SANITIZE:  accelerator_config\n",
    "SANITIZE:  using_fsdp2\n",
    "\n",
    "    \"\"\"\n",
    "    sanitized_keys = [\"debug\", \"fsdp\", \"fsdp_min_num_params\", \"fsdp_config\", \"accelerator_config\"]\n",
    "    \n",
    "    eval_args = {\n",
    "        k: v\n",
    "        for k, v in eval_args.items()\n",
    "        if v is not None and k not in sanitized_keys\n",
    "        # if v is not None and not is_in_sanitized_list(k)\n",
    "    }\n",
    "    \n",
    "    return eval_args\n",
    "\n",
    "\n",
    "def load_and_merge_trainer_to_eval_args(\n",
    "    job_runner_args: JobRunnerArgs,\n",
    ") -> JobRunnerArgs:\n",
    "\n",
    "    ckpt_path_to_load = None\n",
    "    # must specify the \"resume_from_checkpoint\" in eval_args\n",
    "    # pyre-fixme[16]: `Optional` has no attribute `get`.\n",
    "    if job_runner_args.eval_args.get(\"resume_from_checkpoint\", None) is not None:\n",
    "        # pyre-fixme[16]: `Optional` has no attribute `__getitem__`.\n",
    "        ckpt_path_to_load = job_runner_args.eval_args[\"resume_from_checkpoint\"]\n",
    "\n",
    "    if ckpt_path_to_load is None:\n",
    "        raise ValueError(\"No resume_from_checkpoint found in eval_args.\")\n",
    "\n",
    "    # load trainer args from the ckpt path\n",
    "    print(f\"Running test flow on model ckpt {ckpt_path_to_load}\")\n",
    "    training_args_file = os.path.join(ckpt_path_to_load, TRAINING_ARGS_NAME)\n",
    "    print(f\"Reading training arguments from file {training_args_file}\")\n",
    "    if training_args_file.startswith(\"manifold://\") and not manifold_file_exists(\n",
    "        training_args_file\n",
    "    ):\n",
    "        raise ValueError(f\"Training args file {training_args_file} does not exist.\")\n",
    "\n",
    "    # Copy training and model args to eval args. We only keep eval's copy of data and generation args\n",
    "    with get_pathmgr().open(\n",
    "        training_args_file,\n",
    "        \"rb\",\n",
    "    ) as fd:\n",
    "        training_args = torch.load(fd, weights_only=False)\n",
    "        model_args = training_args.model_args\n",
    "        if not hasattr(training_args, \"lr_scheduler_kwargs\"):\n",
    "            training_args.lr_scheduler_kwargs = {}\n",
    "        trainer_training_args = training_args.to_dict()\n",
    "        extracted_data_args = extract_data_args(training_args.data_args)\n",
    "        trainer_model_args = model_args.to_dict()\n",
    "        eval_args = job_runner_args.eval_args\n",
    "        # There should be no overlap between training args and model args\n",
    "        assert len(trainer_training_args.keys() & trainer_model_args) == 0\n",
    "        # print(\"DEBUG trainer_training_args['using_fsdp2']:\", trainer_training_args['using_fsdp2'])\n",
    "        # print(\"DEBUG eval_args['using_fsdp2']:\", eval_args['using_fsdp2'])\n",
    "        # print(\"DEBUG is_in_sanitized_list('using_fsdp2'):\",is_in_sanitized_list('using_fsdp2') )\n",
    "        new_eval_args = {\n",
    "            **trainer_training_args,\n",
    "            **extracted_data_args,\n",
    "            **trainer_model_args,\n",
    "            **eval_args,\n",
    "        }\n",
    "        \n",
    "        # print(\"DEBUG new_eval_args['using_fsdp2']:\", new_eval_args['using_fsdp2'])\n",
    "        \n",
    "        print(f\"eval args: {new_eval_args}\")\n",
    "        # Some eval args needs to be manually overriden/sanitized\n",
    "        new_eval_args = sanitize_eval_args(new_eval_args)\n",
    "        # new_eval_args = sanitize_eval_args_old(new_eval_args)\n",
    "        # print(\"DEBUG 2 new_eval_args['using_fsdp2']:\", new_eval_args['using_fsdp2'])\n",
    "        # set resume path for the classifiers if needed\n",
    "        if model_args.integrity_classifier or model_args.routing_classifier:\n",
    "            # the model_name_or_path must be the resume path\n",
    "            new_eval_args[\"model_name_or_path\"] = new_eval_args[\n",
    "                \"resume_from_checkpoint\"\n",
    "            ]\n",
    "        # set perception based on the freeze_perception arg\n",
    "        \n",
    "        # print(\"DEBUG 3 new_eval_args['using_fsdp2']:\", new_eval_args['using_fsdp2'])\n",
    "        if not model_args.freeze_perception:\n",
    "            perception_path = os.path.join(ckpt_path_to_load, PERCEPTION_NAME)\n",
    "            print(f\"Setting perception to {perception_path}\")\n",
    "            new_eval_args[\"checkpoints_perception\"] = perception_path\n",
    "        # set perception tokenizer\n",
    "        perception_tokenizer_path = os.path.join(\n",
    "            ckpt_path_to_load, PERCEPTION_TOKENIZER_NAME\n",
    "        )\n",
    "        # in the case, where we use frozen perception tokenizer\n",
    "        # we assume \"checkpoints_perception_tokenizer\" is already set\n",
    "        if not model_args.freeze_tokenizer:\n",
    "            new_eval_args[\"checkpoints_perception_tokenizer\"] = (\n",
    "                perception_tokenizer_path\n",
    "            )\n",
    "        print(\n",
    "            f\"Setting perception tokenizer path to {new_eval_args['checkpoints_perception_tokenizer']}\"\n",
    "        )\n",
    "        if not model_args.freeze_lm:\n",
    "            new_eval_args[\"checkpoints_lm\"] = ckpt_path_to_load\n",
    "            print(f\"Setting lm checkpoint path to {new_eval_args['checkpoints_lm']}\")\n",
    "        # Set LORA if needed\n",
    "        if (\"lora\" in new_eval_args) and new_eval_args[\"lora\"]:\n",
    "            new_eval_args[\"lora_checkpoint\"] = ckpt_path_to_load\n",
    "            if model_args.integrity_classifier:\n",
    "                new_eval_args[\"lora_checkpoint\"] = os.path.join(\n",
    "                    ckpt_path_to_load, \"adapter\"\n",
    "                )\n",
    "            print(\n",
    "                f\"Setting LORA checkpoint path to {new_eval_args['lora_checkpoint'] }\"\n",
    "            )\n",
    "\n",
    "        if (\"lora_tokenizer\" in new_eval_args) and new_eval_args[\"lora_tokenizer\"]:\n",
    "            new_eval_args[\"lora_tokenizer_checkpoint\"] = os.path.join(\n",
    "                ckpt_path_to_load, \"adapter_tokenizer\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Setting LORA tokenizer checkpoint path to {new_eval_args['lora_tokenizer_checkpoint'] }\"\n",
    "            )\n",
    "\n",
    "        if (\"lora_perception\" in new_eval_args) and new_eval_args[\"lora_perception\"]:\n",
    "            new_eval_args[\"lora_perception_checkpoint\"] = os.path.join(\n",
    "                ckpt_path_to_load, \"adapter_perception\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Setting LORA perception checkpoint path to {new_eval_args['lora_perception_checkpoint'] }\"\n",
    "            )\n",
    "        if model_args.add_grounding_tokens:\n",
    "            # load embedding & head\n",
    "            if model_args.use_metaformers:\n",
    "                lm_embedding_path = ckpt_path_to_load\n",
    "                lm_head_path = ckpt_path_to_load\n",
    "            else:\n",
    "                lm_embedding_path = os.path.join(ckpt_path_to_load, LM_EMBEDDING_NAME)\n",
    "                lm_head_path = os.path.join(ckpt_path_to_load, LM_HEAD_NAME)\n",
    "            print(f\"Setting checkpoints_lm_embedding to {lm_embedding_path}\")\n",
    "            print(f\"Setting checkpoints_lm_head to {lm_head_path}\")\n",
    "            new_eval_args[\"checkpoints_lm_embedding\"] = lm_embedding_path\n",
    "            new_eval_args[\"checkpoints_lm_head\"] = (\n",
    "                lm_head_path if model_args.add_grounding_llm_head_if_available else None\n",
    "            )\n",
    "        print(f\"eval args after sanitization: {new_eval_args}\")\n",
    "        # Set the new eval args in job runner\n",
    "        job_runner_args.eval_args = new_eval_args\n",
    "    return job_runner_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm_R2/checkpoint-1000\"\n",
    "# path = f\"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/checkpoint-1000/training_args.bin\"\n",
    "checkpoint_path = \"/fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp32a_lr1_n128/checkpoint-8000\"\n",
    "with open(f\"{checkpoint_path}/training_args.bin\", \"rb\") as fd:\n",
    "    training_args = torch.load(fd, weights_only=False)\n",
    "    \n",
    "model_args = training_args.model_args\n",
    "model_args = model_args.to_dict()\n",
    "training_args = training_args.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name_or_path : /fsx_0/checkpoints/llama3/mh21\n",
      "tokenizer_path : /fsx_0/checkpoints/llama3/mh21\n",
      "use_metaformers : True\n",
      "recompute_attn : True\n",
      "recompute_fc1_fc3 : True\n",
      "pretraining_tp : 1\n",
      "model_parallel_size : 8\n",
      "context_parallel_size : 1\n",
      "pipeline_parallel_size : 1\n",
      "virtual_pipeline_parallel_size : 1\n",
      "use_alternate_pipeline_parallel_config : True\n",
      "less_layer_first_pp_stage : 0\n",
      "less_layer_last_pp_stage : 0\n",
      "use_te : True\n",
      "parallelize_llm : True\n",
      "parallelize_perception_tokenizer : True\n",
      "use_sdpa : True\n",
      "use_fused_layernorm : True\n",
      "max_parallel_model_loading : 8\n",
      "model_type : None\n",
      "lm_mh_tokenizer_version : tiktoken_v5\n",
      "ensemble_tokenizer_params : {}\n",
      "use_ensemble_modality_tokenizer : False\n",
      "modality_tokenizer_name : /fsx_0/user/zhenq/checkpoints/huggingface/ks336_29_38907_I336B160E327m_e3\n",
      "use_fast_tokenizer : True\n",
      "n_prefix_embs : 65\n",
      "num_classes : None\n",
      "routing_classifier : False\n",
      "integrity_classifier : False\n",
      "freeze_perception : False\n",
      "freeze_perception_except_pos_emb : False\n",
      "ignore_mismatched_sizes : False\n",
      "freeze_tokenizer : False\n",
      "freeze_lm : True\n",
      "lm_bits : -1\n",
      "checkpoints_lm : None\n",
      "perception_tokenizer_name : None\n",
      "checkpoints_perception : /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_m2c2_036_unfreeze/checkpoint-7300/perception.pt\n",
      "checkpoints_perception_tokenizer : /fsx_0/checkpoints/mm10/MM10-Stage2-70B/MH21_70B_336px_exp30d_m2c2_036_unfreeze/checkpoint-7300/perception_tokenizer.pt\n",
      "checkpoints_logit_scale : None\n",
      "checkpoints_logit_bias : None\n",
      "tokenizer_type : PerceiverV3\n",
      "modality : image\n",
      "mixin_modality : image\n",
      "llm_dropout : 0.0\n",
      "lora : False\n",
      "lora_rank : 8\n",
      "lora_dropout : 0.1\n",
      "lora_alpha : 32\n",
      "lora_checkpoint : None\n",
      "lora_target_modules : ['q_proj', 'v_proj']\n",
      "lora_tokenizer : False\n",
      "lora_tokenizer_rank : 8\n",
      "lora_tokenizer_dropout : 0.1\n",
      "lora_tokenizer_alpha : 32\n",
      "lora_tokenizer_checkpoint : None\n",
      "lora_tokenizer_target_modules : ['to_q', 'to_kv']\n",
      "lora_perception : False\n",
      "lora_perception_rank : 8\n",
      "lora_perception_dropout : 0.1\n",
      "lora_perception_alpha : 32\n",
      "lora_perception_checkpoint : None\n",
      "lora_perception_target_modules : ['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\n",
      "speech_adapter_type : 0\n",
      "num_media_embeds : 4\n",
      "perception_tokenizer_num_layers : 22\n",
      "perception_tokenizer_project_in_dim : None\n",
      "perception_tokenizer_project_out_from_dim : None\n",
      "perception_tokenizer_attention_dropout_p : 0.0\n",
      "perception_tokenizer_hidden_dropout_p : 0.0\n",
      "perception_tokenizer_ensemble_type : pool\n",
      "custom_FSDP : True\n",
      "device_map : cpu\n",
      "load_in_8bit : False\n",
      "low_cpu_mem_usage : True\n",
      "add_bos_token : False\n",
      "add_eos_token : False\n",
      "manifold_bucket : None\n",
      "perception_output_processor : layer\n",
      "keep_vision_hidden_states : False\n",
      "vision_hidden_state_layer : -2\n",
      "gradient_checkpointing_llm : True\n",
      "layer_ckpt : none\n",
      "gradient_checkpointing_perception_tokenizer : True\n",
      "gradient_checkpointing_perception : True\n",
      "clip_patch_dropout : None\n",
      "center_crop : False\n",
      "clip_loss_weight : None\n",
      "clip_logit_scale : 2.6592\n",
      "clip_n_registers : None\n",
      "clip_enable_mi : False\n",
      "debug_logs : False\n",
      "llama_seq_cls_dropout_p : None\n",
      "add_grounding_tokens : False\n",
      "grounding_bin_size : 42\n",
      "grounding_image_size : 336\n",
      "add_grounding_llm_head_if_available : True\n",
      "checkpoints_lm_embedding : None\n",
      "checkpoints_lm_head : None\n",
      "checkpoints_lm_cls_head : None\n",
      "perceiver_num_heads : 32\n",
      "perceiver_dim_override : 4096\n",
      "perceiver_num_kv_heads : 8\n",
      "perceiver_num_latents : 64\n",
      "perceiver_source_num_latents : 64\n",
      "perceiver_ff_mult : 4\n",
      "perceiver_use_pos_embs : True\n",
      "perceiver_pos_emb_type : learned\n",
      "perceiver_enable_query_aware : False\n",
      "perceiver_add_output_norm : True\n",
      "perceiver_add_output_norm_scaler : False\n",
      "clip_num_embeddings : 577\n",
      "encoder_frame_stride : None\n",
      "mi_image_encoder_frame_stride : None\n",
      "encoder_num_frames : None\n",
      "encoder_use_predictor : True\n",
      "perceiver_cat_latents : True\n",
      "perceiver_collapse_chunks : False\n",
      "perceiver_enable_mi : False\n",
      "perceiver_max_img_frames : 16\n",
      "block_sparse : True\n",
      "perceiver_num_frames_per_chunk : 1\n",
      "perceiver_add_eof : True\n",
      "perceiver_collapse_frame_chunks : False\n",
      "perceiver_dynamic_frame_resizing : True\n",
      "perceiver_enable_latent_compression : False\n",
      "perceiver_interleaved_outputs : False\n",
      "perceiver_collapse_modalities : False\n",
      "replace_llama_attention : True\n",
      "ensemble_meta_perceiver_option : None\n",
      "ensemble_freeze_modality : None\n",
      "enable_multi_transformer_blocks : False\n",
      "n_multi_transformer_blocks : 35\n",
      "n_layers_per_multi_transformer_block : 2\n",
      "enable_modality_aggregator : False\n",
      "modality_aggregator_chunk_size : 1\n",
      "modality_aggregator_split_dim : 1\n",
      "modality_aggregator_merge_dim : 1\n",
      "perceiver_enable_moe : False\n",
      "perceiver_num_experts : 8\n",
      "perceiver_num_activated_experts : 2\n",
      "use_scaled_rope : False\n",
      "rope_scale_factor : 8\n",
      "high_freq_factor : 4\n",
      "low_freq_factor : 1\n",
      "embedding_visualization_layers : None\n",
      "enforce_uniform_emb_variance : True\n",
      "clip_interpolation_factor : 1\n"
     ]
    }
   ],
   "source": [
    "for k, v in model_args.items():\n",
    "    print(k, \":\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['output_dir', 'overwrite_output_dir', 'do_train', 'do_eval', 'do_predict', 'eval_strategy', 'prediction_loss_only', 'per_device_train_batch_size', 'per_device_eval_batch_size', 'per_gpu_train_batch_size', 'per_gpu_eval_batch_size', 'gradient_accumulation_steps', 'eval_accumulation_steps', 'eval_delay', 'learning_rate', 'weight_decay', 'adam_beta1', 'adam_beta2', 'adam_epsilon', 'max_grad_norm', 'num_train_epochs', 'max_steps', 'lr_scheduler_type', 'lr_scheduler_kwargs', 'warmup_ratio', 'warmup_steps', 'log_level', 'log_level_replica', 'log_on_each_node', 'logging_dir', 'logging_strategy', 'logging_first_step', 'logging_steps', 'logging_nan_inf_filter', 'save_strategy', 'save_steps', 'save_total_limit', 'save_safetensors', 'save_on_each_node', 'save_only_model', 'restore_callback_states_from_checkpoint', 'no_cuda', 'use_cpu', 'use_mps_device', 'seed', 'data_seed', 'jit_mode_eval', 'use_ipex', 'bf16', 'fp16', 'fp16_opt_level', 'half_precision_backend', 'bf16_full_eval', 'fp16_full_eval', 'tf32', 'local_rank', 'ddp_backend', 'tpu_num_cores', 'tpu_metrics_debug', 'debug', 'dataloader_drop_last', 'eval_steps', 'dataloader_num_workers', 'dataloader_prefetch_factor', 'past_index', 'run_name', 'disable_tqdm', 'remove_unused_columns', 'label_names', 'load_best_model_at_end', 'metric_for_best_model', 'greater_is_better', 'ignore_data_skip', 'fsdp', 'fsdp_min_num_params', 'fsdp_config', 'fsdp_transformer_layer_cls_to_wrap', 'accelerator_config', 'deepspeed', 'label_smoothing_factor', 'optim', 'optim_args', 'adafactor', 'group_by_length', 'length_column_name', 'report_to', 'ddp_find_unused_parameters', 'ddp_bucket_cap_mb', 'ddp_broadcast_buffers', 'dataloader_pin_memory', 'dataloader_persistent_workers', 'skip_memory_metrics', 'use_legacy_prediction_loop', 'push_to_hub', 'resume_from_checkpoint', 'hub_model_id', 'hub_strategy', 'hub_token', 'hub_private_repo', 'hub_always_push', 'gradient_checkpointing', 'gradient_checkpointing_kwargs', 'include_inputs_for_metrics', 'eval_do_concat_batches', 'fp16_backend', 'evaluation_strategy', 'push_to_hub_model_id', 'push_to_hub_organization', 'push_to_hub_token', 'mp_parameters', 'auto_find_batch_size', 'full_determinism', 'torchdynamo', 'ray_scope', 'ddp_timeout', 'torch_compile', 'torch_compile_backend', 'torch_compile_mode', 'dispatch_batches', 'split_batches', 'include_tokens_per_second', 'include_num_input_tokens_seen', 'neftune_noise_alpha', 'optim_target_modules', 'batch_eval_metrics', 'save_top_k', 'load_latest_k', 'checkpoints_optimizer', 'checkpoints_scheduler', 'dpo_alpha', 'dpo_beta', 'mdpo_delta', 'mdpo_blackout_fraction', 'mdpo_copo_weight', 'mdpo_anchor_weight', 'dpo_reference_free', 'dpo_label_smoothing', 'dpo_chosen_nll_factor', 'ipo', 'simpo_beta', 'simpo_gamma', 'mixin_gamma', 'load_best_model', 'clip_head_weight_decay', 'lm_text_lr_scale', 'perception_lr_scale', 'perception_tokenizer_lr_scale', 'norm_loss_alpha', 'tb_logdir', 'eval_ckpt', 'eval_type', 'cosine_decay_to', 'using_fsdp2', 'wandb_project_name', 'wandb_watch'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args['using_fsdp2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Input config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['scheduler_type', 'eval_only', 'num_gpus', 'num_nodes', 'eval_args'])\n",
      "{'add_bos_token': False,\n",
      " 'add_eos_token': False,\n",
      " 'batch_size_generation': 2,\n",
      " 'dataloader_num_workers': 4,\n",
      " 'eval_ckpt': 2520,\n",
      " 'eval_only': True,\n",
      " 'eval_type': 'eval_mmmu',\n",
      " 'generation_task': 'MMMUGenerationTask',\n",
      " 'instr_prompt': 'mmmu',\n",
      " 'max_length': 50,\n",
      " 'max_new_tokens': 1024,\n",
      " 'max_seq_len': 1024,\n",
      " 'model_parallel_size': 8,\n",
      " 'num_beams': 1,\n",
      " 'perception_tokenizer_attention_dropout_p': 0,\n",
      " 'perception_tokenizer_hidden_dropout_p': 0,\n",
      " 'repetition_penalty': 1.05,\n",
      " 'resume_from_checkpoint': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/checkpoint-1000',\n",
      " 'stopping_token_ids': '',\n",
      " 'task_type': 'instruction_tune',\n",
      " 'tb_logdir': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/tensorboard',\n",
      " 'train_file': '/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl',\n",
      " 'validation_file': '/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl'}\n"
     ]
    }
   ],
   "source": [
    "eval_config_file = \"/fsx_0/user/tranx/experiments/eval/sft_eval_overwrite_aws/mmmu.json\"\n",
    "with open(eval_config_file, 'r') as f:\n",
    "    args = json.load(f)\n",
    "    \n",
    "print(args.keys())\n",
    "\n",
    "OUTPUT_DIR=\"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a\"\n",
    "CHECKPOINT_ID = 1000\n",
    "BENCHMARK = \"mmmu\"\n",
    "args[\"eval_args\"][\"resume_from_checkpoint\"] = f\"{OUTPUT_DIR}/checkpoint-{CHECKPOINT_ID}\"\n",
    "args[\"eval_args\"][\"tb_logdir\"] = f\"{OUTPUT_DIR}/tensorboard\"\n",
    "pprint(args['eval_args'])\n",
    "\n",
    "OUTPUT_RESULT = f\"{OUTPUT_DIR}/evals/eval_results_checkpoint-{CHECKPOINT_ID}/{BENCHMARK}_eval_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_config_file = \"/fsx_0/user/tranx/experiments/eval/sft_eval_overwrite_aws/mmmu.json\"\n",
    "eval_config_file = \"/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws/mm10/stage1/eval_overwrite/eval_mmmu.json\"\n",
    "with open(eval_config_file, 'r') as f:\n",
    "    args = json.load(f)\n",
    "    \n",
    "OUTPUT_RESULT = \"/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/evals/eval_results_checkpoint-1000/mmmu_eval_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_eval_job_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_args, data_args, generation_args, training_args \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_eval_job_args\u001b[49m(args)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_eval_job_args' is not defined"
     ]
    }
   ],
   "source": [
    "model_args, data_args, generation_args, training_args = create_eval_job_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/evaluation'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_args.generation_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Shim Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test flow on model ckpt /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/checkpoint-1000\n",
      "Reading training arguments from file /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/checkpoint-1000/training_args.bin\n",
      "eval args: {'output_dir': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-06, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/runs/Sep05_19-33-41_h100-st-p548xlarge-117', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2023, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': '$name', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': False, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock', 'LoraLinear'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/checkpoint-1000', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': 10.0, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': None, 'checkpoints_scheduler': None, 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/tensorboard', 'eval_ckpt': 2520, 'eval_type': 'eval_mmmu', 'cosine_decay_to': 0.1, 'using_fsdp2': False, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'activated_num_image_chunks': None, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'frames_per_clip': 16, 'clips_per_video': 1, 'frame_dilation': 4, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh19', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh19', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': False, 'parallelize_llm': True, 'parallelize_perception_tokenizer': False, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'meta_ai_tiktoken_v4', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/checkpoints/clip/MetaCLIP-BigG-336-0712', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': True, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': True, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': None, 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30/checkpoint-8800/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'lora': True, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj', 'k_proj', 'o_proj', 'ffn']\", 'lora_tokenizer': True, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_k', 'to_v', 'to_o', 'ff.w12', 'ff.w3']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'all', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': False, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': False, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'replace_llama_attention': True, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 35, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': False, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl', 'validation_file': '/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl', 'max_length': 50, 'max_seq_len': 1024, 'max_new_tokens': 1024, 'batch_size_generation': 2, 'instr_prompt': 'mmmu', 'task_type': 'instruction_tune', 'generation_task': 'MMMUGenerationTask', 'eval_only': True, 'stopping_token_ids': '', 'repetition_penalty': 1.05}\n",
      "Setting perception tokenizer path to /fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30/checkpoint-8800/perception_tokenizer.pt\n",
      "Setting LORA checkpoint path to /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/checkpoint-1000\n",
      "Setting LORA tokenizer checkpoint path to /fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/checkpoint-1000/adapter_tokenizer\n",
      "eval args after sanitization: {'output_dir': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 48, 'gradient_accumulation_steps': 1, 'eval_delay': 0, 'learning_rate': 5e-06, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/runs/Sep05_19-33-41_h100-st-p548xlarge-117', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2023, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'past_index': -1, 'run_name': '$name', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': False, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': False, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/checkpoint-1000', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': 10.0, 'batch_eval_metrics': False, 'eval_on_start': False, 'save_top_k': 1, 'load_latest_k': 1, 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/tensorboard', 'eval_ckpt': 2520, 'eval_type': 'eval_mmmu', 'cosine_decay_to': 0.1, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 336, 'num_image_chunks': 27, 'max_total_image_chunks': 81, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'frames_per_clip': 16, 'clips_per_video': 1, 'frame_dilation': 4, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh19', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh19', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': False, 'parallelize_llm': True, 'parallelize_perception_tokenizer': False, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'meta_ai_tiktoken_v4', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/checkpoints/clip/MetaCLIP-BigG-336-0712', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': True, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': True, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30/checkpoint-8800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'lora': True, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj', 'k_proj', 'o_proj', 'ffn']\", 'lora_tokenizer': True, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_k', 'to_v', 'to_o', 'ff.w12', 'ff.w3']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'all', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': False, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': False, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 35, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'enforce_uniform_emb_variance': False, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl', 'validation_file': '/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl', 'max_length': 50, 'max_seq_len': 1024, 'max_new_tokens': 1024, 'batch_size_generation': 2, 'instr_prompt': 'mmmu', 'task_type': 'instruction_tune', 'generation_task': 'MMMUGenerationTask', 'eval_only': True, 'stopping_token_ids': '', 'repetition_penalty': 1.05, 'lora_checkpoint': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/checkpoint-1000', 'lora_tokenizer_checkpoint': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/checkpoint-1000/adapter_tokenizer'}\n",
      "JobRunnerArgs(trainer_args=None,\n",
      "              eval_args={'adafactor': False,\n",
      "                         'adam_beta1': 0.9,\n",
      "                         'adam_beta2': 0.95,\n",
      "                         'adam_epsilon': 1e-08,\n",
      "                         'add_bos_token': False,\n",
      "                         'add_eos_token': False,\n",
      "                         'add_grounding_llm_head_if_available': True,\n",
      "                         'add_grounding_tokens': False,\n",
      "                         'add_resized_image': True,\n",
      "                         'auto_find_batch_size': False,\n",
      "                         'batch_eval_metrics': False,\n",
      "                         'batch_size_generation': 2,\n",
      "                         'bf16': True,\n",
      "                         'bf16_full_eval': False,\n",
      "                         'block_sparse': True,\n",
      "                         'center_crop': False,\n",
      "                         'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/tranx/MM9-Stage2-70B/MH19_336px_128nodes_exp30/checkpoint-8800/perception_tokenizer.pt',\n",
      "                         'chunk_size': 336,\n",
      "                         'clip_enable_mi': False,\n",
      "                         'clip_logit_scale': 2.6592,\n",
      "                         'clip_num_embeddings': 577,\n",
      "                         'clips_per_video': 1,\n",
      "                         'context_parallel_size': 1,\n",
      "                         'cosine_decay_to': 0.1,\n",
      "                         'custom_FSDP': True,\n",
      "                         'dataloader_drop_last': False,\n",
      "                         'dataloader_num_workers': 4,\n",
      "                         'dataloader_persistent_workers': False,\n",
      "                         'dataloader_pin_memory': False,\n",
      "                         'ddp_timeout': 1800,\n",
      "                         'debug_logs': False,\n",
      "                         'device_map': 'cpu',\n",
      "                         'disable_tqdm': False,\n",
      "                         'do_eval': False,\n",
      "                         'do_predict': False,\n",
      "                         'do_train': True,\n",
      "                         'dpo_alpha': 0.0,\n",
      "                         'dpo_beta': 0.1,\n",
      "                         'dpo_chosen_nll_factor': 0.2,\n",
      "                         'dpo_label_smoothing': 0.0,\n",
      "                         'dpo_reference_free': False,\n",
      "                         'enable_modality_aggregator': False,\n",
      "                         'enable_multi_transformer_blocks': False,\n",
      "                         'encoder_use_predictor': True,\n",
      "                         'enforce_uniform_emb_variance': False,\n",
      "                         'ensemble_tokenizer_params': '{}',\n",
      "                         'eval_ckpt': 2520,\n",
      "                         'eval_delay': 0,\n",
      "                         'eval_do_concat_batches': True,\n",
      "                         'eval_on_start': False,\n",
      "                         'eval_only': True,\n",
      "                         'eval_steps': 250.0,\n",
      "                         'eval_strategy': 'no',\n",
      "                         'eval_type': 'eval_mmmu',\n",
      "                         'evaluation_strategy': 'no',\n",
      "                         'fp16': False,\n",
      "                         'fp16_backend': 'auto',\n",
      "                         'fp16_full_eval': False,\n",
      "                         'fp16_opt_level': 'O1',\n",
      "                         'frame_dilation': 4,\n",
      "                         'frames_per_clip': 16,\n",
      "                         'freeze_lm': True,\n",
      "                         'freeze_perception': True,\n",
      "                         'freeze_perception_except_pos_emb': False,\n",
      "                         'freeze_tokenizer': True,\n",
      "                         'full_determinism': False,\n",
      "                         'generation_task': 'MMMUGenerationTask',\n",
      "                         'gradient_accumulation_steps': 1,\n",
      "                         'gradient_checkpointing': False,\n",
      "                         'gradient_checkpointing_llm': True,\n",
      "                         'gradient_checkpointing_perception': False,\n",
      "                         'gradient_checkpointing_perception_tokenizer': True,\n",
      "                         'greater_is_better': False,\n",
      "                         'grounding_bin_size': 42,\n",
      "                         'grounding_image_size': 336,\n",
      "                         'group_by_length': False,\n",
      "                         'half_precision_backend': 'auto',\n",
      "                         'high_freq_factor': 4,\n",
      "                         'hub_always_push': False,\n",
      "                         'hub_private_repo': False,\n",
      "                         'hub_strategy': 'every_save',\n",
      "                         'hub_token': '<HUB_TOKEN>',\n",
      "                         'ignore_data_skip': False,\n",
      "                         'ignore_mismatched_sizes': False,\n",
      "                         'include_inputs_for_metrics': False,\n",
      "                         'include_num_input_tokens_seen': False,\n",
      "                         'include_tokens_per_second': False,\n",
      "                         'instr_prompt': 'mmmu',\n",
      "                         'instruction_model_type': 'MetaAiTikTokv4ChatFormat',\n",
      "                         'integrity_classifier': False,\n",
      "                         'ipo': False,\n",
      "                         'jit_mode_eval': False,\n",
      "                         'keep_vision_hidden_states': False,\n",
      "                         'label_smoothing_factor': 0.0,\n",
      "                         'layer_ckpt': 'all',\n",
      "                         'learning_rate': 5e-06,\n",
      "                         'length_column_name': 'length',\n",
      "                         'less_layer_first_pp_stage': 0,\n",
      "                         'less_layer_last_pp_stage': 0,\n",
      "                         'lm_bits': -1,\n",
      "                         'lm_mh_tokenizer_version': 'meta_ai_tiktoken_v4',\n",
      "                         'lm_text_lr_scale': 1.0,\n",
      "                         'load_best_model': False,\n",
      "                         'load_best_model_at_end': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'load_latest_k': 1,\n",
      "                         'local_rank': 0,\n",
      "                         'log_level': 'info',\n",
      "                         'log_level_replica': 'info',\n",
      "                         'log_on_each_node': True,\n",
      "                         'logging_dir': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/runs/Sep05_19-33-41_h100-st-p548xlarge-117',\n",
      "                         'logging_first_step': True,\n",
      "                         'logging_nan_inf_filter': True,\n",
      "                         'logging_steps': 10,\n",
      "                         'logging_strategy': 'steps',\n",
      "                         'lora': True,\n",
      "                         'lora_alpha': 32,\n",
      "                         'lora_checkpoint': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/checkpoint-1000',\n",
      "                         'lora_dropout': 0.1,\n",
      "                         'lora_perception': False,\n",
      "                         'lora_perception_alpha': 32,\n",
      "                         'lora_perception_dropout': 0.1,\n",
      "                         'lora_perception_rank': 8,\n",
      "                         'lora_perception_target_modules': \"['k_proj', \"\n",
      "                                                           \"'v_proj', \"\n",
      "                                                           \"'q_proj', \"\n",
      "                                                           \"'out_proj', 'fc1', \"\n",
      "                                                           \"'fc2', \"\n",
      "                                                           \"'visual_projection']\",\n",
      "                         'lora_rank': 8,\n",
      "                         'lora_target_modules': \"['q_proj', 'v_proj', \"\n",
      "                                                \"'k_proj', 'o_proj', 'ffn']\",\n",
      "                         'lora_tokenizer': True,\n",
      "                         'lora_tokenizer_alpha': 32,\n",
      "                         'lora_tokenizer_checkpoint': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/checkpoint-1000/adapter_tokenizer',\n",
      "                         'lora_tokenizer_dropout': 0.1,\n",
      "                         'lora_tokenizer_rank': 8,\n",
      "                         'lora_tokenizer_target_modules': \"['to_q', 'to_k', \"\n",
      "                                                          \"'to_v', 'to_o', \"\n",
      "                                                          \"'ff.w12', 'ff.w3']\",\n",
      "                         'low_cpu_mem_usage': True,\n",
      "                         'low_freq_factor': 1,\n",
      "                         'lr_scheduler_type': 'cosine',\n",
      "                         'max_grad_norm': 1.0,\n",
      "                         'max_length': 50,\n",
      "                         'max_new_tokens': 1024,\n",
      "                         'max_parallel_model_loading': 8,\n",
      "                         'max_seq_len': 1024,\n",
      "                         'max_steps': -1,\n",
      "                         'max_total_image_chunks': 81,\n",
      "                         'mdpo_anchor_weight': 0.1,\n",
      "                         'mdpo_blackout_fraction': 0.2,\n",
      "                         'mdpo_copo_weight': 1.0,\n",
      "                         'mdpo_delta': 0.0,\n",
      "                         'metric_for_best_model': 'eval_loss',\n",
      "                         'mixin_gamma': 1.0,\n",
      "                         'mixin_modality': 'image',\n",
      "                         'modality': 'image',\n",
      "                         'modality_aggregator_chunk_size': 1,\n",
      "                         'modality_aggregator_merge_dim': 1,\n",
      "                         'modality_aggregator_split_dim': 1,\n",
      "                         'modality_tokenizer_name': '/fsx_0/checkpoints/clip/MetaCLIP-BigG-336-0712',\n",
      "                         'model_name_or_path': '/fsx_0/checkpoints/llama3/mh19',\n",
      "                         'model_parallel_size': 8,\n",
      "                         'mp_parameters': '',\n",
      "                         'n_layers_per_multi_transformer_block': 2,\n",
      "                         'n_multi_transformer_blocks': 35,\n",
      "                         'n_prefix_embs': 65,\n",
      "                         'neftune_noise_alpha': 10.0,\n",
      "                         'no_cuda': False,\n",
      "                         'norm_loss_alpha': 0.0,\n",
      "                         'num_beams': 1,\n",
      "                         'num_image_chunks': 27,\n",
      "                         'num_media_embeds': 4,\n",
      "                         'num_train_epochs': 3.0,\n",
      "                         'optim': 'adamw_torch',\n",
      "                         'output_dir': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a',\n",
      "                         'overwrite_output_dir': True,\n",
      "                         'parallelize_llm': True,\n",
      "                         'parallelize_perception_tokenizer': False,\n",
      "                         'past_index': -1,\n",
      "                         'per_device_eval_batch_size': 48,\n",
      "                         'per_device_train_batch_size': 8,\n",
      "                         'perceiver_add_eof': True,\n",
      "                         'perceiver_add_output_norm': False,\n",
      "                         'perceiver_add_output_norm_scaler': False,\n",
      "                         'perceiver_cat_latents': True,\n",
      "                         'perceiver_collapse_chunks': False,\n",
      "                         'perceiver_collapse_frame_chunks': False,\n",
      "                         'perceiver_collapse_modalities': False,\n",
      "                         'perceiver_dim_override': 4096,\n",
      "                         'perceiver_dynamic_frame_resizing': True,\n",
      "                         'perceiver_enable_latent_compression': False,\n",
      "                         'perceiver_enable_mi': False,\n",
      "                         'perceiver_enable_moe': False,\n",
      "                         'perceiver_enable_query_aware': False,\n",
      "                         'perceiver_ff_mult': 4,\n",
      "                         'perceiver_interleaved_outputs': False,\n",
      "                         'perceiver_max_img_frames': 16,\n",
      "                         'perceiver_num_activated_experts': 2,\n",
      "                         'perceiver_num_experts': 8,\n",
      "                         'perceiver_num_frames_per_chunk': 1,\n",
      "                         'perceiver_num_heads': 32,\n",
      "                         'perceiver_num_kv_heads': 8,\n",
      "                         'perceiver_num_latents': 64,\n",
      "                         'perceiver_pos_emb_type': 'learned',\n",
      "                         'perceiver_source_num_latents': 64,\n",
      "                         'perceiver_use_pos_embs': True,\n",
      "                         'perception_lr_scale': 1.0,\n",
      "                         'perception_output_processor': 'layer',\n",
      "                         'perception_tokenizer_attention_dropout_p': 0,\n",
      "                         'perception_tokenizer_ensemble_type': 'pool',\n",
      "                         'perception_tokenizer_hidden_dropout_p': 0,\n",
      "                         'perception_tokenizer_num_layers': 22,\n",
      "                         'pipeline_parallel_size': 1,\n",
      "                         'prediction_loss_only': False,\n",
      "                         'pretraining_tp': 1,\n",
      "                         'push_to_hub': False,\n",
      "                         'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>',\n",
      "                         'ray_scope': 'last',\n",
      "                         'recompute_attn': True,\n",
      "                         'recompute_fc1_fc3': True,\n",
      "                         'remove_unused_columns': False,\n",
      "                         'repetition_penalty': 1.05,\n",
      "                         'replace_llama_attention': True,\n",
      "                         'report_to': ['tensorboard'],\n",
      "                         'resize_longest': True,\n",
      "                         'restore_callback_states_from_checkpoint': False,\n",
      "                         'resume_from_checkpoint': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/checkpoint-1000',\n",
      "                         'rope_scale_factor': 8,\n",
      "                         'routing_classifier': False,\n",
      "                         'run_name': '$name',\n",
      "                         'save_on_each_node': False,\n",
      "                         'save_only_model': False,\n",
      "                         'save_safetensors': True,\n",
      "                         'save_steps': 100,\n",
      "                         'save_strategy': 'steps',\n",
      "                         'save_top_k': 1,\n",
      "                         'seed': 2023,\n",
      "                         'simpo_beta': 2.0,\n",
      "                         'simpo_gamma': 1.0,\n",
      "                         'skip_memory_metrics': True,\n",
      "                         'speech_adapter_type': 0,\n",
      "                         'stopping_token_ids': '',\n",
      "                         'task_type': 'instruction_tune',\n",
      "                         'tb_logdir': '/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/tensorboard',\n",
      "                         'tokenizer_path': '/fsx_0/checkpoints/llama3/mh19',\n",
      "                         'tokenizer_type': 'PerceiverV3',\n",
      "                         'torch_compile': False,\n",
      "                         'tpu_metrics_debug': False,\n",
      "                         'train_file': '/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl',\n",
      "                         'treat_frames_as_images': False,\n",
      "                         'use_alternate_pipeline_parallel_config': True,\n",
      "                         'use_cpu': False,\n",
      "                         'use_ensemble_modality_tokenizer': False,\n",
      "                         'use_fast_tokenizer': True,\n",
      "                         'use_fused_layernorm': True,\n",
      "                         'use_ipex': False,\n",
      "                         'use_legacy_prediction_loop': False,\n",
      "                         'use_metaformers': True,\n",
      "                         'use_mps_device': False,\n",
      "                         'use_scaled_rope': False,\n",
      "                         'use_sdpa': True,\n",
      "                         'use_te': False,\n",
      "                         'validation_file': '/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl',\n",
      "                         'virtual_pipeline_parallel_size': 1,\n",
      "                         'vision_hidden_state_layer': -2,\n",
      "                         'warmup_ratio': 0.0,\n",
      "                         'warmup_steps': 0,\n",
      "                         'weight_decay': 0.1},\n",
      "              fsdp_config={},\n",
      "              log_dir='',\n",
      "              is_mast=False,\n",
      "              eval_output_path='/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/evals/eval_results_checkpoint-1000/mmmu_eval_results.json')\n"
     ]
    }
   ],
   "source": [
    "job_runner_args = JobRunnerArgs(\n",
    "    trainer_args=None,\n",
    "    eval_args=args[\"eval_args\"],\n",
    "    fsdp_config={},\n",
    "    log_dir=\"\",\n",
    ")\n",
    "\n",
    "# must load trainer args from eval_args.resume_from_checkpoint\n",
    "job_runner_args = load_and_merge_trainer_to_eval_args(job_runner_args)\n",
    "\n",
    "# assign eval result output file    \n",
    "job_runner_args.eval_output_path = OUTPUT_RESULT\n",
    "pprint(job_runner_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_20240822_v2/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_20240822_v2/lib/python3.10/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_20240822_v2/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of  Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import HfArgumentParser\n",
    "\n",
    "from llm_mm_aligner.lib.configs import (\n",
    "    DataTrainingArguments,\n",
    "    GenerationArguments,\n",
    "    ModelArguments,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "args = job_runner_args \n",
    "\n",
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments, GenerationArguments, TrainingArguments)\n",
    ")\n",
    "(\n",
    "    model_args,\n",
    "    data_args,\n",
    "    generation_args,\n",
    "    training_args,\n",
    ") = parser.parse_args_into_dataclasses(\n",
    "    args=args.get_args_list(args.eval_args),\n",
    ")\n",
    "\n",
    "model_args = cast(ModelArguments, model_args)\n",
    "data_args = cast(DataTrainingArguments, data_args)\n",
    "generation_args = cast(GenerationArguments, generation_args)\n",
    "training_args = cast(TrainingArguments, training_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'fsdp_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfsdp_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fsdp_config'"
     ]
    }
   ],
   "source": [
    "args[\"fsdp_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataTrainingArguments(dataset_name='MMdataset',\n",
      "                      eval_dataset_name=None,\n",
      "                      train_file='/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl',\n",
      "                      mixin_file=None,\n",
      "                      validation_file='/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl',\n",
      "                      filetype='jsonlines',\n",
      "                      mixin_filetype='jsonlines',\n",
      "                      datarecipe_json_str=None,\n",
      "                      use_hive_dataset=False,\n",
      "                      use_mdl=False,\n",
      "                      hive_batch_size=64,\n",
      "                      hive_table_name=None,\n",
      "                      max_train_samples=None,\n",
      "                      max_seq_len=1024,\n",
      "                      max_tokens_in_batch_row=None,\n",
      "                      max_tokens_in_batch=None,\n",
      "                      max_eval_samples=None,\n",
      "                      validation_split_percentage=5,\n",
      "                      instructions='',\n",
      "                      instruction_model_type='MetaAiTikTokv4ChatFormat',\n",
      "                      instr_prompt='mmmu',\n",
      "                      eval_prompt=None,\n",
      "                      mixin_instr_prompt=None,\n",
      "                      instr_pool=None,\n",
      "                      ignore_questions_instr_prompt=False,\n",
      "                      preprocessing_num_workers=None,\n",
      "                      base_path=None,\n",
      "                      tot_frames=0,\n",
      "                      wd_data_path=None,\n",
      "                      wd_data_path_eval=None,\n",
      "                      wd_chunk_size=1000,\n",
      "                      shuffle=100,\n",
      "                      task_type='instruction_tune',\n",
      "                      mixin_task_type='captioning',\n",
      "                      left_padding=False,\n",
      "                      interleaved_seq_len=5,\n",
      "                      image_loader='default',\n",
      "                      video_loader='default',\n",
      "                      audio_loader='default',\n",
      "                      spectrogram_loader='default',\n",
      "                      speech_loader='default',\n",
      "                      imu_loader='default',\n",
      "                      eval_only=True,\n",
      "                      do_face_blur=False,\n",
      "                      use_face_detector=False,\n",
      "                      shortest_edge_resize=None,\n",
      "                      face_confidence_threshold=0.0,\n",
      "                      use_clip_face_blur=False,\n",
      "                      onbox_num_python_transform_workers=8,\n",
      "                      onbox_dpp_server_num_worker_threads=32,\n",
      "                      onbox_dpp_server_worker_buffer_size=512,\n",
      "                      onbox_dpp_client_num_prefetch_threads=8,\n",
      "                      onbox_dpp_client_prefetch_capacity=64,\n",
      "                      onbox_drop_incomplete=True,\n",
      "                      onbox_eval_num_python_transform_workers=1,\n",
      "                      onbox_eval_dpp_server_num_worker_threads=1,\n",
      "                      onbox_eval_dpp_server_worker_buffer_size=1,\n",
      "                      onbox_eval_dpp_client_num_prefetch_threads=1,\n",
      "                      onbox_eval_dpp_client_prefetch_capacity=1,\n",
      "                      dist_onbox_type='fullsync',\n",
      "                      hive_train_shuffle_all=False,\n",
      "                      do_random_horizontal_flip=False,\n",
      "                      do_random_vertical_flip=False,\n",
      "                      do_random_zoom_out=False,\n",
      "                      do_random_rotation=False,\n",
      "                      do_color_jitter=False,\n",
      "                      do_random_resized_crop=False,\n",
      "                      image_size_after_resized_crop=224,\n",
      "                      num_image_chunks=27,\n",
      "                      activated_num_image_chunks=None,\n",
      "                      max_total_image_chunks=81,\n",
      "                      chunk_size=336,\n",
      "                      add_resized_image=True,\n",
      "                      mixed_resolution_training=0.0,\n",
      "                      resize_longest=True,\n",
      "                      do_shuffle=True,\n",
      "                      captioning_force_eos=False,\n",
      "                      pad_to_full_batch=False,\n",
      "                      frames_per_clip=16,\n",
      "                      clips_per_video=1,\n",
      "                      clip_sampler_type='UNIFORM',\n",
      "                      clips_per_second=1,\n",
      "                      keyframe_extraction_method='optical_flow_sparse',\n",
      "                      keyframe_clip_model_path='manifold://sg_scene_ai/tree/llm_mm_aligner/clip-vit-large-patch14-336',\n",
      "                      frame_dilation=4,\n",
      "                      video_min_dimension=0,\n",
      "                      video_max_dimension=0,\n",
      "                      dynamic_frame_sampling=True,\n",
      "                      video_modality_prefix=None,\n",
      "                      convert_frame_timestamps_to_int=False,\n",
      "                      max_video_duration=None,\n",
      "                      sampling_fps=None,\n",
      "                      pad_frames_to_multiple_of=None,\n",
      "                      treat_frames_as_images=False,\n",
      "                      audio_duration_in_sec=None,\n",
      "                      attach_debugger=False,\n",
      "                      max_nos_key_frames=20,\n",
      "                      use_dense_optical_flow=False,\n",
      "                      dense_optical_flow_cuda=False,\n",
      "                      do_object_detection=False,\n",
      "                      object_detector_confidence_threshold=0.8,\n",
      "                      object_detector_model_path='manifold://sg_scene_ai/tree/data_curation/models/object_detection/detr-resnet-101/',\n",
      "                      broadcast_dataset_num_samples=512)\n"
     ]
    }
   ],
   "source": [
    "pprint(data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_num_params': 0,\n",
       " 'xla': False,\n",
       " 'xla_fsdp_v2': False,\n",
       " 'xla_fsdp_grad_ckpt': False}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.fsdp_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TrainingArguments' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtraining_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfsdp_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'TrainingArguments' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "training_args[\"fsdp_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(output_dir='/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a',\n",
      "                  overwrite_output_dir=True,\n",
      "                  do_train=True,\n",
      "                  do_eval=False,\n",
      "                  do_predict=False,\n",
      "                  eval_strategy=<IntervalStrategy.NO: 'no'>,\n",
      "                  prediction_loss_only=False,\n",
      "                  per_device_train_batch_size=8,\n",
      "                  per_device_eval_batch_size=48,\n",
      "                  per_gpu_train_batch_size=None,\n",
      "                  per_gpu_eval_batch_size=None,\n",
      "                  gradient_accumulation_steps=1,\n",
      "                  eval_accumulation_steps=None,\n",
      "                  eval_delay=0.0,\n",
      "                  learning_rate=5e-06,\n",
      "                  weight_decay=0.1,\n",
      "                  adam_beta1=0.9,\n",
      "                  adam_beta2=0.95,\n",
      "                  adam_epsilon=1e-08,\n",
      "                  max_grad_norm=1.0,\n",
      "                  num_train_epochs=3.0,\n",
      "                  max_steps=-1,\n",
      "                  lr_scheduler_type=<SchedulerType.COSINE: 'cosine'>,\n",
      "                  lr_scheduler_kwargs={},\n",
      "                  warmup_ratio=0.0,\n",
      "                  warmup_steps=0,\n",
      "                  log_level='info',\n",
      "                  log_level_replica='info',\n",
      "                  log_on_each_node=True,\n",
      "                  logging_dir='/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/runs/Sep05_19-33-41_h100-st-p548xlarge-117',\n",
      "                  logging_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
      "                  logging_first_step=True,\n",
      "                  logging_steps=10,\n",
      "                  logging_nan_inf_filter=True,\n",
      "                  save_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
      "                  save_steps=100,\n",
      "                  save_total_limit=None,\n",
      "                  save_safetensors=True,\n",
      "                  save_on_each_node=False,\n",
      "                  save_only_model=False,\n",
      "                  restore_callback_states_from_checkpoint=False,\n",
      "                  no_cuda=False,\n",
      "                  use_cpu=False,\n",
      "                  use_mps_device=False,\n",
      "                  seed=2023,\n",
      "                  data_seed=None,\n",
      "                  jit_mode_eval=False,\n",
      "                  use_ipex=False,\n",
      "                  bf16=True,\n",
      "                  fp16=False,\n",
      "                  fp16_opt_level='O1',\n",
      "                  half_precision_backend='auto',\n",
      "                  bf16_full_eval=False,\n",
      "                  fp16_full_eval=False,\n",
      "                  tf32=None,\n",
      "                  local_rank=0,\n",
      "                  ddp_backend=None,\n",
      "                  tpu_num_cores=None,\n",
      "                  tpu_metrics_debug=False,\n",
      "                  debug=[],\n",
      "                  dataloader_drop_last=False,\n",
      "                  eval_steps=250.0,\n",
      "                  dataloader_num_workers=4,\n",
      "                  dataloader_prefetch_factor=None,\n",
      "                  past_index=-1,\n",
      "                  run_name='$name',\n",
      "                  disable_tqdm=False,\n",
      "                  remove_unused_columns=False,\n",
      "                  label_names=None,\n",
      "                  load_best_model_at_end=False,\n",
      "                  metric_for_best_model='eval_loss',\n",
      "                  greater_is_better=False,\n",
      "                  ignore_data_skip=False,\n",
      "                  fsdp=[],\n",
      "                  fsdp_min_num_params=0,\n",
      "                  fsdp_config={'min_num_params': 0,\n",
      "                               'xla': False,\n",
      "                               'xla_fsdp_grad_ckpt': False,\n",
      "                               'xla_fsdp_v2': False},\n",
      "                  fsdp_transformer_layer_cls_to_wrap=None,\n",
      "                  accelerator_config=AcceleratorConfig(split_batches=False,\n",
      "                                                       dispatch_batches=None,\n",
      "                                                       even_batches=True,\n",
      "                                                       use_seedable_sampler=True,\n",
      "                                                       non_blocking=False,\n",
      "                                                       gradient_accumulation_kwargs=None,\n",
      "                                                       use_configured_state=False),\n",
      "                  deepspeed=None,\n",
      "                  label_smoothing_factor=0.0,\n",
      "                  optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>,\n",
      "                  optim_args=None,\n",
      "                  adafactor=False,\n",
      "                  group_by_length=False,\n",
      "                  length_column_name='length',\n",
      "                  report_to=[\"['tensorboard']\"],\n",
      "                  ddp_find_unused_parameters=None,\n",
      "                  ddp_bucket_cap_mb=None,\n",
      "                  ddp_broadcast_buffers=None,\n",
      "                  dataloader_pin_memory=False,\n",
      "                  dataloader_persistent_workers=False,\n",
      "                  skip_memory_metrics=True,\n",
      "                  use_legacy_prediction_loop=False,\n",
      "                  push_to_hub=False,\n",
      "                  resume_from_checkpoint='/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/checkpoint-1000',\n",
      "                  hub_model_id=None,\n",
      "                  hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>,\n",
      "                  hub_token='<PUSH_TO_HUB_TOKEN>',\n",
      "                  hub_private_repo=False,\n",
      "                  hub_always_push=False,\n",
      "                  gradient_checkpointing=False,\n",
      "                  gradient_checkpointing_kwargs=None,\n",
      "                  include_inputs_for_metrics=False,\n",
      "                  eval_do_concat_batches=True,\n",
      "                  fp16_backend='auto',\n",
      "                  evaluation_strategy='no',\n",
      "                  push_to_hub_model_id=None,\n",
      "                  push_to_hub_organization=None,\n",
      "                  push_to_hub_token='<PUSH_TO_HUB_TOKEN>',\n",
      "                  mp_parameters='',\n",
      "                  auto_find_batch_size=False,\n",
      "                  full_determinism=False,\n",
      "                  torchdynamo=None,\n",
      "                  ray_scope='last',\n",
      "                  ddp_timeout=1800,\n",
      "                  torch_compile=False,\n",
      "                  torch_compile_backend=None,\n",
      "                  torch_compile_mode=None,\n",
      "                  dispatch_batches=None,\n",
      "                  split_batches=None,\n",
      "                  include_tokens_per_second=False,\n",
      "                  include_num_input_tokens_seen=False,\n",
      "                  neftune_noise_alpha=10.0,\n",
      "                  optim_target_modules=None,\n",
      "                  batch_eval_metrics=False,\n",
      "                  eval_on_start=False,\n",
      "                  save_top_k=1,\n",
      "                  load_latest_k=1,\n",
      "                  checkpoints_optimizer=None,\n",
      "                  checkpoints_scheduler=None,\n",
      "                  dpo_alpha=0.0,\n",
      "                  dpo_beta=0.1,\n",
      "                  mdpo_delta=0.0,\n",
      "                  mdpo_blackout_fraction=0.2,\n",
      "                  mdpo_copo_weight=1.0,\n",
      "                  mdpo_anchor_weight=0.1,\n",
      "                  dpo_reference_free=False,\n",
      "                  dpo_label_smoothing=0.0,\n",
      "                  dpo_chosen_nll_factor=0.2,\n",
      "                  ipo=False,\n",
      "                  simpo_beta=2.0,\n",
      "                  simpo_gamma=1.0,\n",
      "                  mixin_gamma=1.0,\n",
      "                  load_best_model=False,\n",
      "                  clip_head_weight_decay=None,\n",
      "                  lm_text_lr_scale=1.0,\n",
      "                  perception_lr_scale=1.0,\n",
      "                  norm_loss_alpha=0.0,\n",
      "                  tb_logdir='/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/tensorboard',\n",
      "                  eval_ckpt=2520,\n",
      "                  eval_type='eval_mmmu',\n",
      "                  cosine_decay_to=0.1,\n",
      "                  using_fsdp2=False)\n"
     ]
    }
   ],
   "source": [
    "pprint(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args.generation_output_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fsx_0/checkpoints/tranx/MM9-SFT-70B/MH19_336px_8800_BabyLora_r26a/evals/eval_results_checkpoint-1000/mmmu_eval_results.json'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.eval_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fb dependencies not available, skipping init_aix\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllm_mm_aligner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m main \u001b[38;5;28;01mas\u001b[39;00m llm_mm_aligner_evaluate\n\u001b[0;32m----> 3\u001b[0m eval_metric \u001b[38;5;241m=\u001b[39m \u001b[43mllm_mm_aligner_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_output_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aligner_20240822_v2/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/evaluate.py:663\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model_args, data_args, training_args, generation_args, output_path)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# See all possible arguments in src/transformers/training_args.py\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# or by passing the --help flag to this script.\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# We now keep distinct sets of args, for a cleaner separation of concerns.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m training_args\u001b[38;5;241m.\u001b[39mdata_args \u001b[38;5;241m=\u001b[39m data_args\n\u001b[0;32m--> 663\u001b[0m \u001b[43minitialize_model_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_model_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_parallel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_model_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvirtual_pipeline_model_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_model_parallel_split_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_fp8\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43malternate_pp_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_parallel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackground_nccl_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshared_kv_head_group_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_fsdp2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musing_fsdp2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    676\u001b[0m set_seed(\u001b[38;5;241m2023\u001b[39m \u001b[38;5;241m+\u001b[39m get_pipeline_model_parallel_rank())\n\u001b[1;32m    677\u001b[0m model_parallel_cuda_manual_seed(\u001b[38;5;241m2023\u001b[39m \u001b[38;5;241m+\u001b[39m get_pipeline_model_parallel_rank())\n",
      "File \u001b[0;32m/fsx_0/user/tranx/rsync/llm_mm_aligner/lib/mpu.py:198\u001b[0m, in \u001b[0;36minitialize_model_parallel\u001b[0;34m(tensor_model_parallel_size, pipeline_model_parallel_size, virtual_pipeline_model_parallel_size, pipeline_model_parallel_split_rank, use_fp8, alternate_pp_config, context_parallel_size, background_nccl_init, shared_kv_head_group_size, using_fsdp2)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize model data parallel groups.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    tensor_model_parallel_size (int, default = 1):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03mranks 8 to 15 belong to the second box.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Get world size and rank. Ensure some consistencies.\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_initialized()\n\u001b[1;32m    199\u001b[0m world_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mget_world_size()\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    202\u001b[0m     world_size\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    209\u001b[0m ):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llm_mm_aligner.evaluate import main as llm_mm_aligner_evaluate\n",
    "\n",
    "eval_metric = llm_mm_aligner_evaluate(\n",
    "    model_args,\n",
    "    data_args,\n",
    "    training_args,\n",
    "    generation_args,\n",
    "    args.eval_output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overwrite both eval and training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config_file = \"/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm_R2/evals/eval_results_checkpoint-2800/mmmu_2800_eval_config.json\"\n",
    "with open(eval_config_file, 'r') as f:\n",
    "    input_arguments = json.load(f)\n",
    "    \n",
    "    \n",
    "input_arguments[\"eval_args\"].update({\n",
    "    \"instruction_model_type\": \"MetaAiTikTokv4ChatFormat\",\n",
    "    \"lm_mh_tokenizer_version\": \"meta_ai_tiktoken_v4\"\n",
    "    \n",
    "})\n",
    "# input_arguments[\"eval_args\"][\"using_fsdp2\"] = False\n",
    "# input_arguments['eval_args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'instruction_model_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minput_arguments\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_args\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstruction_model_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'instruction_model_type'"
     ]
    }
   ],
   "source": [
    "input_arguments[\"eval_args\"][\"instruction_model_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v4\n",
    "\"instruction_model_type\": \"MetaAiTikTokv4ChatFormat\"\n",
    "lm_mh_tokenizer_version: Optional[str] = field( \n",
    "    default=\"meta_ai_tiktoken_v4\",\n",
    "    metadata={\n",
    "        \"help\": \"MH LM tokenizer version like meta_ai_tiktoken_v4 or tiktoken_v5\"\n",
    "    },\n",
    ")\n",
    "# v5\n",
    "\"instruction_model_type\": \"TikTokv5ChatFormat\",\n",
    "\"lm_mh_tokenizer_version\": \"tiktoken_v5\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test flow on model ckpt /fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm_R2/checkpoint-2800\n",
      "Reading training arguments from file /fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm_R2/checkpoint-2800/training_args.bin\n",
      "eval args: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm_R2/tensorboard', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 48, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 4, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 0.0001, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 100.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm_R2/tensorboard', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2023, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': '$name', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'fsdp': ['full_shard', 'auto_wrap'], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False, 'fsdp_transformer_layer_cls_to_wrap': ['EncoderLayer', 'MMTokenizer', 'PerceptionTokenizer', 'MetaFormersBlock'], 'forward_prefetch': False, 'backward_prefetch': 'backward_pre', 'limit_all_gathers': True}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm_R2/checkpoint-2800', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm/checkpoint-800/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm/checkpoint-800/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'clip_head_weight_decay': None, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm_R2/tensorboard', 'eval_ckpt': '2800', 'eval_type': 'eval_mmmu', 'cosine_decay_to': 0.1, 'using_fsdp2': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 224, 'num_image_chunks': 3, 'activated_num_image_chunks': None, 'max_total_image_chunks': None, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'frames_per_clip': None, 'clips_per_video': 1, 'frame_dilation': None, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'model_type': None, 'lm_mh_tokenizer_version': 'meta_ai_tiktoken_v4', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/GLTG14_29603_I224B512E262m_e6_0916', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'num_classes': None, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': True, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_lm': None, 'perception_tokenizer_name': None, 'checkpoints_perception': None, 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm/checkpoint-800/perception_tokenizer.pt', 'checkpoints_logit_scale': None, 'checkpoints_logit_bias': None, 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_checkpoint': None, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_checkpoint': None, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_checkpoint': None, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_project_in_dim': None, 'perception_tokenizer_project_out_from_dim': None, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'manifold_bucket': None, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': False, 'clip_patch_dropout': None, 'center_crop': False, 'clip_loss_weight': None, 'clip_logit_scale': 2.6592, 'clip_n_registers': None, 'clip_enable_mi': False, 'debug_logs': False, 'llama_seq_cls_dropout_p': None, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'checkpoints_lm_embedding': None, 'checkpoints_lm_head': None, 'checkpoints_lm_cls_head': None, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_frame_stride': None, 'mi_image_encoder_frame_stride': None, 'encoder_num_frames': None, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'replace_llama_attention': True, 'ensemble_meta_perceiver_option': None, 'ensemble_freeze_modality': None, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 35, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'embedding_visualization_layers': None, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl', 'validation_file': '/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl', 'max_length': 50, 'max_seq_len': 1024, 'max_new_tokens': 1024, 'batch_size_generation': 16, 'instr_prompt': 'mmmu', 'task_type': 'instruction_tune', 'generation_task': 'MMMUGenerationTask', 'eval_only': True, 'stopping_token_ids': '', 'repetition_penalty': 1.05}\n",
      "Setting perception tokenizer path to /fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm_R2/checkpoint-2800/perception_tokenizer.pt\n",
      "eval args after sanitization: {'output_dir': '/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm_R2/tensorboard', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 48, 'gradient_accumulation_steps': 4, 'eval_delay': 0, 'learning_rate': 0.0001, 'weight_decay': 0.1, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 100.0, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 200, 'log_level': 'info', 'log_level_replica': 'info', 'log_on_each_node': True, 'logging_dir': '/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm_R2/tensorboard', 'logging_strategy': 'steps', 'logging_first_step': True, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 100, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 2023, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'local_rank': 0, 'tpu_metrics_debug': False, 'dataloader_drop_last': False, 'eval_steps': 250.0, 'dataloader_num_workers': 4, 'past_index': -1, 'run_name': '$name', 'disable_tqdm': False, 'remove_unused_columns': False, 'load_best_model_at_end': False, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'ignore_data_skip': True, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': '/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm_R2/checkpoint-2800', 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'no', 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'batch_eval_metrics': False, 'save_top_k': 1, 'load_latest_k': 1, 'checkpoints_optimizer': '/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm/checkpoint-800/tp=0/optimizer.bin', 'checkpoints_scheduler': '/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm/checkpoint-800/scheduler.pt', 'dpo_alpha': 0.0, 'dpo_beta': 0.1, 'mdpo_delta': 0.0, 'mdpo_blackout_fraction': 0.2, 'mdpo_copo_weight': 1.0, 'mdpo_anchor_weight': 0.1, 'dpo_reference_free': False, 'dpo_label_smoothing': 0.0, 'dpo_chosen_nll_factor': 0.2, 'ipo': False, 'simpo_beta': 2.0, 'simpo_gamma': 1.0, 'mixin_gamma': 1.0, 'load_best_model': False, 'lm_text_lr_scale': 1.0, 'perception_lr_scale': 1.0, 'norm_loss_alpha': 0.0, 'tb_logdir': '/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm_R2/tensorboard', 'eval_ckpt': '2800', 'eval_type': 'eval_mmmu', 'cosine_decay_to': 0.1, 'using_fsdp2': True, 'resize_longest': True, 'add_resized_image': True, 'chunk_size': 224, 'num_image_chunks': 3, 'instruction_model_type': 'MetaAiTikTokv4ChatFormat', 'clips_per_video': 1, 'treat_frames_as_images': False, 'model_name_or_path': '/fsx_0/checkpoints/llama3/mh21', 'tokenizer_path': '/fsx_0/checkpoints/llama3/mh21', 'use_metaformers': True, 'recompute_attn': True, 'recompute_fc1_fc3': True, 'pretraining_tp': 1, 'model_parallel_size': 8, 'context_parallel_size': 1, 'pipeline_parallel_size': 1, 'virtual_pipeline_parallel_size': 1, 'use_alternate_pipeline_parallel_config': True, 'less_layer_first_pp_stage': 0, 'less_layer_last_pp_stage': 0, 'use_te': True, 'parallelize_llm': True, 'parallelize_perception_tokenizer': True, 'use_sdpa': True, 'use_fused_layernorm': True, 'max_parallel_model_loading': 8, 'lm_mh_tokenizer_version': 'meta_ai_tiktoken_v4', 'ensemble_tokenizer_params': '{}', 'use_ensemble_modality_tokenizer': False, 'modality_tokenizer_name': '/fsx_0/user/zhenq/checkpoints/huggingface/GLTG14_29603_I224B512E262m_e6_0916', 'use_fast_tokenizer': True, 'n_prefix_embs': 65, 'routing_classifier': False, 'integrity_classifier': False, 'freeze_perception': True, 'freeze_perception_except_pos_emb': False, 'ignore_mismatched_sizes': False, 'freeze_tokenizer': False, 'freeze_lm': True, 'lm_bits': -1, 'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm10/MM10-Stage1-70B/MH21_70B_224px_norm_R2/checkpoint-2800/perception_tokenizer.pt', 'tokenizer_type': 'PerceiverV3', 'modality': 'image', 'mixin_modality': 'image', 'llm_dropout': 0.0, 'lora': False, 'lora_rank': 8, 'lora_dropout': 0.1, 'lora_alpha': 32, 'lora_target_modules': \"['q_proj', 'v_proj']\", 'lora_tokenizer': False, 'lora_tokenizer_rank': 8, 'lora_tokenizer_dropout': 0.1, 'lora_tokenizer_alpha': 32, 'lora_tokenizer_target_modules': \"['to_q', 'to_kv']\", 'lora_perception': False, 'lora_perception_rank': 8, 'lora_perception_dropout': 0.1, 'lora_perception_alpha': 32, 'lora_perception_target_modules': \"['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2', 'visual_projection']\", 'speech_adapter_type': 0, 'num_media_embeds': 4, 'perception_tokenizer_num_layers': 22, 'perception_tokenizer_attention_dropout_p': 0, 'perception_tokenizer_hidden_dropout_p': 0, 'perception_tokenizer_ensemble_type': 'pool', 'custom_FSDP': True, 'device_map': 'cpu', 'load_in_8bit': False, 'low_cpu_mem_usage': True, 'add_bos_token': False, 'add_eos_token': False, 'perception_output_processor': 'layer', 'keep_vision_hidden_states': False, 'vision_hidden_state_layer': -2, 'gradient_checkpointing_llm': True, 'layer_ckpt': 'none', 'gradient_checkpointing_perception_tokenizer': True, 'gradient_checkpointing_perception': False, 'center_crop': False, 'clip_logit_scale': 2.6592, 'clip_enable_mi': False, 'debug_logs': False, 'add_grounding_tokens': False, 'grounding_bin_size': 42, 'grounding_image_size': 336, 'add_grounding_llm_head_if_available': True, 'perceiver_num_heads': 32, 'perceiver_dim_override': 4096, 'perceiver_num_kv_heads': 8, 'perceiver_num_latents': 64, 'perceiver_source_num_latents': 64, 'perceiver_ff_mult': 4, 'perceiver_use_pos_embs': True, 'perceiver_pos_emb_type': 'learned', 'perceiver_enable_query_aware': False, 'perceiver_add_output_norm': True, 'perceiver_add_output_norm_scaler': False, 'clip_num_embeddings': 577, 'encoder_use_predictor': True, 'perceiver_cat_latents': True, 'perceiver_collapse_chunks': False, 'perceiver_enable_mi': False, 'perceiver_max_img_frames': 16, 'block_sparse': True, 'perceiver_num_frames_per_chunk': 1, 'perceiver_add_eof': True, 'perceiver_collapse_frame_chunks': False, 'perceiver_dynamic_frame_resizing': True, 'perceiver_enable_latent_compression': False, 'perceiver_interleaved_outputs': False, 'perceiver_collapse_modalities': False, 'replace_llama_attention': True, 'enable_multi_transformer_blocks': False, 'n_multi_transformer_blocks': 35, 'n_layers_per_multi_transformer_block': 2, 'enable_modality_aggregator': False, 'modality_aggregator_chunk_size': 1, 'modality_aggregator_split_dim': 1, 'modality_aggregator_merge_dim': 1, 'perceiver_enable_moe': False, 'perceiver_num_experts': 8, 'perceiver_num_activated_experts': 2, 'use_scaled_rope': False, 'rope_scale_factor': 8, 'high_freq_factor': 4, 'low_freq_factor': 1, 'enforce_uniform_emb_variance': True, 'clip_interpolation_factor': 1, 'num_beams': 1, 'train_file': '/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl', 'validation_file': '/fsx_0/dataset01/MMMU/mmmu_validation_v3.jsonl', 'max_length': 50, 'max_seq_len': 1024, 'max_new_tokens': 1024, 'batch_size_generation': 16, 'instr_prompt': 'mmmu', 'task_type': 'instruction_tune', 'generation_task': 'MMMUGenerationTask', 'eval_only': True, 'stopping_token_ids': '', 'repetition_penalty': 1.05}\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_RESULT = \"/tmp\"\n",
    "\n",
    "args = JobRunnerArgs(\n",
    "    trainer_args=None,\n",
    "    eval_args=input_arguments[\"eval_args\"],\n",
    "    fsdp_config={},\n",
    "    log_dir=\"\",\n",
    ")\n",
    "\n",
    "args = load_and_merge_trainer_to_eval_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None MetaAiTikTokv4ChatFormat\n"
     ]
    }
   ],
   "source": [
    "print(args.trainer_args, args.eval_args['instruction_model_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aws/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/hpcaas/.mounts/fs-036153e63d56f4dc2/home/tranx/conda/envs/aws/lib/python3.11/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of  Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import HfArgumentParser\n",
    "from llm_mm_aligner.lib.configs import (\n",
    "    DataTrainingArguments,\n",
    "    GenerationArguments,\n",
    "    ModelArguments,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments, GenerationArguments, TrainingArguments)\n",
    ")\n",
    "(\n",
    "    model_args,\n",
    "    data_args,\n",
    "    generation_args,\n",
    "    training_args,\n",
    ") = parser.parse_args_into_dataclasses(\n",
    "    args=args.get_args_list(args.eval_args),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TikTokv5ChatFormat tiktoken_v5\n"
     ]
    }
   ],
   "source": [
    "print(data_args.instruction_model_type, model_args.lm_mh_tokenizer_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaAiTikTokv4ChatFormat meta_ai_tiktoken_v4\n"
     ]
    }
   ],
   "source": [
    "print(data_args.instruction_model_type, model_args.lm_mh_tokenizer_version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
