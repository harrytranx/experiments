{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import glob \n",
    "import json\n",
    "import sys \n",
    "import os\n",
    "\n",
    "# lib_path = ['/fsx_0/user/tranx/experiments']\n",
    "import_paths = [\n",
    "    \"/fsx_0/user/tranx/experiments\",\n",
    "    # \"/fsx_0/user/tranx\",\n",
    "    # \"/fsx_0/user/tranx/rsync\",\n",
    "    # \"/fsx_0/user/tranx/experiments/lib\"\n",
    "]\n",
    "for path in import_paths:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "        \n",
    "from lib.ablations import Launcher\n",
    "\n",
    "      \n",
    "rsync_launcher = Launcher(\n",
    "    config_base_dir=\"/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws\",\n",
    "    aligner_parent_dir=\"/fsx_0/user/tranx/rsync\"\n",
    ")\n",
    "\n",
    "moe_launcher = Launcher(\n",
    "    config_base_dir=\"/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws\",\n",
    "    aligner_parent_dir=\"/fsx_0/user/tranx/moe\"\n",
    ")\n",
    "\n",
    "def get_trainer_state(output_dir: str):\n",
    "    checkpoint_dirs = glob.glob(f\"{output_dir}/checkpoint-*\")\n",
    "    checkpoints = [int(d.split(\"/\")[-1].split(\"-\")[-1]) for d in checkpoint_dirs]\n",
    "    if len(checkpoints) == 0:\n",
    "        return None, None \n",
    "    \n",
    "    checkpoint_latest = max(checkpoints)\n",
    "    # print(f\"{checkpoint_latest=}\")\n",
    "\n",
    "    trainer_state_file = os.path.join(\n",
    "        output_dir,\n",
    "        f\"checkpoint-{checkpoint_latest}\",\n",
    "        \"trainer_state.json\"\n",
    "    )\n",
    "\n",
    "    with open(trainer_state_file, 'r') as file:\n",
    "        state = json.load(file)\n",
    "        \n",
    "    return checkpoint_latest, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MM10.2 - 70B - Stage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_stage2_R2_n128/241231_12_35_56_078840/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_stage2_R2_n128/241231_12_35_56_078840/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_stage2_R2_n128/241231_12_35_56_078840/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_stage2_R2_n128/241231_12_35_56_078840/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'MH24_70B_ViTH_336px_stage2_R2_n128'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 128),\n",
      "             ('timestamp', '2024-12-31 07:35:56'),\n",
      "             ('input_config',\n",
      "              'mm10.2/stage2_70B/MH24_70B_ViTH_336px_stage2_R2.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/MM10.2_Stage2_70B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    config=\"mm10.2/stage2_70B/MH24_70B_ViTH_336px_stage2_R2.json\",\n",
    "    # config=\"mm10.2/stage2_70B/MH24_70B_ViTG_392px_stage2_R2.json\",\n",
    "    # config=\"mm10.2/stage2_70B/MH24_70B_ViTH_336px_unfreeze_stage2_R2.json\",\n",
    "    nodes=128,\n",
    "    qos='midpri',\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_stage2_R1_n128/241228_04_46_13_005956/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_stage2_R1_n128/241228_04_46_13_005956/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_stage2_R1_n128/241228_04_46_13_005956/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_stage2_R1_n128/241228_04_46_13_005956/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'MH24_70B_ViTH_336px_stage2_R1_n128'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 128),\n",
      "             ('timestamp', '2024-12-27 23:46:12'),\n",
      "             ('input_config',\n",
      "              'mm10.2/stage2_70B/MH24_70B_ViTH_336px_stage2_R1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 4,\n",
      "               'max_tokens_in_batch': 32768,\n",
      "               'max_tokens_in_batch_row': 32768,\n",
      "               'output_dir': '/fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_stage2_R1_n128',\n",
      "               'per_device_train_batch_size': 32}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/MM10.2_Stage2_70B.json\n"
     ]
    }
   ],
   "source": [
    "n = 128\n",
    "\n",
    "rsync_launcher.run(\n",
    "    config=\"mm10.2/stage2_70B/MH24_70B_ViTH_336px_stage2_R1.json\",\n",
    "    nodes=n,\n",
    "    # qos='midpri',\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"max_tokens_in_batch_row\": 32768,\n",
    "        \"max_tokens_in_batch\": 32768,\n",
    "        \"output_dir\": f\"/fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_stage2_R1_n{n}\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_unfreeze_stage2_R1_n128/241228_13_57_32_861273/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_unfreeze_stage2_R1_n128/241228_13_57_32_861273/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_unfreeze_stage2_R1_n128/241228_13_57_32_861273/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_unfreeze_stage2_R1_n128/241228_13_57_32_861273/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'MH24_70B_ViTH_336px_unfreeze_stage2_R1_n128'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 128),\n",
      "             ('timestamp', '2024-12-28 08:57:32'),\n",
      "             ('input_config',\n",
      "              'mm10.2/stage2_70B/MH24_70B_ViTH_336px_unfreeze_stage2_R1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 4,\n",
      "               'gradient_checkpointing_perception': True,\n",
      "               'max_tokens_in_batch': 32768,\n",
      "               'max_tokens_in_batch_row': 32768,\n",
      "               'output_dir': '/fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_unfreeze_stage2_R1_n128',\n",
      "               'per_device_train_batch_size': 32}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/MM10.2_Stage2_70B.json\n"
     ]
    }
   ],
   "source": [
    "n = 128\n",
    "rsync_launcher.run(\n",
    "    config=\"mm10.2/stage2_70B/MH24_70B_ViTH_336px_unfreeze_stage2_R1.json\",\n",
    "    nodes=n,\n",
    "    qos='midpri',\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"gradient_checkpointing_perception\": True,\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"max_tokens_in_batch_row\": 32768,\n",
    "        \"max_tokens_in_batch\": 32768,\n",
    "        # \"per_device_train_batch_size\": 16,\n",
    "        # \"gradient_accumulation_steps\": 8,\n",
    "        # \"max_tokens_in_batch_row\": 16384,\n",
    "        # \"max_tokens_in_batch\": 16384,\n",
    "        \"output_dir\": f\"/fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTH_336px_unfreeze_stage2_R1_n{n}\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTG_392px_stage2_R1_n128/241229_03_43_22_695285/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTG_392px_stage2_R1_n128/241229_03_43_22_695285/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTG_392px_stage2_R1_n128/241229_03_43_22_695285/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTG_392px_stage2_R1_n128/241229_03_43_22_695285/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'MH24_70B_ViTG_392px_stage2_R1_n128'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 128),\n",
      "             ('timestamp', '2024-12-28 22:43:22'),\n",
      "             ('input_config',\n",
      "              'mm10.2/stage2_70B/MH24_70B_ViTG_392px_stage2_R1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 4,\n",
      "               'max_tokens_in_batch': 32768,\n",
      "               'max_tokens_in_batch_row': 32768,\n",
      "               'output_dir': '/fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTG_392px_stage2_R1_n128',\n",
      "               'per_device_train_batch_size': 32}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/MM10.2_Stage2_70B.json\n"
     ]
    }
   ],
   "source": [
    "n = 128\n",
    "\n",
    "rsync_launcher.run(\n",
    "    config=\"mm10.2/stage2_70B/MH24_70B_ViTG_392px_stage2_R1.json\",\n",
    "    nodes=n,\n",
    "    # qos='midpri',\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"max_tokens_in_batch_row\": 32768,\n",
    "        \"max_tokens_in_batch\": 32768,\n",
    "        \"output_dir\": f\"/fsx_0/checkpoints/mm10.2/MM10.2_Stage2_70B/MH24_70B_ViTG_392px_stage2_R1_n{n}\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MM10.2 - 70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm10.2/MM10.2_Stage1_70B/MH24_70B_ViTH_336px_R2_bz48/241225_01_53_49_587067/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm10.2/MM10.2_Stage1_70B/MH24_70B_ViTH_336px_R2_bz48/241225_01_53_49_587067/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm10.2/MM10.2_Stage1_70B/MH24_70B_ViTH_336px_R2_bz48/241225_01_53_49_587067/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm10.2/MM10.2_Stage1_70B/MH24_70B_ViTH_336px_R2_bz48/241225_01_53_49_587067/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'MH24_70B_ViTH_336px_R2_bz48'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 256),\n",
      "             ('timestamp', '2024-12-24 20:53:49'),\n",
      "             ('input_config',\n",
      "              'mm10.2/stage1_70B/MH24_70B_ViTH_336px_R2_20241223.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/mm10.2_stage1.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    config=\"mm10.2/stage2_70B/MH24_70B_ViTG_392px_stage2_R1.json\",\n",
    "    nodes=256,\n",
    "    qos='midpri',\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm10.2/MM10.2_Stage1_70B/MH24_70B_ViTG_392px_mt8400_R2_bz48_resume/241226_04_44_07_923983/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm10.2/MM10.2_Stage1_70B/MH24_70B_ViTG_392px_mt8400_R2_bz48_resume/241226_04_44_07_923983/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm10.2/MM10.2_Stage1_70B/MH24_70B_ViTG_392px_mt8400_R2_bz48_resume/241226_04_44_07_923983/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm10.2/MM10.2_Stage1_70B/MH24_70B_ViTG_392px_mt8400_R2_bz48_resume/241226_04_44_07_923983/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'MH24_70B_ViTG_392px_mt8400_R2_bz48_resume'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 128),\n",
      "             ('timestamp', '2024-12-25 23:44:07'),\n",
      "             ('input_config',\n",
      "              'mm10.2/stage1_70B/MH24_70B_ViTG_392px_mt8400_R2_20241225_resume.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/mm10.2_stage1.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    # config=\"mm10.2/stage1_70B/LLama3.1_70B_ViTG_392px_mt8400_R2_20241220.json\",\n",
    "    # config=\"mm10.2/stage1_70B/LLama3.1_70B_ViTG_392px_mt8400_layer1_R2_20241222.json\",\n",
    "    # config=\"mm10.2/stage1_70B/MH24_70B_ViTG_392px_mt8400_R2_layer1_20241222.json\",\n",
    "    config=\"mm10.2/stage1_70B/MH24_70B_ViTG_392px_mt8400_R2_20241225_resume.json\",\n",
    "    nodes=128,\n",
    "    # qos='midpri',\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm10.2/MM10.2_Stage1_70B/LLama3.1_70B_ViTG_392px_mt8400_R2_bz48/241222_17_16_09_626914/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm10.2/MM10.2_Stage1_70B/LLama3.1_70B_ViTG_392px_mt8400_R2_bz48/241222_17_16_09_626914/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm10.2/MM10.2_Stage1_70B/LLama3.1_70B_ViTG_392px_mt8400_R2_bz48/241222_17_16_09_626914/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm10.2/MM10.2_Stage1_70B/LLama3.1_70B_ViTG_392px_mt8400_R2_bz48/241222_17_16_09_626914/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'LLama3.1_70B_ViTG_392px_mt8400_R2_bz48'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 128),\n",
      "             ('timestamp', '2024-12-22 12:16:09'),\n",
      "             ('input_config',\n",
      "              'mm10.2/stage1_70B/LLama3.1_70B_ViTG_392px_mt8400_R2_20241220.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 3,\n",
      "               'max_tokens_in_batch': 49152,\n",
      "               'max_tokens_in_batch_row': 49152,\n",
      "               'output_dir': '/fsx_0/checkpoints/mm10.2/MM10.2_Stage1_70B/LLama3.1_70B_ViTG_392px_mt8400_R2_bz48',\n",
      "               'per_device_train_batch_size': 48}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/mm10.2_stage1.json\n"
     ]
    }
   ],
   "source": [
    "bz = 48\n",
    "rsync_launcher.run(\n",
    "    # config=\"mm10.2/stage1_70B/LLama3.3_70B_ViTG_392px_R1_20241215.json\",\n",
    "    # config=\"mm10.1/stage1/LLama3.3_70B_ViTG_504px_R1_20241211.json\",\n",
    "    # config=\"mm10.2/stage1_70B/MH24_70B_ViTG_392px_R1_20241218.json\",\n",
    "    # config=\"mm10.2/stage1_70B/MH24_70B_ViTG_392px_mt8400_R2_20241220.json\",\n",
    "    config=\"mm10.2/stage1_70B/LLama3.1_70B_ViTG_392px_mt8400_R2_20241220.json\",\n",
    "    # config=\"mm10.2/stage1_70B/LLama3.1_70B_BigG_336_R2_20241220.json\",\n",
    "    # nodes=32,\n",
    "    nodes=128,\n",
    "    # qos='midpri',\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": bz,\n",
    "        # \"gradient_accumulation_steps\": int(32*4/bz),\n",
    "        \"gradient_accumulation_steps\": 3,\n",
    "        # \"max_tokens_in_batch_row\": 32768,\n",
    "        # \"max_tokens_in_batch\": 32768,\n",
    "        \"max_tokens_in_batch_row\": 49152,\n",
    "        \"max_tokens_in_batch\": 49152,\n",
    "        # \"output_dir\": f\"/fsx_0/checkpoints/mm10.2/MM10.2_Stage1_70B/MH24_70B_ViTG_392px_mt8400_R2_bz{bz}\",\n",
    "        # \"output_dir\": f\"/fsx_0/checkpoints/mm10.2/MM10.2_Stage1_70B/LLama3.1_70B_ViTG_392px_mt8400_R2_bz{bz}\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsync_launcher.run(\n",
    "    # config=\"mm10.1/stage1/MH23.1_70B_ViTH_336px_R1_20241119.json\",\n",
    "    config=\"mm10.1/stage1/LLama3.3_70B_ViTG_504px_R1_20241211.json\",\n",
    "    nodes=64,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": 64,\n",
    "        \"gradient_accumulation_steps\": 2\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MM10.2 - 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/cyprien/checkpoints/data_ablations_mm10/Llama3.1_8B_ViTG_392_stage1_base_vision_select3/241219_03_19_03_485014/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/cyprien/checkpoints/data_ablations_mm10/Llama3.1_8B_ViTG_392_stage1_base_vision_select3/241219_03_19_03_485014/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/cyprien/checkpoints/data_ablations_mm10/Llama3.1_8B_ViTG_392_stage1_base_vision_select3/241219_03_19_03_485014/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/cyprien/checkpoints/data_ablations_mm10/Llama3.1_8B_ViTG_392_stage1_base_vision_select3/241219_03_19_03_485014/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_392_stage1_base_vision_select3'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 16),\n",
      "             ('timestamp', '2024-12-18 22:19:03'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_392_stage1_base_vision_select3.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 4,\n",
      "               'per_device_train_batch_size': 16}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/mm10.2_stage1_8b.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    # config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_392_stage1_base_vision_select2.json\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_392_stage1_base_vision_select3.json\",\n",
    "    nodes=16,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 4\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2_bz16_no_output_norm/241216_01_01_38_744779/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2_bz16_no_output_norm/241216_01_01_38_744779/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2_bz16_no_output_norm/241216_01_01_38_744779/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2_bz16_no_output_norm/241216_01_01_38_744779/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name',\n",
      "              'Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2_bz16_no_output_norm'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 64),\n",
      "             ('timestamp', '2024-12-15 20:01:38'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'enforce_uniform_emb_variance': False,\n",
      "               'gradient_accumulation_steps': 1,\n",
      "               'max_tokens_in_batch': 16384,\n",
      "               'max_tokens_in_batch_row': 16384,\n",
      "               'output_dir': '/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2_bz16_no_output_norm',\n",
      "               'per_device_train_batch_size': 16,\n",
      "               'perceiver_add_output_norm': False}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "bz=16\n",
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2.json\",\n",
    "    nodes=64,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": bz,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"output_dir\": f\"/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2_bz{bz}_no_output_norm\",\n",
    "        \"perceiver_add_output_norm\": False,\n",
    "        \"enforce_uniform_emb_variance\": False,\n",
    "        \"max_tokens_in_batch_row\": 16384,\n",
    "        \"max_tokens_in_batch\": 16384,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2_bz16/241215_18_21_14_598237/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2_bz16/241215_18_21_14_598237/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2_bz16/241215_18_21_14_598237/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2_bz16/241215_18_21_14_598237/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2_bz16'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 64),\n",
      "             ('timestamp', '2024-12-15 13:21:14'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 1,\n",
      "               'max_tokens_in_batch': 16384,\n",
      "               'max_tokens_in_batch_row': 16384,\n",
      "               'output_dir': '/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2_bz16',\n",
      "               'per_device_train_batch_size': 16}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "bz=16\n",
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2.json\",\n",
    "    nodes=64,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": bz,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"output_dir\": f\"/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlpResampler_x2_bz{bz}\",\n",
    "        \"max_tokens_in_batch_row\": 16384,\n",
    "        \"max_tokens_in_batch\": 16384,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlp_2x2_resume/241216_04_18_36_128352/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlp_2x2_resume/241216_04_18_36_128352/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlp_2x2_resume/241216_04_18_36_128352/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlp_2x2_resume/241216_04_18_36_128352/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_392_warmup_mlp_2x2_resume'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 64),\n",
      "             ('timestamp', '2024-12-15 23:18:36'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_392_warmup_mlp_2x2_resume.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 1,\n",
      "               'per_device_train_batch_size': 16}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "bz=16\n",
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    # config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_504ft_warmup_mlp_v2.json\",\n",
    "    # config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_504ft_warmup_mlp_2x2.json\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_392_warmup_mlp_2x2_resume.json\",\n",
    "    nodes=64,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": bz,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        # \"output_dir\": f\"/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_504ft_warmup_mlp_bz{bz}\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/8b_mlp/Llama3.1_8B_ViTG_warmup_mlp/241210_21_39_28_714605/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/8b_mlp/Llama3.1_8B_ViTG_warmup_mlp/241210_21_39_28_714605/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/8b_mlp/Llama3.1_8B_ViTG_warmup_mlp/241210_21_39_28_714605/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/8b_mlp/Llama3.1_8B_ViTG_warmup_mlp/241210_21_39_28_714605/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_warmup_mlp'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 2),\n",
      "             ('timestamp', '2024-12-10 16:39:28'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/mlp_projector/Llama3.1_8B_ViTG_warmup_mlp.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/8b_mlp.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    experiment=\"8b_mlp\",\n",
    "    config=\"experimental/tranx/mlp_projector/Llama3.1_8B_ViTG_warmup_mlp.json\",\n",
    "    nodes=2,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceiver sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers18_dim4096_heads32_latents64_bz64_step4\n",
      "Llama3.1_70B_ViTG_layers18_dim4096_heads32_latents64_bz64_step4, checkpoint_latest=10600\n",
      "/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8\n",
      "Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8, checkpoint_latest=11200\n",
      "/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents128_bz32_step8\n",
      "Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents128_bz32_step8, checkpoint_latest=8800\n",
      "/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers26_dim4096_heads32_latents64_bz64_step4\n",
      "Llama3.1_70B_ViTG_layers26_dim4096_heads32_latents64_bz64_step4, checkpoint_latest=10300\n",
      "/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers14_dim4096_heads32_latents64_bz64_step4\n",
      "Llama3.1_70B_ViTG_layers14_dim4096_heads32_latents64_bz64_step4, checkpoint_latest=10000\n",
      "/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step4\n",
      "Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step4, checkpoint_latest=None\n",
      "/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents32_bz64_step4\n",
      "Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents32_bz64_step4, checkpoint_latest=10800\n",
      "/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers30_dim4096_heads32_latents64_bz64_step4\n",
      "Llama3.1_70B_ViTG_layers30_dim4096_heads32_latents64_bz64_step4, checkpoint_latest=10300\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/fsx_3/bucket/tranx/checkpoints/perceiver_sizing\"\n",
    "output_dirs = glob.glob(f\"{base_dir}/Llama3.1*\")\n",
    "for d in output_dirs:\n",
    "    print(d)\n",
    "    name = d.split(\"/\")[-1]\n",
    "    checkpoint_latest, state = get_trainer_state(d)\n",
    "    \n",
    "    print(f\"{name}, {checkpoint_latest=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents128_bz32_step8/241217_03_00_35_408302/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents128_bz32_step8/241217_03_00_35_408302/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents128_bz32_step8/241217_03_00_35_408302/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents128_bz32_step8/241217_03_00_35_408302/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents128_bz32_step8\n",
      "OrderedDict([('name',\n",
      "              'Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents128_bz32_step8'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 64),\n",
      "             ('timestamp', '2024-12-16 22:00:35'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 8,\n",
      "               'n_prefix_embs': 129,\n",
      "               'output_dir': '/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents128_bz32_step8',\n",
      "               'per_device_train_batch_size': 32,\n",
      "               'perceiver_num_latents': 128,\n",
      "               'perception_tokenizer_num_layers': 22}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/perceiver_sizing.json\n"
     ]
    }
   ],
   "source": [
    "# search tokens\n",
    "# tokens = 32\n",
    "# cf = {\n",
    "#     \"bz\": 64,\n",
    "#     # \"bz\": 32,\n",
    "#     \"step\": 4,\n",
    "#     # \"step\": 16,\n",
    "#     \"layer\": 22,\n",
    "#     \"dim\": 4096,\n",
    "#     \"heads\": 32,\n",
    "#     \"nodes\": 64,\n",
    "#     \"tokens\": tokens\n",
    "# }\n",
    "\n",
    "tokens = 128\n",
    "cf = {\n",
    "    \"bz\": 32,\n",
    "    \"step\": 8,\n",
    "    \"layer\": 22,\n",
    "    \"dim\": 4096,\n",
    "    \"heads\": 32,\n",
    "    \"nodes\": 64,\n",
    "    \"tokens\": tokens\n",
    "}\n",
    "\n",
    "    \n",
    "name = f\"Llama3.1_70B_ViTG_layers{cf['layer']}_dim{cf['dim']}_heads{cf['heads']}_latents{tokens}_bz{cf['bz']}_step{cf['step']}\"\n",
    "output_dir = f\"/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/{name}\"\n",
    "print(name)\n",
    "    \n",
    "rsync_launcher.run(\n",
    "    experiment=\"perceiver_sizing\",\n",
    "    name=name,\n",
    "    config=\"experimental/tranx/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64.json\",\n",
    "    nodes=cf['nodes'],\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    # qos='midpri',\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": cf['bz'],\n",
    "        \"gradient_accumulation_steps\": cf['step'],\n",
    "        \"output_dir\": output_dir,\n",
    "        \"perception_tokenizer_num_layers\": cf['layer'],\n",
    "        \"perceiver_num_latents\": cf['tokens'],\n",
    "        \"n_prefix_embs\": cf['tokens'] + 1\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = {\n",
    "    \"bz\": 64,\n",
    "    \"step\": 4,\n",
    "    \"layer\": 22,\n",
    "    \"dim\": 4096,\n",
    "    \"heads\": 32,\n",
    "    \"nodes\": 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step4/241211_14_48_29_809993/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step4/241211_14_48_29_809993/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step4/241211_14_48_29_809993/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step4/241211_14_48_29_809993/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name',\n",
      "              'Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step4'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 64),\n",
      "             ('timestamp', '2024-12-11 09:48:29'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 4,\n",
      "               'output_dir': '/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step4',\n",
      "               'per_device_train_batch_size': 64,\n",
      "               'perception_tokenizer_num_layers': 22}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/perceiver_sizing.json\n"
     ]
    }
   ],
   "source": [
    "# layers\n",
    "# layers = [18, 26, 30] # 14, 18, 22, 26, 30\n",
    "# layers = [30]\n",
    "layers = [22]\n",
    "for layer in layers:\n",
    "    cf = default.copy()\n",
    "    cf['layer'] = layer\n",
    "    \n",
    "    name = f\"Llama3.1_70B_ViTG_layers{cf['layer']}_dim{cf['dim']}_heads{cf['heads']}_latents64_bz{cf['bz']}_step{cf['step']}\"\n",
    "    output_dir = f\"/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/{name}\"\n",
    "    print(name)\n",
    "        \n",
    "    rsync_launcher.run(\n",
    "        experiment=\"perceiver_sizing\",\n",
    "        name=name,\n",
    "        config=\"experimental/tranx/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64.json\",\n",
    "        nodes=cf['nodes'],\n",
    "        conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "        qos='midpri',\n",
    "        trainer_args={\n",
    "            \"per_device_train_batch_size\": cf['bz'],\n",
    "            \"gradient_accumulation_steps\": cf['step'],\n",
    "            \"output_dir\": output_dir,\n",
    "            \"perception_tokenizer_num_layers\": cf['layer']\n",
    "            # \"parallelize_perception_tokenizer\": True,\n",
    "            # \"gradient_checkpointing_perception_tokenizer\": False\n",
    "        }\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8/241211_14_51_07_203754/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8/241211_14_51_07_203754/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8/241211_14_51_07_203754/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8/241211_14_51_07_203754/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8\n",
      "OrderedDict([('name',\n",
      "              'Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 32),\n",
      "             ('timestamp', '2024-12-11 09:51:07'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 8,\n",
      "               'output_dir': '/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8',\n",
      "               'per_device_train_batch_size': 64,\n",
      "               'perception_tokenizer_num_layers': 22}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/perceiver_sizing.json\n"
     ]
    }
   ],
   "source": [
    "cf = {\n",
    "    \"bz\": 64,\n",
    "    \"step\": 8,\n",
    "    \"layer\": 22,\n",
    "    \"dim\": 4096,\n",
    "    \"heads\": 32,\n",
    "    \"nodes\": 32\n",
    "}\n",
    "\n",
    "layers = [22]\n",
    "for layer in layers:\n",
    "    cf['layer'] = layer\n",
    "    \n",
    "    name = f\"Llama3.1_70B_ViTG_layers{cf['layer']}_dim{cf['dim']}_heads{cf['heads']}_latents64_bz{cf['bz']}_step{cf['step']}\"\n",
    "    output_dir = f\"/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/{name}\"\n",
    "    print(name)\n",
    "        \n",
    "    rsync_launcher.run(\n",
    "        experiment=\"perceiver_sizing\",\n",
    "        name=name,\n",
    "        config=\"experimental/tranx/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64.json\",\n",
    "        nodes=cf['nodes'],\n",
    "        conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "        # qos='midpri',\n",
    "        trainer_args={\n",
    "            \"per_device_train_batch_size\": cf['bz'],\n",
    "            \"gradient_accumulation_steps\": cf['step'],\n",
    "            \"output_dir\": output_dir,\n",
    "            \"perception_tokenizer_num_layers\": cf['layer']\n",
    "            # \"parallelize_perception_tokenizer\": True,\n",
    "            # \"gradient_checkpointing_perception_tokenizer\": False\n",
    "        }\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz32_step16/241204_04_00_07_804497/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz32_step16/241204_04_00_07_804497/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz32_step16/241204_04_00_07_804497/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz32_step16/241204_04_00_07_804497/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8/241204_04_00_07_877180/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8/241204_04_00_07_877180/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8/241204_04_00_07_877180/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8/241204_04_00_07_877180/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name',\n",
      "              'Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz32_step16'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 32),\n",
      "             ('timestamp', '2024-12-03 23:00:07'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 16,\n",
      "               'output_dir': '/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz32_step16',\n",
      "               'per_device_train_batch_size': 32}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/perceiver_sizing.json\n",
      "OrderedDict([('name',\n",
      "              'Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 32),\n",
      "             ('timestamp', '2024-12-03 23:00:07'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 8,\n",
      "               'output_dir': '/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8',\n",
      "               'per_device_train_batch_size': 64}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/perceiver_sizing.json\n"
     ]
    }
   ],
   "source": [
    "for (bz, step) in [(32,16), (64,8)]:\n",
    "    rsync_launcher.run(\n",
    "        experiment=\"perceiver_sizing\",\n",
    "        name=f\"Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz{bz}_step{step}\",\n",
    "        config=\"experimental/tranx/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64.json\",\n",
    "        nodes=32,\n",
    "        conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "        trainer_args={\n",
    "            \"per_device_train_batch_size\": bz,\n",
    "            \"gradient_accumulation_steps\": step,\n",
    "            \"output_dir\": f\"/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz{bz}_step{step}\"\n",
    "        }\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VEV0 - Midtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## midtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_504noft_midtrain_perceiver_17k5_R4/241218_15_47_49_753107/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_504noft_midtrain_perceiver_17k5_R4/241218_15_47_49_753107/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_504noft_midtrain_perceiver_17k5_R4/241218_15_47_49_753107/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_504noft_midtrain_perceiver_17k5_R4/241218_15_47_49_753107/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_504noft_midtrain_perceiver_17k5_R4'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 64),\n",
      "             ('timestamp', '2024-12-18 10:47:49'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_504noft_midtrain_perceiver_17k5_R4.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 2,\n",
      "               'max_steps': 10000,\n",
      "               'max_tokens_in_batch': 16384,\n",
      "               'max_tokens_in_batch_row': 16384,\n",
      "               'per_device_train_batch_size': 8}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    # config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_504ft_midtrain_perceiver_17k5_R1.json\",\n",
    "    # config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_504noft_midtrain_perceiver_17k5_R1.json\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_504noft_midtrain_perceiver_17k5_R4.json\",\n",
    "    # nodes=16,\n",
    "    nodes=64,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        # \"gradient_accumulation_steps\": 8,\n",
    "        \"gradient_accumulation_steps\": 2,\n",
    "        \"max_tokens_in_batch_row\": 16384,\n",
    "        \"max_tokens_in_batch\": 16384,\n",
    "        \"max_steps\": 10000,\n",
    "        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_midtrain_perceiver_17k5_R3_vision_select3/241220_03_56_53_296067/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_midtrain_perceiver_17k5_R3_vision_select3/241220_03_56_53_296067/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_midtrain_perceiver_17k5_R3_vision_select3/241220_03_56_53_296067/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_midtrain_perceiver_17k5_R3_vision_select3/241220_03_56_53_296067/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name',\n",
      "              'Llama3.1_8B_ViTG_392_midtrain_perceiver_17k5_R3_vision_select3'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 64),\n",
      "             ('timestamp', '2024-12-19 22:56:53'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_392_midtrain_perceiver_17k5_R3_vision_select3.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'max_steps': 10000,\n",
      "               'max_tokens_in_batch': 16384,\n",
      "               'max_tokens_in_batch_row': 16384,\n",
      "               'per_device_train_batch_size': 16}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    # config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_midtrain_perceiver_17k5_R2.json\",\n",
    "    # config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_midtrain_perceiver_17k5_R1.json\",\n",
    "    # config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_392_pe_midtrain_perceiver_17k5_R1.json\",\n",
    "    # config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_392_midtrain_perceiver_17k5_R4.json\",\n",
    "    # config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_392_midtrain_mlp_10k_R5.json\",\n",
    "    # config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_392_midtrain_perceiver_17k5_R5.json\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_392_midtrain_perceiver_17k5_R3_vision_select3.json\",\n",
    "    nodes=64,\n",
    "    # nodes=16,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"max_steps\": 10000,\n",
    "        \"max_tokens_in_batch_row\": 16384,\n",
    "        \"max_tokens_in_batch\": 16384,\n",
    "        # \"gradient_accumulation_steps\": 4\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_R2/241209_16_19_51_325053/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_R2/241209_16_19_51_325053/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_R2/241209_16_19_51_325053/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_R2/241209_16_19_51_325053/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_midtrain_R2'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-12-09 11:19:51'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_midtrain_R2.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'max_tokens_in_batch_row': 32000,\n",
      "               'per_device_train_batch_size': 16}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_midtrain_R2.json\",\n",
    "    nodes=8,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"max_tokens_in_batch_row\": 32000,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_freeze_ve/241209_20_42_25_420055/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_freeze_ve/241209_20_42_25_420055/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_freeze_ve/241209_20_42_25_420055/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_freeze_ve/241209_20_42_25_420055/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_midtrain_freeze_ve'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-12-09 15:42:25'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_midtrain_freeze_ve.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'max_tokens_in_batch_row': 32000,\n",
      "               'per_device_train_batch_size': 16}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_midtrain_freeze_ve.json\",\n",
    "    nodes=8,\n",
    "    qos='midpri',\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"max_tokens_in_batch_row\": 32000,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_lr0.1/241216_01_03_29_515061/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_lr0.1/241216_01_03_29_515061/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_lr0.1/241216_01_03_29_515061/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_lr0.1/241216_01_03_29_515061/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_lr1/241216_01_03_29_566548/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_lr1/241216_01_03_29_566548/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_lr1/241216_01_03_29_566548/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_lr1/241216_01_03_29_566548/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama3.1_8B_ViTG_midtrain_lr0.1\n",
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_midtrain_lr0.1'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 16),\n",
      "             ('timestamp', '2024-12-15 20:03:29'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_midtrain.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 2,\n",
      "               'max_tokens_in_batch_row': 32000,\n",
      "               'output_dir': '/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_lr0.1',\n",
      "               'per_device_train_batch_size': 16,\n",
      "               'perception_lr_scale': 0.1}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n",
      "Llama3.1_8B_ViTG_midtrain_lr1\n",
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_midtrain_lr1'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 16),\n",
      "             ('timestamp', '2024-12-15 20:03:29'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_midtrain.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 2,\n",
      "               'max_tokens_in_batch_row': 32000,\n",
      "               'output_dir': '/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_midtrain_lr1',\n",
      "               'per_device_train_batch_size': 16,\n",
      "               'perception_lr_scale': 1}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.1, 1]:\n",
    "# for lr in [0.1]:\n",
    "    name = f\"Llama3.1_8B_ViTG_midtrain_lr{lr}\"\n",
    "    print(name)\n",
    "    rsync_launcher.run(\n",
    "        experiment=\"vev0_midtrain_8B\",\n",
    "        name=name,\n",
    "        config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_midtrain.json\",\n",
    "        # nodes=8,\n",
    "        nodes=16,\n",
    "        conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "        trainer_args={\n",
    "            \"perception_lr_scale\": lr,\n",
    "            \"output_dir\": f\"/fsx_3/bucket/tranx/checkpoints/vev0/{name}\",\n",
    "            \"per_device_train_batch_size\": 16,\n",
    "            \"max_tokens_in_batch_row\": 32000,\n",
    "            # \"gradient_accumulation_steps\": 4,\n",
    "             \"gradient_accumulation_steps\": 2,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_resume_bz16/241214_17_29_10_199078/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_resume_bz16/241214_17_29_10_199078/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_resume_bz16/241214_17_29_10_199078/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_resume_bz16/241214_17_29_10_199078/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_warmup_resume_bz16'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 64),\n",
      "             ('timestamp', '2024-12-14 12:29:10'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_warmup_resume_bz16.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 1,\n",
      "               'per_device_train_batch_size': 16}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_warmup_resume_bz16.json\",\n",
    "    nodes=64,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        # \"output_dir\": f\"/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTH_warmup_bz{bz}\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlp_2x2/241213_05_44_08_890030/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlp_2x2/241213_05_44_08_890030/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlp_2x2/241213_05_44_08_890030/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_392_warmup_mlp_2x2/241213_05_44_08_890030/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_392_warmup_mlp_2x2'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 16),\n",
      "             ('timestamp', '2024-12-13 00:44:08'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_392_warmup_mlp_2x2.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 8,\n",
      "               'per_device_train_batch_size': 8}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_392_warmup_mlp_2x2.json\",\n",
    "    nodes=16,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"gradient_accumulation_steps\": 8,\n",
    "        # \"output_dir\": f\"/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTH_warmup_bz{bz}\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_504_warmup_mlp_2x2/241213_05_00_24_261510/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_504_warmup_mlp_2x2/241213_05_00_24_261510/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_504_warmup_mlp_2x2/241213_05_00_24_261510/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_504_warmup_mlp_2x2/241213_05_00_24_261510/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_504_warmup_mlp_2x2'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 16),\n",
      "             ('timestamp', '2024-12-13 00:00:24'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_504_warmup_mlp_2x2.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 4,\n",
      "               'per_device_train_batch_size': 16}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_504_warmup_mlp_2x2.json\",\n",
    "    nodes=16,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        # \"output_dir\": f\"/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTH_warmup_bz{bz}\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_504ft_warmup/241214_17_38_12_652276/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_504ft_warmup/241214_17_38_12_652276/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_504ft_warmup/241214_17_38_12_652276/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_504ft_warmup/241214_17_38_12_652276/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_504ft_warmup'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-12-14 12:38:12'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_504ft_warmup.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_504ft_warmup.json\",\n",
    "    nodes=8,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_BigG-504-f572374596_warmup/241208_04_54_00_896309/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_BigG-504-f572374596_warmup/241208_04_54_00_896309/run_log.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_BigG-504-f572374596_warmup/241208_04_54_00_896309/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_BigG-504-f572374596_warmup/241208_04_54_00_896309/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_BigG-504-f572374596_warmup'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-12-07 23:54:00'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_BigG-504-f572374596_warmup.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_BigG-504-f572374596_warmup.json\",\n",
    "    nodes=8,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_BigG-504-0712_warmup/241208_04_54_05_429478/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_BigG-504-0712_warmup/241208_04_54_05_429478/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_BigG-504-0712_warmup/241208_04_54_05_429478/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_BigG-504-0712_warmup/241208_04_54_05_429478/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_BigG-504-0712_warmup'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-12-07 23:54:05'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_BigG-504-0712_warmup.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_BigG-504-0712_warmup.json\",\n",
    "    nodes=8,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTH_warmup_bz32/241208_04_25_30_155848/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTH_warmup_bz32/241208_04_25_30_155848/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTH_warmup_bz32/241208_04_25_30_155848/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTH_warmup_bz32/241208_04_25_30_155848/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTH_warmup_bz32'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-12-07 23:25:30'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTH_warmup.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 4,\n",
      "               'output_dir': '/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTH_warmup_bz32',\n",
      "               'per_device_train_batch_size': 32}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "bz=32\n",
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    # note=\"bz = 8\",\n",
    "    name=f\"Llama3.1_8B_ViTH_warmup_bz{bz}\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTH_warmup.json\",\n",
    "    nodes=8,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": bz,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"output_dir\": f\"/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTH_warmup_bz{bz}\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz32/241214_03_49_55_290103/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz32/241214_03_49_55_290103/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz32/241214_03_49_55_290103/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz32/241214_03_49_55_290103/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_warmup_bz32'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 16),\n",
      "             ('timestamp', '2024-12-13 22:49:55'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_warmup.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 2,\n",
      "               'output_dir': '/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz32',\n",
      "               'per_device_train_batch_size': 32,\n",
      "               'use_te': True}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "bz = 32\n",
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    # note=\"bz = 8\",\n",
    "    name=f\"Llama3.1_8B_ViTG_warmup_bz{bz}\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_warmup.json\",\n",
    "    # nodes=8,\n",
    "    nodes=16,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": bz,\n",
    "        # \"gradient_accumulation_steps\": 4,\n",
    "        \"gradient_accumulation_steps\": 2,\n",
    "        \"output_dir\": f\"/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}\",\n",
    "        \"use_te\": True # new\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz32_dim6144/241204_15_46_38_019076/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz32_dim6144/241204_15_46_38_019076/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz32_dim6144/241204_15_46_38_019076/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz32_dim6144/241204_15_46_38_019076/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_warmup_bz32_dim6144'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-12-04 10:46:38'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_warmup.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 4,\n",
      "               'output_dir': '/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz32_dim6144',\n",
      "               'per_device_train_batch_size': 32,\n",
      "               'perceiver_dim_override': 6144}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "# ViT-G with dim = 6144\n",
    "bz = 32\n",
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    name=f\"Llama3.1_8B_ViTG_warmup_bz{bz}_dim6144\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_warmup.json\",\n",
    "    nodes=8,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": bz,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"perceiver_dim_override\": 6144,\n",
    "        \"output_dir\": f\"/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}_dim6144\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_gelu_warmup/241205_17_11_12_199670/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_gelu_warmup/241205_17_11_12_199670/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_gelu_warmup/241205_17_11_12_199670/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_gelu_warmup/241205_17_11_12_199670/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_gelu_warmup'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-12-05 12:11:12'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_gelu_warmup.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "bz = 32\n",
    "rsync_launcher.run(\n",
    "    experiment=\"vev0_midtrain_8B\",\n",
    "    config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_gelu_warmup.json\",\n",
    "    nodes=8,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    # trainer_args={\n",
    "    #     \"per_device_train_batch_size\": bz,\n",
    "    #     \"gradient_accumulation_steps\": 4,\n",
    "    #     \"output_dir\": f\"/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}\"\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_657120/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_657120/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_657120/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_657120/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_751729/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_751729/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_751729/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_751729/launch_script.sh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_801906/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_801906/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_801906/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_801906/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_warmup_bz16'),\n",
      "             ('note', 'bz = 16'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-12-03 19:05:46'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_warmup.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'output_dir': '/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}',\n",
      "               'per_device_train_batch_size': 16}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n",
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_warmup_bz32'),\n",
      "             ('note', 'bz = 32'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-12-03 19:05:46'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_warmup.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'output_dir': '/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}',\n",
      "               'per_device_train_batch_size': 32}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n",
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_warmup_bz64'),\n",
      "             ('note', 'bz = 64'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-12-03 19:05:46'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_warmup.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'output_dir': '/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}',\n",
      "               'per_device_train_batch_size': 64}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n",
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_warmup_bz128'),\n",
      "             ('note', 'bz = 128'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-12-03 19:05:46'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_warmup.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'output_dir': '/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}',\n",
      "               'per_device_train_batch_size': 128}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_856221/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_856221/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_856221/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_856221/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_912038/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_912038/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_912038/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}/241204_00_05_46_912038/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n",
      "OrderedDict([('name', 'Llama3.1_8B_ViTG_warmup_bz256'),\n",
      "             ('note', 'bz = 256'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-12-03 19:05:46'),\n",
      "             ('input_config',\n",
      "              'experimental/tranx/vev0/Llama3.1_8B_ViTG_warmup.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'output_dir': '/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}',\n",
      "               'per_device_train_batch_size': 256}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/vev0_midtrain_8B.json\n"
     ]
    }
   ],
   "source": [
    "# test bz\n",
    "for bz in [16, 32, 64, 128, 256]:\n",
    "    rsync_launcher.run(\n",
    "        name=f\"Llama3.1_8B_ViTG_warmup_bz{bz}\",\n",
    "        experiment=\"vev0_midtrain_8B\",\n",
    "        note=f\"bz = {bz}\",\n",
    "        config=\"experimental/tranx/vev0/Llama3.1_8B_ViTG_warmup.json\",\n",
    "        nodes=8,\n",
    "        conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "        trainer_args={\n",
    "            \"per_device_train_batch_size\": bz,\n",
    "            \"output_dir\": f\"/fsx_3/bucket/tranx/checkpoints/vev0/Llama3.1_8B_ViTG_warmup_bz{bz}\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MM9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/241031_21_42_52_857541/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/241031_21_42_52_857541/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/241031_21_42_52_857541/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/241031_21_42_52_857541/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'LLama3.1_70B_ViTH_336px_R1_recap'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 128),\n",
      "             ('timestamp', '2024-10-31 17:42:52'),\n",
      "             ('input_config',\n",
      "              'mm9.2/stage1/LLama3.1_70B_ViTH_336px_R1_recap_20241031.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/MM9.2_Stage1_70B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    config=\"mm9.2/stage1/LLama3.1_70B_ViTH_336px_R1_recap_20241031.json\",\n",
    "    nodes=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/241102_13_36_49_561652/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/241102_13_36_49_561652/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/241102_13_36_49_561652/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/241102_13_36_49_561652/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'LLama3.1_70B_ViTH_336px_R1_recap'),\n",
      "             ('note', 'test max_tokens_in_batch'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 256),\n",
      "             ('timestamp', '2024-11-02 09:36:49'),\n",
      "             ('input_config',\n",
      "              'mm9.2/stage1/LLama3.1_70B_ViTH_336px_R1_recap_20241031.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/checkpoint-600/perception_tokenizer.pt',\n",
      "               'gradient_accumulation_steps': 1,\n",
      "               'max_tokens_in_batch': 32768,\n",
      "               'max_tokens_in_batch_row': 32768,\n",
      "               'per_device_train_batch_size': 64,\n",
      "               'resume_from_checkpoint': '/fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/checkpoint-600/'}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/MM9.2_Stage1_70B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    config=\"mm9.2/stage1/LLama3.1_70B_ViTH_336px_R1_recap_20241031.json\",\n",
    "    nodes=256,\n",
    "    note=\"test max_tokens_in_batch\",\n",
    "    trainer_args={\n",
    "        \"checkpoints_perception_tokenizer\": \"/fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/checkpoint-600/perception_tokenizer.pt\",\n",
    "        \"resume_from_checkpoint\": \"/fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/checkpoint-600/\",\n",
    "        \"per_device_train_batch_size\": 64,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"max_tokens_in_batch_row\": 32768,\n",
    "        \"max_tokens_in_batch\": 32768,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap_test/241101_02_47_52_852463/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap_test/241101_02_47_52_852463/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap_test/241101_02_47_52_852463/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap_test/241101_02_47_52_852463/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'LLama3.1_70B_ViTH_336px_R1_recap_test'),\n",
      "             ('note', 'test max_tokens_in_batch'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 32),\n",
      "             ('timestamp', '2024-10-31 22:47:52'),\n",
      "             ('input_config',\n",
      "              'mm9.2/stage1/LLama3.1_70B_ViTH_336px_R1_recap_20241031.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'checkpoints_perception_tokenizer': '/fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/checkpoint-600/perception_tokenizer.pt',\n",
      "               'gradient_accumulation_steps': 1,\n",
      "               'max_tokens_in_batch': 50000,\n",
      "               'max_tokens_in_batch_row': 32000,\n",
      "               'output_dir': '/fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap_test',\n",
      "               'per_device_train_batch_size': 64,\n",
      "               'resume_from_checkpoint': '/fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/checkpoint-600/'}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/MM9.2_Stage1_70B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    config=\"mm9.2/stage1/LLama3.1_70B_ViTH_336px_R1_recap_20241031.json\",\n",
    "    nodes=32,\n",
    "    note=\"test max_tokens_in_batch\",\n",
    "    trainer_args={\n",
    "        \"checkpoints_perception_tokenizer\": \"/fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/checkpoint-600/perception_tokenizer.pt\",\n",
    "        \"resume_from_checkpoint\": \"/fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap/checkpoint-600/\",\n",
    "        \"output_dir\": \"/fsx_0/checkpoints/mm9.2/MM9.2_Stage1_70B/LLama3.1_70B_ViTH_336px_R1_recap_test\",\n",
    "        \"per_device_train_batch_size\": 64,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"max_tokens_in_batch_row\": 32000,\n",
    "        \"max_tokens_in_batch\": 50000,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v4_20241121_64nodes/241126_19_41_11_696357/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v4_20241121_64nodes/241126_19_41_11_696357/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v4_20241121_64nodes/241126_19_41_11_696357/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v4_20241121_64nodes/241126_19_41_11_696357/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_70B_ViTH_336px_exp28_plus_v4_20241121_64nodes'),\n",
      "             ('note', 'push bz'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 32),\n",
      "             ('timestamp', '2024-11-26 14:41:11'),\n",
      "             ('input_config',\n",
      "              'mm9.2/stage2/Llama3.1_70B_ViTH_336px_stage2_exp28_plus_v4_20241121_64nodes.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/MM9.2_Stage2_70B.json\n"
     ]
    }
   ],
   "source": [
    "# v2\n",
    "# Test bz\n",
    "rsync_launcher.run(\n",
    "    experiment=\"MM9.2_Stage2_70B\",\n",
    "    config=\"mm9.2/stage2/Llama3.1_70B_ViTH_336px_stage2_exp28_plus_v4_20241121_64nodes.json\",\n",
    "    nodes=32,\n",
    "    note=\"push bz\",\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    # trainer_args={\n",
    "    #     \"learning_rate\": 0.0001,\n",
    "    #     # \"per_device_train_batch_size\": 256,\n",
    "    #     # \"gradient_accumulation_steps\": 1,\n",
    "    #     # \"max_tokens_in_batch\": 128000,\n",
    "    #     \"output_dir\": \"/fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus\",\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v2_20241111_64nodes/241111_18_16_56_266988/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v2_20241111_64nodes/241111_18_16_56_266988/run_log.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v2_20241111_64nodes/241111_18_16_56_266988/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v2_20241111_64nodes/241111_18_16_56_266988/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_70B_ViTH_336px_exp28_plus_v2_20241111_64nodes'),\n",
      "             ('note', 'v2 x 64 nodes'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 64),\n",
      "             ('timestamp', '2024-11-11 13:16:56'),\n",
      "             ('input_config',\n",
      "              'mm9.2/stage2/Llama3.1_70B_ViTH_336px_stage2_exp28_plus_v2_20241111_64nodes.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/MM9.2_Stage2_70B.json\n"
     ]
    }
   ],
   "source": [
    "# v2\n",
    "# Test bz\n",
    "rsync_launcher.run(\n",
    "    experiment=\"MM9.2_Stage2_70B\",\n",
    "    config=\"mm9.2/stage2/Llama3.1_70B_ViTH_336px_stage2_exp28_plus_v2_20241111_64nodes.json\",\n",
    "    nodes=64,\n",
    "    note=\"v2 x 64 nodes\",\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    #     trainer_args={\n",
    "    #     \"per_device_train_batch_size\": 80,\n",
    "    #     \"gradient_accumulation_steps\": 1,\n",
    "    #     \"output_dir\": \"/fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus\"\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp31a_plus_v4/241110_17_41_22_412236/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp31a_plus_v4/241110_17_41_22_412236/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp31a_plus_v4/241110_17_41_22_412236/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp31a_plus_v4/241110_17_41_22_412236/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_70B_ViTH_336px_exp31a_plus_v4'),\n",
      "             ('note', 'exp31a - lr = 1e-5'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 192),\n",
      "             ('timestamp', '2024-11-10 12:41:22'),\n",
      "             ('input_config',\n",
      "              'mm9.2/stage2/Llama3.1_70B_ViTH_336px_stage2_exp31a_plus_v4_20241110.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/MM9.2_Stage2_70B.json\n"
     ]
    }
   ],
   "source": [
    "# v2\n",
    "# Test bz\n",
    "rsync_launcher.run(\n",
    "    experiment=\"MM9.2_Stage2_70B\",\n",
    "    config=\"mm9.2/stage2/Llama3.1_70B_ViTH_336px_stage2_exp31a_plus_v4_20241110.json\",\n",
    "    nodes=64,\n",
    "    note=\"exp31a - lr = 1e-5\",\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    #     trainer_args={\n",
    "    #     \"per_device_train_batch_size\": 80,\n",
    "    #     \"gradient_accumulation_steps\": 1,\n",
    "    #     \"output_dir\": \"/fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus\"\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v2/241108_03_24_29_050323/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v2/241108_03_24_29_050323/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v2/241108_03_24_29_050323/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v2/241108_03_24_29_050323/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_70B_ViTH_336px_exp28_plus_v2'),\n",
      "             ('note', 'fix tally to 1x'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 192),\n",
      "             ('timestamp', '2024-11-07 22:24:29'),\n",
      "             ('input_config',\n",
      "              'mm9.2/stage2/Llama3.1_70B_ViTH_336px_stage2_exp28_plus_v2_20241107.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/MM9.2_Stage2_70B.json\n"
     ]
    }
   ],
   "source": [
    "# v2\n",
    "# Test bz\n",
    "rsync_launcher.run(\n",
    "    experiment=\"MM9.2_Stage2_70B\",\n",
    "    config=\"mm9.2/stage2/Llama3.1_70B_ViTH_336px_stage2_exp28_plus_v2_20241107.json\",\n",
    "    nodes=192,\n",
    "    note=\"fix tally to 1x\",\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    #     trainer_args={\n",
    "    #     \"per_device_train_batch_size\": 80,\n",
    "    #     \"gradient_accumulation_steps\": 1,\n",
    "    #     \"output_dir\": \"/fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus\"\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v3/241108_23_31_35_248805/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v3/241108_23_31_35_248805/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v3/241108_23_31_35_248805/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v3/241108_23_31_35_248805/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_70B_ViTH_336px_exp28_plus_v3'),\n",
      "             ('note', 'v3 recipe + exp28'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 192),\n",
      "             ('timestamp', '2024-11-08 18:31:35'),\n",
      "             ('input_config',\n",
      "              'mm9.2/stage2/Llama3.1_70B_ViTH_336px_stage2_exp28_plus_v3_20241108.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/MM9.2_Stage2_70B.json\n"
     ]
    }
   ],
   "source": [
    "# Test bz\n",
    "rsync_launcher.run(\n",
    "    experiment=\"MM9.2_Stage2_70B\",\n",
    "    config=\"mm9.2/stage2/Llama3.1_70B_ViTH_336px_stage2_exp28_plus_v3_20241108.json\",\n",
    "    nodes=192,\n",
    "    note=\"v3 recipe + exp28\",\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    #     trainer_args={\n",
    "    #     \"per_device_train_batch_size\": 80,\n",
    "    #     \"gradient_accumulation_steps\": 1,\n",
    "    #     \"output_dir\": \"/fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus\"\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28/241106_23_23_38_233825/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28/241106_23_23_38_233825/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28/241106_23_23_38_233825/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28/241106_23_23_38_233825/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_70B_ViTH_336px_exp28'),\n",
      "             ('note', 'Prod, no ft, no resume, mm9 recipe'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 64),\n",
      "             ('timestamp', '2024-11-06 18:23:38'),\n",
      "             ('input_config',\n",
      "              'mm9.2/stage2/Llama3.1_70B_ViTH_336px_stage2_exp28_20241106.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/MM9.2_Stage2_70B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    experiment=\"MM9.2_Stage2_70B\",\n",
    "    config=\"mm9.2/stage2/Llama3.1_70B_ViTH_336px_stage2_exp28_20241106.json\",\n",
    "    nodes=64,\n",
    "    note=\"Prod, no ft, no resume, mm9 recipe\",\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_r2a_20241106/241106_23_27_17_219413/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_r2a_20241106/241106_23_27_17_219413/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_r2a_20241106/241106_23_27_17_219413/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_r2a_20241106/241106_23_27_17_219413/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama3.1_70B_ViTH_336px_r2a_20241106'),\n",
      "             ('note', 'prod - no ft, no resume, new recipe'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 64),\n",
      "             ('timestamp', '2024-11-06 18:27:17'),\n",
      "             ('input_config',\n",
      "              'mm9.2/stage2/Llama3.1_70B_ViTH_336px_stage2_r2a_20241106.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 4,\n",
      "               'per_device_train_batch_size': 64}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/MM9.2_Stage2_70B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    experiment=\"MM9.2_Stage2_70B\",\n",
    "    config=\"mm9.2/stage2/Llama3.1_70B_ViTH_336px_stage2_r2a_20241106.json\",\n",
    "    nodes=64,\n",
    "    note=\"prod - no ft, no resume, new recipe\",\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "        trainer_args={\n",
    "        \"per_device_train_batch_size\": 64,\n",
    "        \"gradient_accumulation_steps\": 4\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama31_70B_ftvith_336_r14600_s2_r2a_1106/241106_23_28_46_670512/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama31_70B_ftvith_336_r14600_s2_r2a_1106/241106_23_28_46_670512/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama31_70B_ftvith_336_r14600_s2_r2a_1106/241106_23_28_46_670512/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama31_70B_ftvith_336_r14600_s2_r2a_1106/241106_23_28_46_670512/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'Llama31_70B_ftvith_336_r14600_s2_r2a_1106'),\n",
      "             ('note', 'prod + ft + r2a'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 64),\n",
      "             ('timestamp', '2024-11-06 18:28:46'),\n",
      "             ('input_config',\n",
      "              'mm9.2/stage2/Llama3.1_70B_ViTHft_336px_stage2_r2a_20241106.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 4,\n",
      "               'per_device_train_batch_size': 64}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/MM9.2_Stage2_70B.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    experiment=\"MM9.2_Stage2_70B\",\n",
    "    config=\"mm9.2/stage2/Llama3.1_70B_ViTHft_336px_stage2_r2a_20241106.json\",\n",
    "    nodes=64,\n",
    "    note=\"prod + ft + r2a\",\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "        trainer_args={\n",
    "        \"per_device_train_batch_size\": 64,\n",
    "        \"gradient_accumulation_steps\": 4\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MM10.1 Prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stage 1\n",
    "# rsync_launcher.run(\n",
    "#     config=\"mm10.1/stage1/MH22final_70B_ViTH_336px_R1_recap_20241024_resume.json\",\n",
    "#     nodes=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/LLama3.3_70B_ViTG_504px_R1/241211_16_28_26_512248/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/LLama3.3_70B_ViTG_504px_R1/241211_16_28_26_512248/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/LLama3.3_70B_ViTG_504px_R1/241211_16_28_26_512248/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/LLama3.3_70B_ViTG_504px_R1/241211_16_28_26_512248/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'LLama3.3_70B_ViTG_504px_R1'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 32),\n",
      "             ('timestamp', '2024-12-11 11:28:26'),\n",
      "             ('input_config',\n",
      "              'mm10.1/stage1/LLama3.3_70B_ViTG_504px_R1_20241211.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 4,\n",
      "               'per_device_train_batch_size': 64}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/mm10.1_stage1.json\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH23.1_70B_ViTH_336px_R1_no_norm/241127_03_26_33_406000/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH23.1_70B_ViTH_336px_R1_no_norm/241127_03_26_33_406000/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH23.1_70B_ViTH_336px_R1_no_norm/241127_03_26_33_406000/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH23.1_70B_ViTH_336px_R1_no_norm/241127_03_26_33_406000/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'MH23.1_70B_ViTH_336px_R1_no_norm'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 32),\n",
      "             ('timestamp', '2024-11-26 22:26:33'),\n",
      "             ('input_config',\n",
      "              'mm10.1/stage1/MH23.1_70B_ViTH_336px_R1_20241126_no_norm.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 4,\n",
      "               'per_device_train_batch_size': 64}),\n",
      "             ('conda_env', '/fsx_0/shared/conda/aligner_20241030')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/mm10.1_stage1.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    # config=\"mm10.1/stage1/MH23.1_70B_ViTH_336px_R1_20241119.json\",\n",
    "    config=\"mm10.1/stage1/MH23.1_70B_ViTH_336px_R1_20241126_no_norm.json\",\n",
    "    nodes=32,\n",
    "    conda_env=\"/fsx_0/shared/conda/aligner_20241030\",\n",
    "    trainer_args={\n",
    "        \"per_device_train_batch_size\": 64,\n",
    "        \"gradient_accumulation_steps\": 4\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_idl/241031_14_07_24_819119/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_idl/241031_14_07_24_819119/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_idl/241031_14_07_24_819119/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_idl/241031_14_07_24_819119/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'MH22final_70B_ViTH_336px_idl'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 32),\n",
      "             ('timestamp', '2024-10-31 10:07:24'),\n",
      "             ('input_config',\n",
      "              'mm10.1/stage1/MH22final_70B_ViTH_336px_idl_20241031.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/mm10.1_stage1.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    config=\"mm10.1/stage1/MH22final_70B_ViTH_336px_idl_20241031.json\",\n",
    "    nodes=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_stage2_baseline/241104_21_28_57_019409/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_stage2_baseline/241104_21_28_57_019409/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_stage2_baseline/241104_21_28_57_019409/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/moe/70B_stage2_baseline/241104_21_28_57_019409/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', '70B_stage2_baseline'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 32),\n",
      "             ('timestamp', '2024-11-04 16:28:57'),\n",
      "             ('input_config',\n",
      "              'mm10.1/stage2/MH22final_70B_ViTH_336px_stage2_exp30d.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/tranx_test.json\n"
     ]
    }
   ],
   "source": [
    "moe_launcher.run(\n",
    "    config=\"mm10.1/stage2/MH22final_70B_ViTH_336px_stage2_exp30d.json\",\n",
    "    nodes=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_stage2_baseline/241104_18_06_44_455811/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_stage2_baseline/241104_18_06_44_455811/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_stage2_baseline/241104_18_06_44_455811/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/moe/70B_stage2_baseline/241104_18_06_44_455811/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', '70B_stage2_baseline'),\n",
      "             ('note', None),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 32),\n",
      "             ('timestamp', '2024-11-04 13:06:44'),\n",
      "             ('input_config',\n",
      "              'mm10.1/stage2/MH22final_70B_ViTH_336px_stage2_exp30d.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/tranx_test.json\n"
     ]
    }
   ],
   "source": [
    "# stage 2 baseline to compare with moe\n",
    "rsync_launcher.run(\n",
    "    config=\"mm10.1/stage2/MH22final_70B_ViTH_336px_stage2_exp30d.json\",\n",
    "    nodes=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage2_70B/MH22final_70B_ViTHft_336px_stage2_r2a/241104_18_08_47_495896/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage2_70B/MH22final_70B_ViTHft_336px_stage2_r2a/241104_18_08_47_495896/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage2_70B/MH22final_70B_ViTHft_336px_stage2_r2a/241104_18_08_47_495896/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage2_70B/MH22final_70B_ViTHft_336px_stage2_r2a/241104_18_08_47_495896/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'MH22final_70B_ViTHft_336px_stage2_r2a'),\n",
      "             ('note', 'r2a'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 128),\n",
      "             ('timestamp', '2024-11-04 13:08:47'),\n",
      "             ('input_config',\n",
      "              'mm10.1/stage2/MH22final_70B_ViTHft_336px_stage2_r2a_20241101.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides', None),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/mm10.1_stage2.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    config=\"mm10.1/stage2/MH22final_70B_ViTHft_336px_stage2_r2a_20241101.json\",\n",
    "    nodes=128,\n",
    "    note=\"r2a\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage2_70B/MH22final_70B_ViTHft_336px_stage2_pdfa/241101_00_02_58_082508/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage2_70B/MH22final_70B_ViTHft_336px_stage2_pdfa/241101_00_02_58_082508/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage2_70B/MH22final_70B_ViTHft_336px_stage2_pdfa/241101_00_02_58_082508/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage2_70B/MH22final_70B_ViTHft_336px_stage2_pdfa/241101_00_02_58_082508/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'MH22final_70B_ViTHft_336px_stage2_pdfa'),\n",
      "             ('note', 'pdfa + idl'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 4),\n",
      "             ('timestamp', '2024-10-31 20:02:58'),\n",
      "             ('input_config',\n",
      "              'mm10.1/stage2/MH22final_70B_ViTHft_336px_stage2_20241030.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'output_dir': '/fsx_0/checkpoints/mm10.1/MM10.1_Stage2_70B/MH22final_70B_ViTHft_336px_stage2_pdfa',\n",
      "               'wd_data_path': ['/fsx_3/dataset01/pdfa-eng-wds-converted',\n",
      "                                '/fsx_0/user/yetian12/datasets/idl-wds_v2']}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/mm10.1_stage2.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    config=\"mm10.1/stage2/MH22final_70B_ViTHft_336px_stage2_20241030.json\",\n",
    "    nodes=4,\n",
    "    note=\"pdfa + idl\",\n",
    "    trainer_args={\n",
    "        \"wd_data_path\": [\n",
    "            \"/fsx_3/dataset01/pdfa-eng-wds-converted\",\n",
    "            \"/fsx_0/user/yetian12/datasets/idl-wds_v2\"\n",
    "        ],\n",
    "        \"output_dir\": \"/fsx_0/checkpoints/mm10.1/MM10.1_Stage2_70B/MH22final_70B_ViTHft_336px_stage2_pdfa\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "moe_launcher = Launcher(\n",
    "    config_base_dir=\"/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws\",\n",
    "    aligner_parent_dir=\"/fsx_0/user/tranx/moe\"\n",
    ")\n",
    "\n",
    "# moe_launcher.cancel(\n",
    "#     wandb_project_name=\"tranx_test\",\n",
    "#     experiment=\"bz_x_max_tokens_no_cp\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Test limit on batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = \"mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_8x1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_5000/241028_16_06_49_077917/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_5000/241028_16_06_49_077917/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_5000/241028_16_06_49_077917/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_5000/241028_16_06_49_077917/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_13000/241028_16_06_49_146971/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_13000/241028_16_06_49_146971/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_13000/241028_16_06_49_146971/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_13000/241028_16_06_49_146971/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_32000/241028_16_06_49_207884/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_32000/241028_16_06_49_207884/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_32000/241028_16_06_49_207884/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_32000/241028_16_06_49_207884/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 5000\n",
      "/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_5000\n",
      "OrderedDict([('name', '70B_moe_22x8x2_meta_dev_resume'),\n",
      "             ('note', 'Test batch_size and max_tokens'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-10-28 12:06:49'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_8x1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.max_tokens_in_batch': 5000,\n",
      "               'trainer_args.max_tokens_in_batch_row': 5000,\n",
      "               'trainer_args.output_dir': '/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_5000',\n",
      "               'trainer_args.per_device_train_batch_size': 8}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/tranx_test.json\n",
      "8 13000\n",
      "/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_13000\n",
      "OrderedDict([('name', '70B_moe_22x8x2_meta_dev_resume'),\n",
      "             ('note', 'Test batch_size and max_tokens'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-10-28 12:06:49'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_8x1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.max_tokens_in_batch': 13000,\n",
      "               'trainer_args.max_tokens_in_batch_row': 13000,\n",
      "               'trainer_args.output_dir': '/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_13000',\n",
      "               'trainer_args.per_device_train_batch_size': 8}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/tranx_test.json\n",
      "8 32000\n",
      "/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_32000\n",
      "OrderedDict([('name', '70B_moe_22x8x2_meta_dev_resume'),\n",
      "             ('note', 'Test batch_size and max_tokens'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-10-28 12:06:49'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_8x1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.max_tokens_in_batch': 32000,\n",
      "               'trainer_args.max_tokens_in_batch_row': 32000,\n",
      "               'trainer_args.output_dir': '/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1_max_tokens_32000',\n",
      "               'trainer_args.per_device_train_batch_size': 8}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_5000/241028_16_06_49_271103/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_5000/241028_16_06_49_271103/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_5000/241028_16_06_49_271103/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_5000/241028_16_06_49_271103/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_13000/241028_16_06_49_333016/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_13000/241028_16_06_49_333016/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_13000/241028_16_06_49_333016/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_13000/241028_16_06_49_333016/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_32000/241028_16_06_49_398957/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_32000/241028_16_06_49_398957/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_32000/241028_16_06_49_398957/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_32000/241028_16_06_49_398957/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_5000/241028_16_06_49_461869/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_5000/241028_16_06_49_461869/run_log.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/tranx_test.json\n",
      "16 5000\n",
      "/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_5000\n",
      "OrderedDict([('name', '70B_moe_22x8x2_meta_dev_resume'),\n",
      "             ('note', 'Test batch_size and max_tokens'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-10-28 12:06:49'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_8x1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.max_tokens_in_batch': 5000,\n",
      "               'trainer_args.max_tokens_in_batch_row': 5000,\n",
      "               'trainer_args.output_dir': '/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_5000',\n",
      "               'trainer_args.per_device_train_batch_size': 16}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/tranx_test.json\n",
      "16 13000\n",
      "/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_13000\n",
      "OrderedDict([('name', '70B_moe_22x8x2_meta_dev_resume'),\n",
      "             ('note', 'Test batch_size and max_tokens'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-10-28 12:06:49'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_8x1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.max_tokens_in_batch': 13000,\n",
      "               'trainer_args.max_tokens_in_batch_row': 13000,\n",
      "               'trainer_args.output_dir': '/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_13000',\n",
      "               'trainer_args.per_device_train_batch_size': 16}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/tranx_test.json\n",
      "16 32000\n",
      "/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_32000\n",
      "OrderedDict([('name', '70B_moe_22x8x2_meta_dev_resume'),\n",
      "             ('note', 'Test batch_size and max_tokens'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-10-28 12:06:49'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_8x1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.max_tokens_in_batch': 32000,\n",
      "               'trainer_args.max_tokens_in_batch_row': 32000,\n",
      "               'trainer_args.output_dir': '/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_16x1_max_tokens_32000',\n",
      "               'trainer_args.per_device_train_batch_size': 16}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/tranx_test.json\n",
      "32 5000\n",
      "/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_5000\n",
      "OrderedDict([('name', '70B_moe_22x8x2_meta_dev_resume'),\n",
      "             ('note', 'Test batch_size and max_tokens'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-10-28 12:06:49'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_8x1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.max_tokens_in_batch': 5000,\n",
      "               'trainer_args.max_tokens_in_batch_row': 5000,\n",
      "               'trainer_args.output_dir': '/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_5000',\n",
      "               'trainer_args.per_device_train_batch_size': 32}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_5000/241028_16_06_49_461869/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_5000/241028_16_06_49_461869/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_13000/241028_16_06_49_702547/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_13000/241028_16_06_49_702547/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_13000/241028_16_06_49_702547/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_13000/241028_16_06_49_702547/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_32000/241028_16_06_49_767939/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_32000/241028_16_06_49_767939/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_32000/241028_16_06_49_767939/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_32000/241028_16_06_49_767939/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/tranx_test.json\n",
      "32 13000\n",
      "/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_13000\n",
      "OrderedDict([('name', '70B_moe_22x8x2_meta_dev_resume'),\n",
      "             ('note', 'Test batch_size and max_tokens'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-10-28 12:06:49'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_8x1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.max_tokens_in_batch': 13000,\n",
      "               'trainer_args.max_tokens_in_batch_row': 13000,\n",
      "               'trainer_args.output_dir': '/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_13000',\n",
      "               'trainer_args.per_device_train_batch_size': 32}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/tranx_test.json\n",
      "32 32000\n",
      "/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_32000\n",
      "OrderedDict([('name', '70B_moe_22x8x2_meta_dev_resume'),\n",
      "             ('note', 'Test batch_size and max_tokens'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-10-28 12:06:49'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_8x1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.max_tokens_in_batch': 32000,\n",
      "               'trainer_args.max_tokens_in_batch_row': 32000,\n",
      "               'trainer_args.output_dir': '/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_32x1_max_tokens_32000',\n",
      "               'trainer_args.per_device_train_batch_size': 32}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/tranx_test.json\n"
     ]
    }
   ],
   "source": [
    "for bz in [8, 16, 32]:\n",
    "    for max_tokens in [5000, 13000, 32000]:\n",
    "        print(bz, max_tokens)\n",
    "        output = f\"/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_{bz}x1_max_tokens_{max_tokens}\"\n",
    "        print(output)\n",
    "        moe_launcher.run(\n",
    "            note = \"Test batch_size and max_tokens\",\n",
    "            config=base_config,\n",
    "            nodes=8,\n",
    "            overrides_dict={\n",
    "                \"trainer_args.output_dir\": output,\n",
    "                \"trainer_args.per_device_train_batch_size\": bz,\n",
    "                \"trainer_args.max_tokens_in_batch_row\": max_tokens,\n",
    "                \"trainer_args.max_tokens_in_batch\": max_tokens,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_32x1_cp4.json/241028_19_13_19_909836/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_32x1_cp4.json/241028_19_13_19_909836/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_32x1_cp4.json/241028_19_13_19_909836/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_32x1_cp4.json/241028_19_13_19_909836/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', '70B_moe_22x8x2_bz_32x1_cp4'),\n",
      "             ('note', 'Test CP'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 32),\n",
      "             ('timestamp', '2024-10-28 15:13:19'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_32x1_cp4.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.context_parallel_size': 4,\n",
      "               'trainer_args.output_dir': 'mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_32x1_cp4.json'}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/tranx_test.json\n"
     ]
    }
   ],
   "source": [
    "moe_launcher = Launcher(\n",
    "    config_base_dir=\"/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws\",\n",
    "    aligner_parent_dir=\"/fsx_0/user/tranx/moe\"\n",
    ")\n",
    "\n",
    "base_config = \"mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_32x1_cp4.json\"\n",
    "\n",
    "moe_launcher.run(\n",
    "    note = \"Test CP\",\n",
    "    config=base_config,\n",
    "    nodes=32,\n",
    "    experiment=\"test_cp\",\n",
    "    overrides_dict={\n",
    "        \"trainer_args.output_dir\": \"mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_32x1_cp4.json\",\n",
    "        \"trainer_args.context_parallel_size\": 4\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_16x1_cp4.json/241028_19_16_56_528849/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_16x1_cp4.json/241028_19_16_56_528849/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_16x1_cp4.json/241028_19_16_56_528849/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_16x1_cp4.json/241028_19_16_56_528849/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', '70B_moe_22x8x2_bz_32x1_cp4'),\n",
      "             ('note', 'Test CP'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 32),\n",
      "             ('timestamp', '2024-10-28 15:16:56'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_32x1_cp4.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.context_parallel_size': 4,\n",
      "               'trainer_args.output_dir': 'mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_16x1_cp4.json',\n",
      "               'trainer_args.per_device_train_batch_size': 16}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/tranx_test.json\n"
     ]
    }
   ],
   "source": [
    "moe_launcher.run(\n",
    "    note = \"Test CP\",\n",
    "    config=base_config,\n",
    "    nodes=32,\n",
    "    experiment=\"test_cp\",\n",
    "    overrides_dict={\n",
    "        \"trainer_args.output_dir\": \"mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_16x1_cp4.json\",\n",
    "        \"trainer_args.context_parallel_size\": 4,\n",
    "        \"trainer_args.per_device_train_batch_size\": 16\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = \"mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_8x1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/config.json\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/launch_script.sh\n"
     ]
    }
   ],
   "source": [
    "run(\n",
    "    config=base_config,\n",
    "    nodes=8,\n",
    "    overrides_dict={\n",
    "        \"trainer_args.gradient_accumulation_steps\": 2,\n",
    "        \"trainer_args.output_dir\": \"/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bz in [16, 32]:\n",
    "    run(\n",
    "        config=f\"mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_{bz}x1.json\",\n",
    "        nodes=8,\n",
    "        overrides_dict={\n",
    "            \"trainer_args.output_dir\": \"/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # trainer_args={\n",
    "    #     \"per_device_train_batch_size\": 256,\n",
    "    #     \"gradient_accumulation_steps\": 1,\n",
    "    #     \"max_tokens_in_batch\": 128000,\n",
    "    #     \"output_dir\": \"/fsx_0/checkpoints/mm9.2/MM9.2_Stage2_70B/Llama3.1_70B_ViTH_336px_exp28_plus_v2_20241111_64nodes_bz256_tokens_128k\"\n",
    "    # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_22layers/241120_14_06_00_558714/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_22layers/241120_14_06_00_558714/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_22layers/241120_14_06_00_558714/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_22layers/241120_14_06_00_558714/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'LLama3_8B_ViTH_336px_22layers'),\n",
      "             ('note', 'baseline'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 4),\n",
      "             ('timestamp', '2024-11-20 09:06:00'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/stage1/pretrain_8B_Llama3_ViTH_336px_8layers.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'output_dir': '/fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_22layers',\n",
      "               'perception_tokenizer_num_layers': 22}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/mm_stage1_8B.json\n"
     ]
    }
   ],
   "source": [
    "# baseline - 22 layers \n",
    "\n",
    "moe_launcher.run(\n",
    "    note = \"baseline\",\n",
    "    config=\"mm10.1_moe/stage1/pretrain_8B_Llama3_ViTH_336px_8layers.json\",\n",
    "    nodes=4,\n",
    "    experiment=\"8B_stage1\",\n",
    "    trainer_args={\n",
    "        \"perception_tokenizer_num_layers\": 22,\n",
    "        \"output_dir\": \"/fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_22layers\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_1x1/241030_04_02_28_283458/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_1x1/241030_04_02_28_283458/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_1x1/241030_04_02_28_283458/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_1x1/241030_04_02_28_283458/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_2x1/241030_04_02_28_333573/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_2x1/241030_04_02_28_333573/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_2x1/241030_04_02_28_333573/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_2x1/241030_04_02_28_333573/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_4x1/241030_04_02_28_379673/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_4x1/241030_04_02_28_379673/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_4x1/241030_04_02_28_379673/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_4x1/241030_04_02_28_379673/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_4x2/241030_04_02_28_417928/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_4x2/241030_04_02_28_417928/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_4x2/241030_04_02_28_417928/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_4x2/241030_04_02_28_417928/launch_script.sh\n",
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_8x2/241030_04_02_28_458578/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_8x2/241030_04_02_28_458578/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_8x2/241030_04_02_28_458578/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_8x2/241030_04_02_28_458578/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'LLama3_8B_ViTH_336px_8layers'),\n",
      "             ('note', ''),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 4),\n",
      "             ('timestamp', '2024-10-30 00:02:28'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/pretrain_8B_Llama3_ViTH_336px_8layers_moe_2x1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.output_dir': '/fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_1x1',\n",
      "               'trainer_args.perceiver_num_activated_experts': 1,\n",
      "               'trainer_args.perceiver_num_experts': 1}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/mm_stage1_8B.json\n",
      "OrderedDict([('name', 'LLama3_8B_ViTH_336px_8layers'),\n",
      "             ('note', ''),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 4),\n",
      "             ('timestamp', '2024-10-30 00:02:28'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/pretrain_8B_Llama3_ViTH_336px_8layers_moe_2x1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.output_dir': '/fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_2x1',\n",
      "               'trainer_args.perceiver_num_activated_experts': 1,\n",
      "               'trainer_args.perceiver_num_experts': 2}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/mm_stage1_8B.json\n",
      "OrderedDict([('name', 'LLama3_8B_ViTH_336px_8layers'),\n",
      "             ('note', ''),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 4),\n",
      "             ('timestamp', '2024-10-30 00:02:28'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/pretrain_8B_Llama3_ViTH_336px_8layers_moe_2x1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.output_dir': '/fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_4x1',\n",
      "               'trainer_args.perceiver_num_activated_experts': 1,\n",
      "               'trainer_args.perceiver_num_experts': 4}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/mm_stage1_8B.json\n",
      "OrderedDict([('name', 'LLama3_8B_ViTH_336px_8layers'),\n",
      "             ('note', ''),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 4),\n",
      "             ('timestamp', '2024-10-30 00:02:28'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/pretrain_8B_Llama3_ViTH_336px_8layers_moe_2x1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.output_dir': '/fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_4x2',\n",
      "               'trainer_args.perceiver_num_activated_experts': 2,\n",
      "               'trainer_args.perceiver_num_experts': 4}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/mm_stage1_8B.json\n",
      "OrderedDict([('name', 'LLama3_8B_ViTH_336px_8layers'),\n",
      "             ('note', ''),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 4),\n",
      "             ('timestamp', '2024-10-30 00:02:28'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/moe/pretrain_8B_Llama3_ViTH_336px_8layers_moe_2x1.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides',\n",
      "              {'trainer_args.output_dir': '/fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_8x2',\n",
      "               'trainer_args.perceiver_num_activated_experts': 2,\n",
      "               'trainer_args.perceiver_num_experts': 8}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/mm_stage1_8B.json\n"
     ]
    }
   ],
   "source": [
    "moe_8b_base = \"mm10.1_moe/moe/pretrain_8B_Llama3_ViTH_336px_8layers_moe_2x1.json\"\n",
    "\n",
    "# for num_experts, active_experts in [(1,1), (2,1)]: \n",
    "for num_experts, active_experts in [(1,1), (2,1), (4,1), (4,2), (8,2)]:\n",
    "    moe_launcher.run(\n",
    "        note=\"\",\n",
    "        config=moe_8b_base,\n",
    "        nodes=4,\n",
    "        experiment=\"8B_stage1\",\n",
    "        name=f\"LLama3_8B_ViTH_336px_8layers_moe_{num_experts}x{active_experts}\",\n",
    "        overrides_dict={\n",
    "            \"trainer_args.perceiver_num_experts\": num_experts,\n",
    "            \"trainer_args.perceiver_num_activated_experts\": active_experts,\n",
    "            \"trainer_args.output_dir\": f\"/fsx_0/checkpoints/tranx/Aligner_Pretrain_LLama3_8B/LLama3_8B_ViTH_336px_8layers_moe_{num_experts}x{active_experts}\"\n",
    "        }\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "2 1\n",
      "4 1\n",
      "4 2\n",
      "8 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for num_experts, active_experts in [(1,1), (2,1), (4,1), (4,2), (8,2)]:\n",
    "    print(num_experts, active_experts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upcycling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_stage2_upcycling_22x8x2/241112_17_07_37_719550/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_stage2_upcycling_22x8x2/241112_17_07_37_719550/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_stage2_upcycling_22x8x2/241112_17_07_37_719550/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_stage2_upcycling_22x8x2/241112_17_07_37_719550/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', '70B_moe_stage2_upcycling_22x8x2'),\n",
      "             ('note', 'test moe on rsync code'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 8),\n",
      "             ('timestamp', '2024-11-12 12:07:37'),\n",
      "             ('input_config',\n",
      "              'mm10.1/stage3_moe/MH22final_70B_ViTH_336px_stage3_upcycling_moe_22x8x2.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 4,\n",
      "               'per_device_train_batch_size': 32}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/tranx_test.json\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    note = \"test moe on rsync code\",\n",
    "    config=\"mm10.1/stage3_moe/MH22final_70B_ViTH_336px_stage3_upcycling_moe_22x8x2.json\",\n",
    "    nodes=8,\n",
    "    experiment=\"stage3_upcycling\",\n",
    "    trainer_args={\n",
    "        # \"trainer_args.output_dir\": \"mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_16x1_cp4.json\",\n",
    "        # \"trainer_args.context_parallel_size\": 4,\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"gradient_accumulation_steps\": 4\n",
    "        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:lib.launch_job_tranx:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_stage2_upcycling_22x8x2/241104_21_30_53_314767/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_stage2_upcycling_22x8x2/241104_21_30_53_314767/run_log.txt\n",
      "WARNING:lib.launch_job_tranx:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_stage2_upcycling_22x8x2/241104_21_30_53_314767/config.json\n",
      "WARNING:lib.launch_job_tranx:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_stage2_upcycling_22x8x2/241104_21_30_53_314767/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', '70B_moe_stage2_upcycling_22x8x2'),\n",
      "             ('note', 'Try avoid error in collator'),\n",
      "             ('job_id', None),\n",
      "             ('nodes', 32),\n",
      "             ('timestamp', '2024-11-04 16:30:53'),\n",
      "             ('input_config',\n",
      "              'mm10.1_moe/stage2/MH22final_70B_ViTH_336px_stage2_upcycling_moe_22x8x2.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws'),\n",
      "             ('trainer_args_overrides',\n",
      "              {'gradient_accumulation_steps': 4,\n",
      "               'per_device_train_batch_size': 32}),\n",
      "             ('conda_env',\n",
      "              '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822')])\n",
      "Save run log to /fsx_0/user/tranx/experiments/ablations/run_log/tranx_test.json\n"
     ]
    }
   ],
   "source": [
    "moe_launcher.run(\n",
    "    note = \"Try avoid error in collator\",\n",
    "    config=\"mm10.1_moe/stage2/MH22final_70B_ViTH_336px_stage2_upcycling_moe_22x8x2.json\",\n",
    "    nodes=32,\n",
    "    experiment=\"stage2_upcycling\",\n",
    "    trainer_args={\n",
    "        # \"trainer_args.output_dir\": \"mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_16x1_cp4.json\",\n",
    "        # \"trainer_args.context_parallel_size\": 4,\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"gradient_accumulation_steps\": 4\n",
    "        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [\"/fsx_3/dataset01/cauldron/cocoqa\", 78736, 10, 0.7874],\n",
    "    [\"/fsx_3/dataset01/cauldron/multihiertt\", 7830, 1, 0.008],\n",
    "    [\"/fsx_3/dataset01/cauldron/robut_sqa\", 34141, 1, 0.0341],\n",
    "    [\"/fsx_3/dataset01/cauldron/tat_qa\", 13215, 1, 0.013],\n",
    "    [\"/fsx_3/dataset01/cauldron/chart2text\", 30215, 1, 0.0302],\n",
    "    [\"/fsx_3/dataset01/cauldron/datikz\", 48296, 1, 0.048],\n",
    "    [\"/fsx_3/dataset01/cauldron/diagram_image_to_text\", 300, 100, 0.03],\n",
    "    [\"/fsx_3/dataset01/cauldron/mapqa\", 483416, 1, 0.483],\n",
    "    [\"/fsx_3/dataset01/cauldron/ocrvqa\", 801579, 1, 0.802],\n",
    "    [\"/fsx_3/dataset01/cauldron/plotqa\", 20249479, 0.01, 0.2025],\n",
    "    [\"/fsx_3/dataset01/cauldron/robut_wikisql\", 86202, 1, 0.086],\n",
    "    [\"/fsx_3/dataset01/cauldron/st_vqa\", 23121, 1, 0.0231],\n",
    "    [\"/fsx_3/dataset01/cauldron/textcaps\", 21953, 10, 0.22],\n",
    "    [\"/fsx_3/dataset01/cauldron/tqa\", 6482, 10, 0.0648],\n",
    "    [\"/fsx_3/dataset01/cauldron/vistext\", 9969, 1, 0.01],\n",
    "    [\"/fsx_3/dataset01/cauldron/tallyqa\", 180000, 1, 0.18]\n",
    "]\n",
    "\n",
    "recipe = {}\n",
    "for d in data:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = \"/fsx_0/checkpoints/tranx/moe/70B_stage2_baseline/evals/eval_ai2d_checkpoint-100\"\n",
    "if glob.glob(f\"{wp}/*results1.txt\"):\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/hpcaas/.mounts/fs-06bc3d6b93146dddd/user/tranx/experiments/ablations\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.get_bash_output(\"pwd\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
