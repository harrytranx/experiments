{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import socket\n",
    "import psutil\n",
    "import sys \n",
    "import os\n",
    "from typing import Any\n",
    "from functools import partial\n",
    "import json\n",
    "from pprint import pprint\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_recent_config_json(directory, file_name='config.json', max_depth=2):\n",
    "    \"\"\"\n",
    "    Find the most recently written config.json file in a directory tree, up to 2 levels deep.\n",
    "    Args:\n",
    "        directory (str): The root directory to search in.\n",
    "    Returns:\n",
    "        str: The path to the most recently written config.json file, or None if no such file is found.\n",
    "    \"\"\"\n",
    "    most_recent_file = None\n",
    "    most_recent_timestamp = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        depth = root.replace(directory, '').count(os.sep)\n",
    "        if max_depth is not None and depth > max_depth:\n",
    "            continue\n",
    "        for file in files:\n",
    "            if file == file_name:\n",
    "                file_path = os.path.join(root, file)\n",
    "                timestamp = os.path.getmtime(file_path)\n",
    "                if timestamp > most_recent_timestamp:\n",
    "                    most_recent_timestamp = timestamp\n",
    "                    most_recent_file = file_path\n",
    "                    \n",
    "    if most_recent_file is not None:\n",
    "        with open(most_recent_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            return data\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "def get_trainer_state(output_dir: str):\n",
    "    checkpoint_dirs = glob.glob(f\"{output_dir}/checkpoint-*\")\n",
    "    checkpoints = [int(d.split(\"/\")[-1].split(\"-\")[-1]) for d in checkpoint_dirs]\n",
    "    if len(checkpoints) == 0:\n",
    "        return None, None \n",
    "    \n",
    "    checkpoint_latest = max(checkpoints)\n",
    "    # print(f\"{checkpoint_latest=}\")\n",
    "\n",
    "    trainer_state_file = os.path.join(\n",
    "        output_dir,\n",
    "        f\"checkpoint-{checkpoint_latest}\",\n",
    "        \"trainer_state.json\"\n",
    "    )\n",
    "\n",
    "    with open(trainer_state_file, 'r') as file:\n",
    "        state = json.load(file)\n",
    "        \n",
    "    return checkpoint_latest, state\n",
    "\n",
    "\n",
    "def get_train_summary(state, config):\n",
    "    grad_accumulation_steps = config['trainer_args']['gradient_accumulation_steps']\n",
    "    num_nodes = config['slurm_args']['nodes']\n",
    "    num_gpus = config['slurm_args']['gpus_per_task']\n",
    "\n",
    "    mp = config['trainer_args'].get(\"model_parallel_size\", 1)\n",
    "    cp = config['trainer_args'].get(\"context_parallel_size\", 1)\n",
    "    pp = config['trainer_args'].get(\"pipeline_parallel_size\", 1)\n",
    "\n",
    "    num_batches_per_step =  state['logging_steps'] * (num_gpus * num_nodes * grad_accumulation_steps) / (mp * pp * cp)\n",
    "    num_input_tokens = 0\n",
    "    num_modality_tokens = 0\n",
    "\n",
    "    for record in state['log_history'][1:]:\n",
    "        # skipping the first record, which always report step=1\n",
    "        # the subsequent record are for step = 10, 20, 30, ... (assuming logging_step = 10)\n",
    "        num_input_tokens += num_batches_per_step * record['num_input_tokens_per_batch']\n",
    "        num_modality_tokens += num_batches_per_step * record['num_modality_embs_per_batch']\n",
    "        \n",
    "    num_input_tokens, num_modality_tokens\n",
    "    total_tokens = num_input_tokens + num_modality_tokens\n",
    "\n",
    "    # calculate VE tokens\n",
    "    chunk_size = config['trainer_args']['chunk_size']\n",
    "    patch_size = 14\n",
    "    cls_token = False\n",
    "    n_prefix_embs = config['trainer_args']['n_prefix_embs']\n",
    "\n",
    "    compression_ratio = ((chunk_size/patch_size)**2 + int(cls_token)) / n_prefix_embs\n",
    "\n",
    "    summary = {\n",
    "        \"num_steps\": state['global_step'],\n",
    "        \"billion_text_tokens\": int(num_input_tokens/1e9),\n",
    "        \"billion_modality_tokens\": int(num_modality_tokens/1e9),\n",
    "        \"billion_llm_tokens\": int(total_tokens/1e9),\n",
    "        \"billion_visual_tokens\": int(compression_ratio * num_modality_tokens/1e9)\n",
    "    }\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers18_dim4096_heads32_latents64_bz64_step4\n",
      "Llama3.1_70B_ViTG_layers18_dim4096_heads32_latents64_bz64_step4, checkpoint_latest=10600\n",
      "/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8\n",
      "Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents64_bz64_step8, checkpoint_latest=11200\n",
      "/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents128_bz32_step8\n",
      "Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents128_bz32_step8, checkpoint_latest=10100\n",
      "/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers26_dim4096_heads32_latents64_bz64_step4\n",
      "Llama3.1_70B_ViTG_layers26_dim4096_heads32_latents64_bz64_step4, checkpoint_latest=10300\n",
      "/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers14_dim4096_heads32_latents64_bz64_step4\n",
      "Llama3.1_70B_ViTG_layers14_dim4096_heads32_latents64_bz64_step4, checkpoint_latest=10000\n",
      "/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents32_bz64_step4\n",
      "Llama3.1_70B_ViTG_layers22_dim4096_heads32_latents32_bz64_step4, checkpoint_latest=10800\n",
      "/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers30_dim4096_heads32_latents64_bz64_step4\n",
      "Llama3.1_70B_ViTG_layers30_dim4096_heads32_latents64_bz64_step4, checkpoint_latest=10300\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/fsx_3/bucket/tranx/checkpoints/perceiver_sizing\"\n",
    "output_dirs = glob.glob(f\"{base_dir}/Llama3*\")\n",
    "for d in output_dirs:\n",
    "    print(d)\n",
    "    name = d.split(\"/\")[-1]\n",
    "    checkpoint_latest, state = get_trainer_state(d)\n",
    "    \n",
    "    print(f\"{name}, {checkpoint_latest=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_steps': 10300,\n",
       " 'billion_text_tokens': 57,\n",
       " 'billion_modality_tokens': 51,\n",
       " 'billion_llm_tokens': 109,\n",
       " 'billion_visual_tokens': 616,\n",
       " 'last_checkpoint': 10300}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"/fsx_3/bucket/tranx/checkpoints/perceiver_sizing/Llama3.1_70B_ViTG_layers18_dim4096_heads32_latents64_bz64_step4\"\n",
    "\n",
    "checkpoint_latest, state = get_trainer_state(d)\n",
    "config = find_most_recent_config_json(output_dir)\n",
    "summary = get_train_summary(state, config)\n",
    "summary['last_checkpoint'] = checkpoint_latest\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-dos:\n",
    "- count GPU hours total\n",
    "- count GPU hours per step \n",
    "- smoothing loss\n",
    "- fit losses/evals on log scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
