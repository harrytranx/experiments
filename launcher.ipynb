{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host name: submit-1\n",
      "Number of CPUs: 32\n",
      "Total memory (GB): 247.74\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import socket\n",
    "import psutil\n",
    "import sys \n",
    "import os\n",
    "from typing import Any\n",
    "from functools import partial\n",
    "import json\n",
    "from pprint import pprint\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pytz\n",
    "# lib_path = ['/fsx_0/user/tranx/experiments']\n",
    "import_paths = [\n",
    "    \"/fsx_0/user/tranx\",\n",
    "    # \"/fsx_0/user/tranx/rsync\",\n",
    "    # \"/fsx_0/user/tranx/experiments/lib\"\n",
    "]\n",
    "for path in import_paths:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "from rsync.llm_mm_aligner.experiments.aws.launch_job import run_job as rsync_run_job\n",
    "from moe.llm_mm_aligner.experiments.aws.launch_job import run_job as moe_run_job\n",
    "# import utils\n",
    "    \n",
    "hostname = socket.gethostname()\n",
    "print(\"Host name:\", hostname)\n",
    "num_cpus = psutil.cpu_count()\n",
    "print(\"Number of CPUs:\", num_cpus)\n",
    "total_memory = psutil.virtual_memory().total / (1024 ** 3)\n",
    "print(\"Total memory (GB):\", round(total_memory, 2))\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Launcher:\n",
    "    def __init__(self, launch_fn, config_base_dir: str, run_log_file: str):\n",
    "        self.launch_fn = launch_fn\n",
    "        self.config_base_dir = config_base_dir \n",
    "        self.run_log_file = run_log_file\n",
    "        self.run_log = self._read_run_log()\n",
    "        \n",
    "    def run(\n",
    "        self,\n",
    "        config: str,\n",
    "        nodes: int = 1,\n",
    "        overrides_dict = None,\n",
    "        qos: str = \"ar-ai-hipri\",\n",
    "        conda_env: str = \"/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822\",\n",
    "        note = None\n",
    "    ):\n",
    "        config_file = os.path.join(self.config_base_dir, config)\n",
    "        \n",
    "        overrides = [[\n",
    "            (\"slurm_args.qos\", qos),\n",
    "            (\"slurm_args.account\", qos),\n",
    "            (\"slurm_args.nodes\", nodes)\n",
    "        ]]\n",
    "        \n",
    "        if overrides_dict is not None:\n",
    "            for k, v in overrides_dict.items():\n",
    "                overrides[0].append((\n",
    "                    k, v\n",
    "                ))\n",
    "\n",
    "        timestamp = datetime.now(pytz.timezone(zone='America/New_York'))\n",
    "        timestamp = timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "        info = OrderedDict([\n",
    "            (\"job_id\", None),\n",
    "            (\"nodes\", nodes),\n",
    "            (\"config\", config),\n",
    "            (\"config_base\", self.config_base_dir),\n",
    "            (\"overrides\", overrides_dict),\n",
    "            (\"conda_env\", conda_env),\n",
    "            (\"timestamp\", timestamp),\n",
    "            (\"note\", note)\n",
    "        ])\n",
    "            \n",
    "        pprint(info)\n",
    "\n",
    "        job_id = self.launch_fn(\n",
    "            config_file = config_file,\n",
    "            conda_env = conda_env,\n",
    "            overrides = overrides\n",
    "        )\n",
    "        \n",
    "        print(int(job_id))        \n",
    "        if job_id is not None:\n",
    "            info.update({\"job_id\": int(job_id)})\n",
    "            \n",
    "            # insert job to top of the list\n",
    "            self.run_log[\"jobs\"] = [info] + self.run_log[\"jobs\"]\n",
    "            self._save_run_log()\n",
    "        \n",
    "    def _read_run_log(self):\n",
    "        try:\n",
    "            with open(self.run_log_file, 'r') as f:\n",
    "                run_log = json.load(f)\n",
    "        except Exception:\n",
    "            run_log = {\"jobs\": []}\n",
    "        \n",
    "        return run_log\n",
    "    \n",
    "    def _save_run_log(self):\n",
    "        with open(self.run_log_file, 'w') as f:\n",
    "            json.dump(self.run_log, f, indent=4)\n",
    "            \n",
    "rsync_launcher = Launcher(\n",
    "    launch_fn=rsync_run_job,\n",
    "    config_base_dir=\"/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws\",\n",
    "    run_log_file=\"/fsx_0/user/tranx/experiments/run_log/run_log_rsync.json\"\n",
    ")\n",
    "\n",
    "moe_launcher = Launcher(\n",
    "    launch_fn=moe_run_job,\n",
    "    config_base_dir=\"/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws\",\n",
    "    run_log_file=\"/fsx_0/user/tranx/experiments/run_log/run_log_moe.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:rsync.llm_mm_aligner.experiments.aws.launch_job:stdout is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1_recap_64nodes_fix/241028_02_18_40_075478/run_log.txt\n",
      "WARNING:rsync.llm_mm_aligner.experiments.aws.launch_job:stderr is redirected to /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1_recap_64nodes_fix/241028_02_18_40_075478/run_log.txt\n",
      "WARNING:rsync.llm_mm_aligner.experiments.aws.launch_job:Config file written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1_recap_64nodes_fix/241028_02_18_40_075478/config.json\n",
      "WARNING:rsync.llm_mm_aligner.experiments.aws.launch_job:script written to: /fsx_0/checkpoints/mm10.1/MM10.1_Stage1_70B/MH22final_70B_ViTH_336px_R1_recap_64nodes_fix/241028_02_18_40_075478/launch_script.sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('job_id', None),\n",
      "             ('nodes', 64),\n",
      "             ('config',\n",
      "              'mm10.1/stage1/MH22final_70B_ViTH_336px_R1_recap_20241024_resume.json'),\n",
      "             ('config_base',\n",
      "              '/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws'),\n",
      "             ('overrides', None),\n",
      "             ('conda_env', '/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822'),\n",
      "             ('timestamp', '2024-10-27 22:18:40'),\n",
      "             ('note', None)])\n",
      "61131\n"
     ]
    }
   ],
   "source": [
    "rsync_launcher.run(\n",
    "    config=\"mm10.1/stage1/MH22final_70B_ViTH_336px_R1_recap_20241024_resume.json\",\n",
    "    nodes=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsync_launcher.run_log\n",
    "rsync_launcher.run_log[\"jobs\"].extend([\"1\", 2, 3])\n",
    "rsync_launcher._save_run_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function run at 0x7f7a4a2653a0>, launcher=<module 'moe.llm_mm_aligner.experiments.aws.launch_job' from '/fsx_0/user/tranx/moe/llm_mm_aligner/experiments/aws/launch_job.py'>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run(\n",
    "    config: str,\n",
    "    overrides_dict: None,\n",
    "    nodes: int = 1,\n",
    "    config_base: str = \"/fsx_0/user/tranx/rsync/llm_mm_aligner/experiments/aws\",\n",
    "    qos: str = \"ar-ai-hipri\",\n",
    "    conda_env: str = \"/fsx_0/user/ahmadyan/.conda/envs/aligner_20240822\",\n",
    "    launcher = launch_job_rsync\n",
    "):\n",
    "    config_file = os.path.join(config_base, config)\n",
    "    \n",
    "    overrides = [[\n",
    "        (\"slurm_args.qos\", qos),\n",
    "        (\"slurm_args.account\", qos),\n",
    "        (\"slurm_args.nodes\", nodes)\n",
    "    ]]\n",
    "    \n",
    "    if overrides_dict is not None:\n",
    "        for k, v in overrides_dict.items():\n",
    "            overrides[0].append((\n",
    "                k, v\n",
    "            ))\n",
    "    \n",
    "    launcher.run_job(\n",
    "        config_file = config_file,\n",
    "        conda_env = conda_env,\n",
    "        overrides = overrides\n",
    "    )\n",
    "    \n",
    "run_moe = partial(run, launcher=launch_job_moe)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Test limit on batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1/241027_03_45_58_614409/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1/241027_03_45_58_614409/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1/241027_03_45_58_614409/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1/241027_03_45_58_614409/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1/241027_03_45_58_614409/config.json\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1/241027_03_45_58_614409/launch_script.sh\n"
     ]
    }
   ],
   "source": [
    "run(\n",
    "    config=\"mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_8x1.json\",\n",
    "    nodes=8,\n",
    "    overrides_dict={\n",
    "        \"trainer_args.output_dir\": \"/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = \"mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_8x1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Slurm config is missing. Default values are used: {'nodes': 1, 'gpus_per_task': 8, 'cpus_per_task': 24, 'mem': 0, 'time': '168:00:00', 'account': 'midpri', 'qos': 'midpri', 'wait_all_nodes': 1, 'conda_env': '/fsx_0/shared/conda/latest', 'job_name': 'test_name', 'job_type': 'train', 'output': '', 'error': '', 'output_file': None}\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stdout is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:stderr is redirected to /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/run_log.txt\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:Config file written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/config.json\n",
      "WARNING:llm_mm_aligner.experiments.aws.launch_job:script written to: /fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2/241027_03_50_53_309031/launch_script.sh\n"
     ]
    }
   ],
   "source": [
    "run(\n",
    "    config=base_config,\n",
    "    nodes=8,\n",
    "    overrides_dict={\n",
    "        \"trainer_args.gradient_accumulation_steps\": 2,\n",
    "        \"trainer_args.output_dir\": \"/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x2\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bz in [16, 32]:\n",
    "    run(\n",
    "        config=f\"mm10.1_moe/moe/MH22final_70B_ViTH_336px_R1_moe_22x8x2_bz_{bz}x1.json\",\n",
    "        nodes=8,\n",
    "        overrides_dict={\n",
    "            \"trainer_args.output_dir\": \"/fsx_0/checkpoints/tranx/moe/70B_moe_22x8x2_n8_bz_8x1\"\n",
    "        }\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
